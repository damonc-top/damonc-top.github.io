[ { "title": "Git Worktree并行开发：原理、实践与最佳实践", "url": "/posts/git-worktree-parallel-development/", "categories": "Git, 开发工具", "tags": "git-worktree, 并行开发, 版本控制, 工作流优化", "date": "2026-01-22 23:30:00 +0800", "content": "Git Worktree是现代Git工作流中的重要工具，为并行开发提供了轻量级解决方案 Git Worktree工作原理 Git Worktree是Git 2.5+版本引入的功能，允许在同一个Git仓库中创建多个工作目录。其核心设计理念基于共享对象数据库和独立工作环境的巧妙结合。 架构演进 传统Git工作流中，每个仓库只有一个工作目录，而Worktree打破了这一限制，实现了”一个仓库，多个工作环境”的架构： graph TB subgraph \"传统Git架构\" A[主仓库] --&gt; B[工作目录] A --&gt; C[对象数据库] A --&gt; D[引用数据库] end subgraph \"Git Worktree架构\" E[主仓库] --&gt; F[对象数据库] E --&gt; G[引用数据库] E --&gt; H[工作树管理] H --&gt; I[工作树1] H --&gt; J[工作树2] H --&gt; K[工作树3] I --&gt; L[工作目录1] J --&gt; M[工作目录2] K --&gt; N[工作目录3] end 完整工作流程 下图展示了Git Worktree的完整工作流程： 图：Git Worktree完整工作流程示意图 环境变量机制 基本架构 graph TD A[主Git仓库] --&gt; B[对象数据库 objects/] A --&gt; C[打包引用 packed-refs] A --&gt; D[共享配置] A --&gt; E[工作树管理目录 .git/worktrees/] E --&gt; F[工作树A HEAD, index, gitdir] E --&gt; G[工作树B HEAD, index, gitdir] F --&gt; H[链接工作目录A] G --&gt; I[链接工作目录B] H --&gt; J[工作文件A] I --&gt; K[工作文件B] 环境变量机制 每个工作树通过环境变量管理路径解析： GIT_DIR: 指向工作树特定的管理目录 GIT_COMMON_DIR: 指向共享的主仓库目录 工作目录空间占用原理 共享内容（零额外开销） 对象数据库 (objects/ 目录) - 所有commit、tree、blob对象 打包引用 (packed-refs) - 压缩的引用信息 配置共享部分 - 全局Git配置 钩子脚本 (hooks/) - Git事件处理脚本 独立内容（最小开销） 索引文件 (index) - 每个工作树独有的暂存区状态 HEAD引用 - 当前工作树指向的commit 工作树特定引用 - 如 refs/worktree/, refs/bisect/ 工作树配置 - 当启用 extensions.worktreeConfig 时 空间节省示例 # 主仓库大小 du -sh .git # 例如: 50MB # 每个额外工作树增加的大小 du -sh .git/worktrees/* # 通常只有几KB到几十KB 性能对比分析 特性 Git Worktree 传统 git clone 优势 存储占用 ~50MB + 微量 50MB × N 节省 90%+空间 时间成本 即时创建 整个仓库克隆 节省 95%+时间 网络带宽 无需网络 整个历史下载 节省 99%+带宽 数据一致性 完全一致 需要手动同步 自动同步 并行能力 真正并行 分离环境 无等待时间 实际应用效果 graph LR A[传统方式] --&gt; B[大型项目: 2GB] B --&gt; C[创建3个工作环境] C --&gt; D[总占用: 6GB] C --&gt; E[时间: 10分钟] C --&gt; F[网络: 6GB下载] G[使用Worktree] --&gt; H[大型项目: 2GB] H --&gt; I[创建3个工作树] I --&gt; J[总占用: 2GB + 300KB] I --&gt; K[时间: 3秒钟] I --&gt; L[网络: 0KB] 对于大型项目，Git Worktree可以节省数GB的存储空间和数十分钟的网络下载时间 冲突避免与锁机制 引用隔离机制 # 共享引用（所有工作树可见） refs/heads/* # 分支引用 refs/tags/* # 标签引用 # 工作树特定引用（各自独立） refs/bisect/* # 二分查找状态 refs/worktree/* # 工作树特定状态 refs/rewritten/* # 变基操作状态 文件锁机制 Git通过多种锁机制确保操作安全： 索引锁 - 防止多个进程同时修改暂存区 引用锁 - 保护引用更新操作 工作树锁 - git worktree lock 手动锁定 修改同一文件的场景 当两个工作树修改同一文件时，Git通过以下机制避免冲突： sequenceDiagram participant DevA as 开发者A participant WT_A as 工作树A participant Index_A as 索引A participant Repo as 共享仓库 participant Index_B as 索引B participant WT_B as 工作树B participant DevB as 开发者B Note over DevA, DevB: 并行修改文件file.txt DevA-&gt;&gt;WT_A: vim file.txt (修改内容) DevB-&gt;&gt;WT_B: vim file.txt (修改内容) DevA-&gt;&gt;Index_A: git add file.txt DevB-&gt;&gt;Index_B: git add file.txt DevA-&gt;&gt;Repo: git commit -m \"修改A\" DevB-&gt;&gt;Repo: git commit -m \"修改B\" Note over Repo: 两个提交存在于不同分支 DevA-&gt;&gt;Repo: git merge 工作树B的提交 Repo--&gt;&gt;DevA: 检测到冲突 DevA-&gt;&gt;DevA: 解决冲突 DevA-&gt;&gt;Repo: git commit -m \"解决冲突\" 冲突避免机制详解 文件副本独立性：每个工作树有自己的文件副本 索引隔离：每个工作树的索引独立运作 引用分离：分支引用共享，但工作树特定引用独立 合并时检测：只有在合并操作时才会检测冲突 由于每个工作树有独立的索引和工作目录，修改的是各自的文件副本，只有在推送或合并时才会遇到冲突 与vib-kanban项目的相似性 虽然具体实现不同，但设计理念相似： 并行工作流支持 特性 Git Worktree vib-kanban 多任务处理 多个分支同时开发 多列任务并行处理 上下文隔离 独立工作目录 独立任务卡片 资源复用 共享对象数据库 可能共享状态管理 管理界面相似性 都需要列表显示所有工作环境/任务 提供创建、删除、切换等操作 支持状态跟踪和进度管理 实际操作指南 基本操作命令 # 创建新工作树 git worktree add ../new-feature feature-branch # 列出所有工作树 git worktree list # 移动到特定工作树 cd ../new-feature # 删除工作树 git worktree remove ../new-feature # 清理过期工作树 git worktree prune 实际应用场景 场景1：紧急热修复 # 在主分支开发时发现紧急bug git worktree add ../hotfix master git -C ../hotfix checkout -b hotfix/urgent # 在hotfix目录中修复并测试 cd ../hotfix vim fix.js git add . git commit -m \"紧急修复\" git push origin hotfix/urgent 场景2：功能并行开发 # 同时开发两个功能 git worktree add ../feature-a feature/a git worktree add ../feature-b feature/b # 在不同终端中并行工作 # 终端1: cd ../feature-a &amp;&amp; npm run dev # 终端2: cd ../feature-b &amp;&amp; npm run dev 场景3：代码审查 # 审查PR时创建独立环境 git worktree add ../pr-review pr-branch cd ../pr-review npm install npm test # 在隔离环境中测试 高级配置 启用工作树特定配置： git config extensions.worktreeConfig true # 设置工作树特定配置 git config --worktree user.email \"worktree@example.com\" git config --worktree core.editor \"code --wait\" 企业级应用场景 大型团队开发流程 graph TD A[主开发分支] --&gt; B[快速修复环境] A --&gt; C[功能开发环境] A --&gt; D[测试验证环境] A --&gt; E[代码审查环境] B --&gt; F[热修工作树] C --&gt; G[功能工作树] D --&gt; H[测试工作树] E --&gt; I[审查工作树] F --&gt; J[立即部署测试] G --&gt; K[并行开发测试] H --&gt; L[独立测试环境] I --&gt; M[原始代码审查] 微服务架构下的应用 场景1：多服务并行开发 # 为每个微服务创建独立开发环境 git worktree add ../user-service feature/user-auth git worktree add ../order-service feature/order-process git worktree add ../payment-service feature/payment-gateway # 并行启动所有服务 cd ../user-service &amp;&amp; npm run dev &amp; cd ../order-service &amp;&amp; npm run dev &amp; cd ../payment-service &amp;&amp; npm run dev &amp; 场景2：多环境测试 # 创建不同测试环境 git worktree add ../test-staging staging git worktree add ../test-production production # 同时运行多环境测试 cd ../test-staging &amp;&amp; npm test cd ../test-production &amp;&amp; npm test 连续集成/连续部署(CI/CD) sequenceDiagram participant Dev as 开发者 participant WT as Worktree participant CI as CI/CD服务器 participant Prod as 生产环境 Dev-&gt;&gt;WT: git worktree add ../ci-test Dev-&gt;&gt;WT: 在测试环境测试 Dev-&gt;&gt;CI: 推送到CI服务器 CI-&gt;&gt;CI: 自动创建测试Worktree CI-&gt;&gt;CI: 执行自动化测试 CI-&gt;&gt;Prod: 部署到生产环境 最佳实践与注意事项 企业级推荐实践 统一命名规范：&lt;team&gt;-&lt;project&gt;-&lt;purpose&gt; 自动化管理：通过脚本管理工作树生命周期 监控告警：设置工作树超时和资源监控 文档化：维护工作树使用文档和流程图 高级配置建议 # 启用工作树特定配置 git config extensions.worktreeConfig true # 设置团队级别配置 git config --worktree user.name \"Team Developer\" git config --worktree user.email \"team@company.com\" git config --worktree core.editor \"code --wait\" # 自动化清理脚本 # 添加到.git/hooks/post-commit #!/bin/sh git worktree prune --expire=30.days 注意事项 避免在工作树中使用子模块，官方文档标注此功能为实验性 在企业环境中，建议配合Docker容器使用，实现更好的环境隔离 故障排除与恢复 工作树连接断开时： # 手动修复连接 git worktree repair # 或重新链接 echo \"gitdir: /path/to/main/.git/worktrees/worktree-name\" &gt; .git # 快速重建工作树 git worktree remove ../broken-tree git worktree add ../new-tree branch-name 未来发展趋势 智能化发展 未来Git Worktree可能集成更多智能功能： graph TB A[智能Worktree] --&gt; B[自动分配资源] A --&gt; C[动态扩缩容] A --&gt; D[预觨发冲突] A --&gt; E[自动优化缓存] B --&gt; F[根据项目需求分配CPU/内存] C --&gt; G[根据工作负荷自动扩缩容] D --&gt; H[预测并避免潜在冲突] E --&gt; I[自动管理缓存提高性能] 云原生支持 与云平台深度集成，实现： 云端工作树同步 分布式缓存共享 跨区域并行开发 AI助理自动优化 总结 Git Worktree通过巧妙的文件链接和路径重定向机制，在保持Git强大功能的同时，提供了轻量级的并行开发解决方案。它： ✅ 显著减少存储空间占用（90%+节省） ✅ 提供真正的并行开发能力（无等待时间） ✅ 避免上下文切换的开销（保持开发流畅） ✅ 保持操作的安全性和隔离性（内置锁机制） ✅ 支持企业级应用（微服务、CI/CD集成） ✅ 提供丰富可视化展示（SVG+Mermaid） 案例效果展示 pie title Git Worktree效果对比 \"节省存储空间\" : 90 \"节省网络带宽\" : 99 \"节省时间成本\" : 95 \"提升开发效率\" : 80 对于需要同时处理多个分支、进行代码审查或紧急修复的开发场景，Git Worktree是一个不可或缺的工具。随着云原生技术的发展，它还将集成更多智能化功能，为开发者提供更高效、更便捷的并行开发体验。 本文基于实际项目经验和技术分析撰写，包含丰富的可视化图表和实际案例，希望对您的开发工作有所帮助。" }, { "title": "Claude Code(十)思考与技巧：克服\"空白瘫痪\"，不断向前演进", "url": "/posts/claude-code-tips_tricks/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent, Best Practices", "date": "2026-01-22 20:31:00 +0800", "content": "这是 Claude Code 系列文章的最后一篇。 经过前面九章的学习，我已经掌握了从环境配置到 MCP 协议的所有硬核技能。但在每天高强度的使用中，我发现要把工具用好，“心法”往往比”招式”更重要。 本文记录了我在与 Claude Code 结对编程数月后，沉淀下来的一些思考和实用技巧。 克服”空白瘫痪” (Blank Canvas Paralysis) 我刚开始使用 Agent 时，经常盯着闪烁的光标发呆：我该怎么描述这个复杂的重构任务？我是不是应该先写一个 500 字的完美 Prompt？ 这种犹豫被称为”空白瘫痪”。 我的对策：Just Start (先开始再说)。 Claude Code 不是一次性交付的黑盒，它是交互式的。我现在的习惯是： “嘿，先帮我看看 src/auth 目录下的代码，我感觉逻辑有点乱。” 哪怕只是这样一个模糊的指令，Claude 也会开始运行 ls 和 read_file。随着它的反馈（”我看到了 user.ts 和 auth.ts…“），我的思路会被打开，然后我再进行第二轮指令： “对，就是 auth.ts，里面的 login 函数有点太长了，帮我拆分一下。” 技巧：不要试图一次性把话说完。把对话当成是和同事在白板前的讨论，从模糊到清晰，迭代前进。 小步快跑与频繁提交 AI 是有短期记忆瓶颈的（Context Window）。如果我让它一口气”重构整个后端”，它很可能会在修改了 50 个文件后，因为 Token 超标而崩溃，或者改到最后忘了最初的目标。 我的对策：原子化提交 (Atomic Commits)。 我现在的 workflow 是这样的： Task 1: “给 User 增加 phone 字段。” -&gt; Claude 做完 -&gt; 我 /review -&gt; git commit。 Task 2: “更新相关的 API 验证逻辑。” -&gt; Claude 做完 -&gt; 我 /review -&gt; git commit。 Task 3: “补充单元测试。” -&gt; Claude 做完 -&gt; 我 /review -&gt; git commit。 每一步都保持上下文清爽。如果某一步搞砸了，我也能轻松 /rewind 或 git reset，而不会损失整个下午的工作。 从”纠正”到”教学” 这是新手和高手的最大区别。 当 Claude 犯错时（比如它又用了 console.log 而不是 logger），新手会说： “错了，改成 logger。” Claude 会改过来，但下一次它可能还会犯。 高手会说： “错了。请更新 CLAUDE.md，添加一条规则：’前端项目严禁使用 console.log，必须统一使用 src/utils/logger‘。然后帮我修正代码。” 我的对策：知识沉淀。 我把每一次错误都视为一次完善系统记忆（System Memory）的机会。随着 CLAUDE.md 的不断丰富，我的 Agent 会越来越懂我，错误率呈指数级下降。 善用 /compact 保持大脑清醒 当会话进行到 30 轮以上时，我能明显感觉到 Claude 变”笨”了。它开始忽略我的新指令，或者在旧代码上打转。 这是因为无关的上下文噪音（Noise）太多了。 我的对策：主动垃圾回收 (GC)。 一旦我完成了一个阶段性的小任务（比如修好了一个 Bug），我会立刻执行： /compact \"刚刚修复了登录 Bug，现在准备开始优化注册流程。\" 这相当于给 Claude 洗了个脸，让它清空短期记忆，只保留核心的项目状态，轻装上阵迎接下一个任务。 结语：人机共生的未来 写完这个系列，我最大的感触是：Claude Code 并没有取代我，它增强了我。 以前，我 70% 的时间在写样板代码、查文档、调试低级错误，只有 30% 的时间在思考架构和业务。 现在，这个比例倒过来了。我变成了架构师、Code Reviewer 和产品经理。 我不再是那个在那敲键盘的”码农”，我是指挥硅基大脑构建数字大厦的”工程师”。 希望这个系列能帮助你（哦不，是帮助未来的我）在这个 AI 辅助编程的新时代，找到属于自己的节奏。 Keep Coding, Keep Evolving." }, { "title": "Claude Code(九)Plugins：扩展功能的无限可能", "url": "/posts/claude-code-plugins/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent, Plugins", "date": "2026-01-21 21:30:10 +0800", "content": "在构建了坚实的配置基础（Config）、强大的技能（Skills）和连接万物的协议（MCP）之后，我发现 Claude Code 的生态拼图还差最后一块——Plugins（插件）。 如果把 Claude Code 比作 Chrome 浏览器，MCP 是底层的 HTTP 协议，Subagents 是不同的网页标签，那么 Plugins 就是那些让我们爱不释手的 Chrome 扩展程序。 本文记录了我对 Claude Code 插件系统的探索，以及它如何填补了功能的最后”一公里”。 Core Principles：插件的定位 在使用过程中，我经常被问到一个问题：“Plugins 和 MCP 到底有什么区别？” 经过实战，我总结了以下对比： 特性 MCP Server Plugins 核心隐喻 数据线 (USB Cable) 功能包 (Extension) 侧重点 连接外部数据（如 DB、GitHub） 增强核心能力（如绘图、搜索） 运行位置 独立进程，可能在远程 注入在 Claude Code 运行时内部 典型例子 postgres-server (连接数据库) mermaid-renderer (渲染流程图) 简单来说，如果我想让 Claude “看到” 更多东西，我用 MCP；如果我想让 Claude “做” 更多事情，我用 Plugins。 Practical Implementation：插件管理 Claude Code 提供了一套类似 npm 或 brew 的包管理体验。 发现与安装 我可以使用 /plugins 命令来管理插件： # 列出已安装插件 /plugins list # 搜索市场中的插件 /plugins search search-web # 安装插件 /plugins install @anthropic/web-search 实战案例：Web Search 插件 在我安装了 @anthropic/web-search 插件后，Claude 瞬间拥有了联网能力。这与 MCP 的 brave-search 类似，但 Plugin 通常封装得更深，体验更丝滑。 “Claude, 帮我查一下 React 19 最新的 Hook 变更，并总结对我们项目的影响。” 它会自动调用插件提供的 search 工具，阅读多篇最新的技术博客，然后结合我当前的项目代码（Context），给出一份针对性的迁移报告。 实战案例：Code Interpreter (代码解释器) 这是我最依赖的一个官方插件。虽然 Claude Code 原生能写代码，但它默认是在我的终端里运行。 安装 code-interpreter 插件后，Claude 获得了一个隔离的 Python 沙箱。 “分析这个 CSV 文件的数据分布，画一张热力图。” Claude 会在沙箱中运行 Python 代码，生成 Matplotlib 图表，并直接在终端或 IDE 中展示给我。这完全不污染我的本地环境，非常适合做数据分析和原型验证。 Deep Analysis：插件的解剖学 在深入研究了官方的 code-review、hookify 和 plugin-dev 等插件源码后，我发现一个 Claude Code 插件的结构远比我想象的要严谨。它不仅仅是一堆 Prompt 的集合，而是一个标准的工程项目。 核心目录结构 一个典型的生产级插件遵循特定的物理布局，这决定了 Claude Code 如何加载和识别各个组件： my-plugin/ ├── .claude-plugin/ │ └── plugin.json # 插件的\"身份证\"，定义元数据 ├── commands/ # 快捷命令 (Slash Commands)，如 /code-review │ └── review.md # 每个 Markdown 文件定义一个命令 ├── agents/ # 专用子智能体 (Sub-agents) │ └── bug-hunter.md # 复杂任务的编排逻辑 ├── skills/ # Agent 技能 (Skills) │ └── my-skill/ │ └── SKILL.md # 核心说明文档，定义工具使用逻辑 ├── hooks/ # 事件钩子 (Hooks) │ └── hooks.json # 钩子触发定义 └── scripts/ # 辅助脚本 (Bash/Python/JS) 关键约束：千万不要把 commands/、agents/ 等目录放进 .claude-plugin/ 文件夹内部，它们必须位于插件根目录。 Manifest 定义 (plugin.json) plugin.json 是插件的入口，它除了定义名称和版本，最重要的作用是权限声明和命名空间定义： { \"name\": \"my-first-plugin\", \"description\": \"A professional toolset for AI-assisted dev\", \"version\": \"1.0.0\", \"permissions\": [\"bash\", \"gh\", \"read\", \"write\"], // 显式声明工具访问权限 \"author\": { \"name\": \"Damon\", \"email\": \"damon@example.com\" } } 通过 name 字段，插件的所有命令会自动获得命名空间保护（如 /my-first-plugin:hello），这有效避免了不同插件之间的命令冲突。 事件钩子 (Hooks)：干预生命周期 Hooks 是插件系统中最具”侵入性”但也最强大的特性。通过 hooks/hooks.json，插件可以监听并干预 Claude Code 的核心操作流： PreToolUse: 在工具（如 Bash, Write）执行前运行。常用于安全审计或自动注入参数。 PostToolUse: 工具执行后运行。常用于自动格式化代码或运行测试。 UserPromptSubmit: 在用户提交问题时运行。可以拦截用户输入并注入特定的上下文（如项目的架构图）。 Stop: 任务完成时触发。用于最终的质量把关。 { \"hooks\": { \"PostToolUse\": [ { \"matcher\": \"Write|Edit\", \"hooks\": [{ \"type\": \"command\", \"command\": \"python3 ${CLAUDE_PLUGIN_ROOT}/scripts/validate.py\", \"timeout\": 10 }] } ] } } ${CLAUDE_PLUGIN_ROOT} 环境变量的存在，使得插件内的脚本调用能够保持路径独立，这对于分发和共享至关重要。 Advanced Implementation：剖析官方 Code Review 插件 Anthropic 官方提供的 code-review 插件是智能体编排（Agent Orchestration）的最佳实践。 成本与效率的权衡 (Tiered Processing) 当你输入 /code-review 时，它采用的是分层处理策略： Haiku Gatekeeper: 首先启动快速的 Haiku 模型进行初步检查。如果是 Draft PR 或极其琐碎的修改（如拼写错误），流程直接终止。 Context Gathering: 它会扫描项目中的 CLAUDE.md，不仅是根目录的，还包括被修改文件所在子目录的特定规范。 并行协同流水线 (Parallel Sub-agents) 插件会同时启动 4 个 Agent 并行工作，这在 Claude Code 中通过 Task 接口实现： Worker 1 &amp; 2 (Sonnet): 拿着 CLAUDE.md 逐行审计合规性。 Worker 3 (Opus): 专注于 Bug 扫描。它被告知”只看 Diff，不看上下文”，以保持对逻辑错误的极致敏感度。 Worker 4 (Opus): 专注于安全与逻辑一致性。 置信度过滤与二次验证 (Validation Loop) 这是最严谨的一步：对于 Worker 发现的每一个 Issue，插件会启动独立的验证子智能体。 验证者会拿到 PR 的描述和 Issue 详情，确认”这真的算个 Bug 吗？” 只有通过二次验证且置信度 ≥ 80 的问题，才会最终通过 GitHub CLI (gh) 提交为 Inline Comment。 实战案例：从 Standalone 迁移到 Plugin 如果你已经在 .claude/ 下积累了很多好用的配置，迁移到插件模式只需三步： 物理隔离：创建新文件夹，建立 .claude-plugin/plugin.json。 文件对齐： .claude/commands/ -&gt; my-plugin/commands/ .claude/agents/ -&gt; my-plugin/agents/ .claude/skills/ -&gt; my-plugin/skills/ 配置解耦：将 settings.json 中的 hooks 配置提取到 my-plugin/hooks/hooks.json。 生态精选：Marketplace Highlights 在官方的 marketplace.json 中，有几个插件展示了插件系统的不同边界： hookify: 展示了如何通过插件提供一个”配置框架”，允许用户通过简单的 Markdown 定义钩子。 pr-review-toolkit: 提供了更细粒度的 PR 审查，包含 silent-failure-hunter 和 type-design-analyzer 等专项 Agent。 plugin-dev: 包含了大量的开发辅助 Skill，甚至包括 plugin-validator 和 skill-reviewer。 结语 Plugins 将 Claude Code 从一个单纯的 “对话式 AI” 升华为一个 “可插拔的研发操作系统”。通过 Manifest 权限边界、Hooks 生命周期干预 和 Sub-agents 并行编排，我们能够构建出真正具备”工程直觉”的自动化工具链。 下一篇预告：Claude Code(十)思考与技巧：克服”空白瘫痪”，不断向前演进" }, { "title": "Claude Code(八)MCP Server：连接万物的通用协议", "url": "/posts/claude-code-mcp-server/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent, MCP", "date": "2026-01-20 00:05:10 +0800", "content": "在使用 Claude Code 的早期，我常常有一种”天才被关在小黑屋”的感觉。虽然它精通代码，但它看不见我运行中的 PostgreSQL 数据库，读不到 GitHub 上的 Issue 详情，也无法访问公司内部的知识库 API。 它被困在本地文件系统的沙箱里。 Model Context Protocol (MCP) 的出现，打破了这堵墙。如果说 USB 接口让电脑可以连接万物，那么 MCP 就是 AI 时代的 USB 协议。 本文记录了我如何利用 MCP 将 Claude Code 连接到外部世界，实现真正的数据互通。 更多关于 MCP 开源标准的信息，请参考：Model Context Protocol 官方文档 Core Principles：什么是 MCP？ Model Context Protocol (MCP) 是 Anthropic 牵头制定的一套开放标准。它的核心理念很简单：标准化 AI 模型获取上下文（Context）和执行动作（Action）的方式。 在没有 MCP 之前，如果我想让 Claude 访问 GitHub，我需要等待官方开发 “GitHub 插件”。如果我想让它访问我的私有数据库，我得自己写复杂的 Glue Code。 有了 MCP，我只需要运行一个符合标准的 MCP Server。Claude Code 作为 MCP Client，就能自动发现并使用这个 Server 提供的资源（Resources）和工具（Tools）。 架构图解 graph LR subgraph Claude_Code [Claude Code Client] Core[AI Model] Client[MCP Client 模块] end subgraph MCP_Layer [MCP Servers] GitHub[GitHub Server] DB[Postgres Server] Web[Browser Server] end subgraph Real_World [外部世界] Repo[代码仓库/Issues] Data[业务数据] Internet[互联网] end Core &lt;--&gt; Client Client -- JSON-RPC --&gt; GitHub Client -- JSON-RPC --&gt; DB Client -- JSON-RPC --&gt; Web GitHub &lt;--&gt; Repo DB &lt;--&gt; Data Web &lt;--&gt; Internet MCP 能做什么 通过 MCP 服务器，我可以让 Claude Code： 从问题跟踪系统实现功能：”添加 JIRA issue ENG-4521 中描述的功能，并在 GitHub 上创建 PR” 分析监控数据：”检查 Sentry 和 Statsig，分析 ENG-4522 功能的使用情况” 查询数据库：”基于我们的 PostgreSQL 数据库，找到使用 ENG-4523 功能的 10 个随机用户的邮箱” 集成设计：”根据 Slack 中发布的新 Figma 设计更新我们的标准邮件模板” 自动化工作流：”创建 Gmail 草稿，邀请这 10 个用户参加关于新功能的反馈会话” Practical Implementation：实战配置 安装方式概览 MCP 服务器可以通过三种不同的传输方式进行配置，具体取决于我的需求： HTTP 服务器：连接远程 MCP 服务的推荐选项，广泛应用于云端服务 SSE 服务器：基于 Server-Sent Events 的传输方式（已弃用，建议使用 HTTP） Stdio 服务器：在本地机器上运行的服务器进程，适合需要直接系统访问或自定义脚本的工具 添加 HTTP 服务器 HTTP 服务器是连接远程 MCP 服务的推荐选项。这是最广泛支持的云端服务传输方式。 # 基本语法 claude mcp add --transport http &lt;name&gt; &lt;url&gt; # 实际示例：连接到 Notion claude mcp add --transport http notion https://mcp.notion.com/mcp # 带有 Bearer token 的示例 claude mcp add --transport http secure-api https://api.example.com/mcp \\ --header \"Authorization: Bearer your-token\" 添加 SSE 服务器 SSE (Server-Sent Events) 传输已弃用。尽可能使用 HTTP 服务器。 # 基本语法 claude mcp add --transport sse &lt;name&gt; &lt;url&gt; # 实际示例：连接到 Asana claude mcp add --transport sse asana https://mcp.asana.com/sse # 带有认证头的示例 claude mcp add --transport sse private-api https://api.company.com/sse \\ --header \"X-API-Key: your-key-here\" 添加本地 Stdio 服务器 Stdio 服务器作为本地进程在机器上运行，适合需要直接系统访问或自定义脚本的工具。 # 基本语法 claude mcp add [options] &lt;name&gt; -- &lt;command&gt; [args...] # 实际示例：添加 Airtable 服务器 claude mcp add --transport stdio --env AIRTABLE_API_KEY=YOUR_KEY airtable \\ -- npx -y airtable-mcp-server 选项顺序 所有选项（--transport、--env、--scope、--header）必须在服务器名称之前。--（双破折号）然后将服务器名称与传递给 MCP 服务器的命令和参数分隔开。 例如： claude mcp add --transport stdio myserver -- npx server → 运行 npx server claude mcp add --transport stdio --env KEY=value myserver -- python server.py --port 8080 → 以环境变量 KEY=value 运行 python server.py --port 8080 这样可以防止 Claude 的标志与服务器的标志发生冲突。 管理服务器 配置完成后，我可以通过以下命令管理 MCP 服务器： # 列出所有配置的服务器 claude mcp list # 获取特定服务器的详细信息 claude mcp get github # 移除服务器 claude mcp remove github #（在 Claude Code 中）检查服务器状态 /mcp 常用 MCP 服务器 以下是一些常用的可以连接到 Claude Code 的 MCP 服务器： 📋 常用 MCP 服务器列表 服务器名称 描述 安装命令 GitHub GitHub 仓库和 Issue 访问 claude mcp add --transport http github https://api.githubcopilot.com/mcp/ Sentry 错误监控和分析 claude mcp add --transport http sentry https://mcp.sentry.dev/mcp Notion Notion 数据库和页面 claude mcp add --transport http notion https://mcp.notion.com/mcp PostgreSQL 数据库查询 claude mcp add --transport stdio db -- npx -y @bytebase/dbhub --dsn \"postgresql://user:pass@host:5432/db\" Google Drive Google Drive 文件访问 claude mcp add --transport http gdrive https://mcp.google.com/drive 使用第三方 MCP 服务器需自行承担风险——Anthropic 未验证所有这些服务器的正确性或安全性。确保信任你安装的 MCP 服务器。在使用可能获取不受信任内容的 MCP 服务器时要格外小心，因为这可能会使你面临提示注入风险。 MCP 配置作用域 MCP 服务器可以在三个不同的作用域级别进行配置，每个级别都有独特的管理服务器可访问性和共享的目的。 Local 作用域 Local 作用域服务器是默认的配置级别，存储在 ~/.claude.json 下的项目路径中。这些服务器仅属于我个人，仅在当前项目目录内工作时可访问。这个作用域适合个人开发服务器、实验性配置或包含不应共享的敏感凭证的服务器。 注意：MCP 的 “local 作用域” 术语与一般的 local 设置不同。MCP local 作用域服务器存储在 ~/.claude.json（你的主目录）中，而一般 local 设置使用 .claude/settings.local.json（在项目目录中）。 # 添加 local 作用域服务器（默认） claude mcp add --transport http stripe https://mcp.stripe.com # 明确指定 local 作用域 claude mcp add --transport http stripe --scope local https://mcp.stripe.com Project 作用域 Project 作用域服务器通过在项目根目录存储 .mcp.json 文件来实现团队协作。该文件旨在签入版本控制，确保所有团队成员可以访问相同的 MCP 工具和服务。 # 添加 project 作用域服务器 claude mcp add --transport http paypal --scope project https://mcp.paypal.com/mcp 生成的 .mcp.json 文件遵循标准化格式： { \"mcpServers\": { \"shared-server\": { \"command\": \"/path/to/server\", \"args\": [], \"env\": {} } } } 出于安全原因，Claude Code 在使用来自 .mcp.json 文件的 project 作用域服务器之前会提示批准。如果需要重置这些批准选择，请使用 claude mcp reset-project-choices 命令。 User 作用域 User 作用域服务器存储在 ~/.claude.json 中，提供跨项目可访问性，使我机器上所有项目都可以使用这些服务器，同时对我的用户账户保持私密。这个作用域适用于跨不同项目的个人实用工具、开发工具或经常使用的服务。 # 添加 user 作用域服务器 claude mcp add --transport http hubspot --scope user https://mcp.hubspot.com/anthropic 选择合适的作用域 根据以下标准选择作用域： Local 作用域：个人服务器、实验性配置或特定于一个项目的敏感凭证 Project 作用域：团队共享服务器、项目特定工具或协作所需的服务 User 作用域：跨多个项目需要的个人实用工具、开发工具或经常使用的服务 MCP 服务器存储在哪里？ User 和 local 作用域：~/.claude.json（在 mcpServers 字段中或项目路径下） Project 作用域：项目根目录中的 .mcp.json（签入源代码控制） Managed：系统目录中的 managed-mcp.json（见下文） 作用域层次和优先级 MCP 服务器配置遵循清晰的优先级层次。当多个作用域中存在同名服务器时，系统通过优先考虑 local 作用域服务器来解决冲突，其次是 project 作用域服务器，最后是 user 作用域服务器。这种设计确保了在需要时个人配置可以覆盖共享配置。 环境变量扩展 Claude Code 支持 .mcp.json 文件中的环境变量扩展，允许团队在保持机器特定路径和敏感值（如 API 密钥）的灵活性的同时共享配置。 支持的语法： ${VAR} - 扩展为环境变量 VAR 的值 ${VAR:-default} - 如果设置了则扩展为 VAR，否则使用 default 扩展位置： 环境变量可以在以下位置扩展： command - 服务器可执行文件路径 args - 命令行参数 env - 传递给服务器的环境变量 url - 对于 HTTP 服务器类型 headers - 对于 HTTP 服务器认证 带变量扩展的示例： { \"mcpServers\": { \"api-server\": { \"type\": \"http\", \"url\": \"${API_BASE_URL:-https://api.example.com}/mcp\", \"headers\": { \"Authorization\": \"Bearer ${API_KEY}\" } } } } 如果所需的环境变量未设置且没有默认值，Claude Code 将无法解析配置。 实战案例 案例 1：使用 Chrome DevTools MCP Chrome DevTools MCP 服务器让我能够直接通过 Claude Code 分析网页性能和调试问题。 前提条件： Node.js v20.19 或更高版本 Chrome 当前稳定版本 方法一：使用 CLI 命令添加 # 添加 Chrome DevTools MCP 服务器到 user 作用域 claude mcp add --scope user chrome-devtools -- npx chrome-devtools-mcp@latest # 重启 Claude Code 使配置生效 方法二：通过 settings.json 配置 如果更喜欢手动配置，可以直接编辑 settings.json 文件： # 编辑用户配置文件 vim ~/.claude.json 在 mcpServers 部分添加以下配置： { \"mcpServers\": { \"chrome-devtools\": { \"command\": \"npx\", \"args\": [\"-y\", \"chrome-devtools-mcp@latest\"], \"env\": {} } } } 如果不想收集使用统计，可以添加环境变量： { \"mcpServers\": { \"chrome-devtools\": { \"command\": \"npx\", \"args\": [ \"-y\", \"chrome-devtools-mcp@latest\", \"--no-usage-statistics\" ], \"env\": {} } } } 使用示例 # 重启 Claude Code 使配置生效后，可以这样使用： # 分析网页性能 &gt; \"检查 https://developers.chrome.com 的性能表现\" # 调试页面问题 &gt; \"分析我本地运行在 localhost:3000 的页面，找出性能瓶颈\" # 检查网络请求 &gt; \"获取 https://example.com 页面加载时的所有网络请求\" 配置说明： CLI 命令会自动将配置写入 ~/.claude.json（user 作用域） 浏览器配置文件存储在 $HOME/.cache/chrome-devtools-mcp/chrome-profile-$CHANNEL 使用 --no-usage-statistics 标志可以禁用 Google 的使用统计收集 案例 2：连接 GitHub 进行代码审查 # 添加 GitHub MCP 服务器 claude mcp add --transport http github https://api.githubcopilot.com/mcp/ # 在 Claude Code 中，如果需要则进行认证 &gt; /mcp # 选择 GitHub 的 \"Authenticate\" # 现在可以请求 Claude 与 GitHub 交互 &gt; \"审查 PR #456 并提出改进建议\" &gt; \"为我们刚刚发现的 Bug 创建一个新 issue\" &gt; \"显示分配给我的所有开放 PR\" 案例 3：查询 PostgreSQL 数据库 # 添加带有连接字符串的数据库服务器 claude mcp add --transport stdio db -- npx -y @bytebase/dbhub \\ --dsn \"postgresql://readonly:pass@prod.db.com:5432/analytics\" # 自然地查询数据库 &gt; \"我们这个月的总收入是多少？\" &gt; \"显示 orders 表的 schema\" &gt; \"找到 90 天内没有购买的客户\" 安全提示：对于生产数据库，我通常配置一个只读账户 (Read-only User) 给 MCP Server，以防止 AI 意外执行 DROP TABLE 等毁灭性操作。 案例 4：使用 MCP 资源 MCP 服务器可以暴露我可以使用 @ 提及来引用的资源，类似于引用文件的方式。 # 引用特定资源 &gt; 可以分析 @github:issue://123 并建议修复方案吗？ &gt; 请查看 @docs:file://api/authentication 的 API 文档 # 在单个提示中引用多个资源 &gt; 比较 @postgres:schema://users 和 @docs:file://database/user-model MCP 提示作为命令 MCP 服务器可以暴露在 Claude Code 中可作为命令使用的提示。 执行 MCP 提示 # 在提示中发现可用的命令 &gt; / # 无参数执行提示 &gt; /mcp__github__list_prs # 带参数执行提示 &gt; /mcp__github__pr_review 456 &gt; /mcp__jira__create_issue \"登录流程中的 Bug\" 高 MCP 提示从连接的服务器动态发现 参数根据提示的已定义参数解析 提示结果直接注入到对话中 服务器和提示名称已规范化（空格变为下划线） 认证远程 MCP 服务器 许多基于云的 MCP 服务器需要认证。Claude Code 支持 OAuth 2.0 进行安全连接。 # 添加需要认证的服务器 claude mcp add --transport http sentry https://mcp.sentry.dev/mcp # 在 Claude Code 中使用 /mcp 命令 &gt; /mcp # 在浏览器中按照步骤登录 认证令牌安全存储并自动刷新 使用 /mcp 菜单中的 “清除认证” 来撤销访问 如果你的浏览器没有自动打开，复制提供的 URL OAuth 认证适用于 HTTP 服务器 使用预配置的 OAuth 凭据 某些 MCP 服务器不支持自动 OAuth 设置。如果看到类似 “不兼容的认证服务器：不支持动态客户端注册” 的错误，则服务器需要预配置的凭据。首先通过服务器的开发者门户注册一个 OAuth 应用，然后在添加服务器时提供凭据。 # 使用 --client-id 传递应用的客户端 ID # --client-secret 标志提示输入带有掩码输入的密钥 claude mcp add --transport http \\ --client-id your-client-id --client-secret --callback-port 8080 \\ my-server https://mcp.example.com/mcp MCP 输出限制和警告 当 MCP 工具产生大型输出时，Claude Code 帮助管理 token 使用以防止使对话上下文不堪重负： 输出警告阈值：当任何 MCP 工具输出超过 10,000 个 token 时，Claude Code 显示警告 可配置限制：可以使用 MAX_MCP_OUTPUT_TOKENS 环境变量调整允许的最大 MCP 输出 token 默认限制：默认最大值为 25,000 个 token # 为 MCP 工具输出设置更高的限制 export MAX_MCP_OUTPUT_TOKENS=50000 claude 这对于使用以下 MCP 服务器特别有用： 查询大型数据集或数据库 生成详细报告或文档 处理大量日志文件或调试信息 动态工具更新 Claude Code 支持 MCP list_changed 通知，允许 MCP 服务器动态更新其可用的工具、提示和资源，而无需断开并重新连接。当 MCP 服务器发送 list_changed 通知时，Claude Code 自动从该服务器刷新可用的功能。 使用 JSON 配置添加 MCP 服务器 如果我有 MCP 服务器的 JSON 配置，可以直接添加： # 基本语法 claude mcp add-json &lt;name&gt; '&lt;json&gt;' # 示例：添加带有 JSON 配置的 HTTP 服务器 claude mcp add-json weather-api '{\"type\":\"http\",\"url\":\"https://api.weather.com/mcp\",\"headers\":{\"Authorization\":\"Bearer token\"}}' # 示例：添加带有 JSON 配置的 stdio 服务器 claude mcp add-json local-weather '{\"type\":\"stdio\",\"command\":\"/path/to/weather-cli\",\"args\":[\"--api-key\",\"abc123\"],\"env\":{\"CACHE_DIR\":\"/tmp\"}}' # 示例：添加带有预配置 OAuth 凭据的 HTTP 服务器 claude mcp add-json my-server '{\"type\":\"http\",\"url\":\"https://mcp.example.com/mcp\",\"oauth\":{\"clientId\":\"your-client-id\",\"callbackPort\":8080}}' --client-secret 从 Claude Desktop 导入 MCP 服务器 如果已经在 Claude Desktop 中配置了 MCP 服务器，可以导入它们： # 基本语法 claude mcp add-from-claude-desktop 这将显示一个交互式对话框，允许选择要导入的服务器。 此功能仅在 macOS 和 Windows Subsystem for Linux (WSL) 上可用 它从这些平台上的标准位置读取 Claude Desktop 配置文件 使用 --scope user 标志将服务器添加到用户配置 导入的服务器将与 Claude Desktop 中的名称相同 实战心得 在使用 MCP 的过程中，我深刻体会到： 去中心化：我不需要等待 Anthropic 官方支持每一个工具。只要有 API，我就可以写一个 MCP Server 把它接进来。 上下文动态化：以前我需要手动把文件贴给 Claude，现在它通过 Resource 订阅，能在文件/数据变更时自动感知。 安全性隔离：MCP Server 运行在独立的进程中。即使 AI 试图越权，它也被限制在 MCP Server 定义的工具集内，无法触碰操作系统的其他部分。 跨项目一致性：通过 project 作用域，我可以为团队配置统一的 MCP 工具集，确保所有人使用相同的工具和配置。 灵活作用域：local、project 和 user 三个作用域让我可以根据需求选择最合适的配置级别，平衡个人使用、团队协作和跨项目共享的需求。 结语 MCP Server 是 Claude Code 从”文本处理工具”进化为”全能数字助手”的关键拼图。通过连接 GitHub、数据库和各类内部系统，我让 AI 真正融入了我的研发工作流闭环。 它不再是一个孤立的聊天窗口，而是成为了系统的一部分。 更多资源： MCP 官方文档 GitHub 上的 MCP 服务器仓库 下一篇预告：Claude Code(九)Plugins：扩展功能的无限可能" }, { "title": "Claude Code(七)Subagents：构建分工协作的多智能体系统", "url": "/posts/claude-code-subagents/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent, Multi-agent", "date": "2026-01-19 23:01:09 +0800", "content": "随着我对 Claude Code 使用的深入，我发现一个问题：当任务变得过于复杂时，单一的通用 Agent 开始显露出疲态。 比如，当我要求它”重构整个后端 API 并同步更新前端组件和文档”时，它往往会顾此失彼——要么改了后端忘了前端，要么更新了代码忘了文档。这是因为通用大模型的注意力（Attention）是有限的，试图让一个大脑同时处理三个截然不同的领域（SQL、React、Markdown），很容易导致幻觉和遗漏。 这促使我开始探索 Subagents（子智能体） 机制。这不仅仅是功能的堆叠，而是组织架构的变革，标志着我们将从随性的 “Vibe Coding” 转向更严谨的 “Spec-Driven Development” (规格驱动开发)。 Core Principles：为什么需要 Subagents？ 在软件团队中，我们不会让一个人既做产品经理，又做后端架构师，还兼职前端切图。专业分工是提高效率的根本。 Subagent 的核心逻辑也是如此：将复杂任务解耦，分配给拥有特定上下文和工具的专家。 单体 vs 多智能体 维度 单体 Agent (Generalist) 多智能体系统 (Subagents) 上下文 试图装入整个项目的所有细节 独立上下文窗口（每个 Agent 可达 200k Tokens），互不污染 Prompt 冗长的通用指令 精简的专用指令（Expert Persona） 工具集 访问所有工具（风险高） 仅访问特定工具（如 DB Agent 只能用 SQL 工具） 执行模式 串行阻塞 支持 并行/后台执行 (Ctrl + B) Architecture：编排与分发 在 Claude Code 的 Subagent 架构中，我观察到它采用的是 Orchestrator-Workers（指挥官-工兵） 模式。 graph TD User[\"我\"] --&gt; Main[\"主 Agent Orchestrator\"] Main -- 任务分发 --&gt; FE[\"前端专家 Subagent A\"] Main -- 任务分发 --&gt; BE[\"后端专家 Subagent B\"] Main -- 任务分发 --&gt; QA[\"测试专家 Subagent C\"] subgraph Context_A[\"前端上下文\"] FE --&gt; React[\"React 组件库\"] FE --&gt; CSS[\"Tailwind 配置\"] end subgraph Context_B[\"后端上下文\"] BE --&gt; DB[\"Schema 定义\"] BE --&gt; API[\"Controller 代码\"] end FE -- 交付代码 --&gt; Main BE -- 交付接口 --&gt; Main QA -- 交付报告 --&gt; Main Main -- 最终整合 --&gt; User Main Agent：作为入口，它负责理解我的高层意图（Intent），规划任务路径，并决定调用哪个 Subagent。它就像是一个 Technical Lead。 Subagent：作为执行者，它们拥有独立的 System Prompt 和 Context Window。它们看不见彼此的工作细节，只通过 Main Agent 交换必要的信息。 Practical Implementation：定义我的专家团队 Claude Code 允许我通过配置文件定义 Subagent。 目录结构 通常我会建立 .claude/agents/ 目录： .claude/agents/ ├── sql-optimizer.md # SQL 优化专家 ├── frontend-dev.md # 前端开发专家 └── design-enforcer.md # 设计系统执行者 定义专家 1: SQL Optimizer (特定领域专家) 在 sql-optimizer.md 中，我这样定义： --- name: sql-expert description: 专门负责 SQL 查询优化和 Schema 设计的专家 model: claude-3-opus-20240229 tools: [read_file, execute_sql] # 限制只能用这些工具 --- 你是一个拥有 20 年经验的数据库管理员 (DBA)。 你的任务是分析慢查询并提供优化建议。 ## 核心原则 1. **索引优先**：总是检查 WHERE 子句中的列是否有索引。 2. **避免全表扫描**：严禁 SELECT *。 3. **执行计划**：在修改前，必须先使用 EXPLAIN 分析。 ## 上下文 只关注 `src/database/schema.sql` 和 `src/queries/` 目录。 示例Game Performance Profiler (游戏性能分析专家) 性能优化是游戏开发的核心需求，这个 Subagent 专门用于分析 Unity Profiler 数据，定位性能瓶颈。 --- name: game-profiler description: 游戏性能分析专家，分析 Unity Profiler 数据，定位 CPU/GPU/内存瓶颈。 使用场景：帧率下降、内存泄漏、CPU 占用过高、GC 过于频繁。 model: opus color: red tools: [Read, Write, Edit, Bash] --- 你是一位游戏性能优化专家，精通 Unity Profiler 各模块的使用。 **Core Responsibilities:** 1. CPU 分析 - 识别高耗时函数（超过 1ms/帧） - 检查 Physics.FixedUpdate 频率 - 分析脚本执行时间分布 2. Rendering 分析 - 识别 Draw Calls 和 SetPass Calls - 检查 Overdraw（透明物体重叠渲染） - 分析 Shadow cascade 配置 3. Memory 分析 - 识别内存泄漏（持续增长的 Texture/Mesh 数量） - 分析 GC Alloc 来源 - 检查 Mono Heap 使用情况 4. Physics 分析 - 识别碰撞体过多的问题 - 检查 Rigidbody 设置是否合理 - 分析物理计算开销 **Analysis Process:** 1. 读取 Profiler 导出数据（.csv 或 .json） 2. 识别性能异常区间 3. 按类型（CPU/GPU/Memory）分类问题 4. 提供优化建议 **Output Format:** 1. 性能摘要（平均 FPS、最低 FPS、主要瓶颈） 2. 问题列表（按严重程度排序） 3. 优化建议（具体到代码/资源修改） 4. 预估性能提升幅度 **Critical Rules:** - NEVER 建议使用 #pragma warning disable 来隐藏警告 - ALWAYS 先验证优化不会破坏功能 - 对于复杂的优化，建议分步骤实施 &lt;example&gt; Context: Game drops to 15 FPS on mid-range devices user: \"帮我分析一下为什么帧率这么低\" assistant: \"I'll use the game-profiler agent to analyze the performance data and identify the bottlenecks.\" &lt;/example&gt; 调用流程 当我在主对话框中输入： “User 表的查询变慢了，请优化一下。” Main Agent 会分析这个请求，识别到关键词 “查询” 和 “User 表”，然后： 挂起当前状态。 唤醒 sql-expert。 将任务传递给 sql-expert。 sql-expert 加载它的专用 Prompt，读取数据库文件，给出优化方案。 sql-expert 退出，将结果返回给 Main Agent。 Main Agent 向我汇报：”已优化 User 表查询，添加了联合索引…” 如何主动触发 Subagent Main Agent 的调度是语义驱动的，而非关键词匹配。要让它正确调用你的 Agent： 在 description 中使用场景化描述而非简单列表 包含典型对话示例（&lt;example&gt; 块） 明确 Agent 的输入输出契约 Step-by-Step：如何创建一个 Subagent 在深入实战应用之前，让我先梳理一下 Subagent 的完整创建流程。经过多次实践，我发现创建一个高质量的 Subagent 其实非常直观——它本质上就是一个 Markdown 文件。 文件结构解析 Claude Code 的 Subagent 定义非常简洁：一个 Markdown 文件，包含 YAML Frontmatter 和 System Prompt 两部分。 .claude/agents/ └── my-specialist.md Anatomy of an Agent（Agent 解剖学） 让我们从一个完整的例子开始，拆解每个部分的作用： --- name: code-refactor-master description: 专门负责代码重构的专家，用于重组文件结构、拆分大组件、更新导入路径等 model: opus color: cyan --- You are the Code Refactor Master, an elite specialist in code organization... ## Core Responsibilities 1. File Organization &amp; Structure 2. Dependency Tracking &amp; Import Management 3. Component Refactoring ## Your Refactoring Process 1. Discovery Phase 2. Planning Phase 3. Execution Phase 4. Verification Phase ## Critical Rules - NEVER move a file without first documenting ALL its importers - ALWAYS maintain backward compatibility Frontmatter：配置元数据 YAML Frontmatter 定义了 Agent 的基本身份和能力： 字段 必填 说明 示例 name ✅ Agent 的唯一标识符，用于在对话中引用 code-refactor-master description ✅ 详细描述 Agent 的用途和适用场景，包含使用示例 用于帮助主 Agent 识别何时调用此 Agent model ❌ 指定使用的模型 opus, sonnet, haiku, 或 inherit（继承主 Agent 模型） color ❌ UI 中显示的颜色标识 cyan, blue, green tools ❌ 限制可用的工具列表 [Read, Write, Edit, Bash] 实践心得：description 字段至关重要，它不仅是给人类看的，更是主 Agent 决策的依据。我在编写时会刻意包含 &lt;example&gt; 块，描述典型的使用场景和触发条件。 编写有效的 Agent Description 一个好的 description 应该回答三个问题： 这个 Agent 做什么？（核心职责） 什么时候调用它？（触发条件） 和其他 Agent 有什么区别？（边界界定） System Prompt：塑造 Agent 的行为 这是 Agent 的”大脑”，决定了它如何思考和执行。一个高质量的 System Prompt 通常包含以下结构： 角色定义（Persona） You are the Code Refactor Master, an elite specialist in code organization, architecture improvement, and meticulous refactoring. 明确 Agent 的身份，使用专业术语来界定它的能力边界。 核心职责（Core Responsibilities） **Core Responsibilities:** 1. File Organization &amp; Structure - Analyze existing file structures - Create logical directory hierarchies - Establish clear naming conventions 2. Dependency Tracking &amp; Import Management - Document every single import before moving files - Update all import paths systematically 使用嵌套列表结构，清晰地定义 Agent 的能力范围。 工作流程（Process） 这是 Agent 执行任务的”剧本”，确保它按照正确的方式工作： **Your Refactoring Process:** 1. Discovery Phase - Analyze current file structure - Map all dependencies - Document anti-patterns 2. Planning Phase - Design new organizational structure - Create dependency update matrix 3. Execution Phase - Execute in atomic steps - Update imports immediately 4. Verification Phase - Verify all imports resolve - Ensure no functionality broken 关键规则（Critical Rules） 使用强语气词（NEVER, ALWAYS）来强调不可违背的约束： **Critical Rules:** - NEVER move a file without first documenting ALL its importers - NEVER leave broken imports in the codebase - ALWAYS maintain backward compatibility 约束的艺术 Agent 的约束密度决定了其可靠性上限： NEVER 用于安全边界（防止破坏性操作） ALWAYS 用于质量底线（确保输出一致性） 避免过度约束，否则会降低 Agent 的灵活性 输出格式（Output Format） 定义 Agent 返回结果的标准格式： **Output Format:** When presenting refactoring plans, you provide: 1. Current structure analysis 2. Proposed new structure 3. Complete dependency map 4. Step-by-step migration plan 定义明确的交付契约 Agent 的输出格式决定了它是否能无缝集成到你的工作流： 结构化输出优于自然语言（JSON/表格 &gt; 段落） 明确可执行步骤而非模糊建议 提供验证标准（如何判断成功） 包含回滚方案（高风险操作必选） 好的输出 = 主 Agent 可以直接执行，不需要进一步澄清 创建流程：从零到一 让我用一个实际案例来演示完整的创建过程。 场景：我需要一个”文档编写专家” 我发现自己经常需要为新功能编写文档，但主 Agent 的文档质量参差不齐——有时太简略，有时又太冗长。我决定创建一个专门的 documentation-architect。 Step 1：定位存放位置 mkdir -p .claude/agents Step 2：编写 Frontmatter --- name: documentation-architect description: Use this agent when you need to create, update, or enhance documentation for any part of the codebase. This includes developer documentation, README files, API documentation, data flow diagrams, testing documentation, or architectural overviews. model: sonnet color: blue --- Step 3：设计 System Prompt 我开始思考：什么样的文档才是好文档？ 需要理解完整的上下文，不只是刚修改的文件 应该查阅现有的文档，保持风格一致 要考虑目标读者的需求 需要包含代码示例和故障排除 基于这些思考，我写出了 System Prompt： You are a documentation architect specializing in creating comprehensive, developer-focused documentation for complex software systems. **Core Responsibilities:** 1. Context Gathering: - Check memory MCP for stored knowledge - Examine /documentation/ directory - Analyze source files beyond current session 2. Documentation Creation: - Developer guides with code examples - README files with setup/troubleshooting - API documentation with endpoints and examples - Data flow diagrams and architectural overviews **Methodology:** 1. Discovery Phase: - Query memory MCP for relevant information - Scan /documentation/ and subdirectories - Identify all related source files 2. Analysis Phase: - Understand complete implementation - Identify key concepts needing explanation - Recognize patterns and edge cases 3. Documentation Phase: - Structure content logically - Write concise yet comprehensive - Include practical code examples Step 4：添加约束和输出格式 **Documentation Standards:** - Use clear technical language - Include table of contents for longer documents - Provide both quick start and detailed sections - Cross-reference related documentation **Output Format:** 1. Explain documentation strategy before creating files 2. Provide summary of context gathered 3. Suggest documentation structure and get confirmation 4. Create documentation that developers will want to read Step 5：测试和迭代 在主对话中测试： “Use the documentation-architect agent to document the new authentication flow” 观察返回结果，如果发现： 文档太泛泛 → 在 Prompt 中增加 “Include specific code examples” 约束 文档风格不一致 → 添加 “Match existing documentation style in /docs/” 缺少重要信息 → 在 Discovery Phase 中增加检查点 Agent 的成长曲线 第一个版本永远不会完美，迭代是必经之路： | 迭代阶段 | 关注点 | 典型调整 | |———-|——–|———-| | V1 | 基本可用 | 修复明显错误，补充遗漏职责 | | V2 | 稳定性 | 添加约束，优化输出格式 | | V3 | 鲁棒性 | 处理边缘情况，完善错误处理 | 每次迭代后记录变化，这样你才能追踪 Agent 的进化轨迹。 Advanced Patterns：高级技巧 技巧 1：使用 Example 块引导主 Agent 在 description 中嵌入示例，帮助主 Agent 识别调用时机： description: Use this agent when you need to document features. &lt;example&gt; Context: User just implemented JWT authentication. user: \"I've finished implementing the auth flow. Can you document this?\" assistant: \"I'll use the documentation-architect agent to create comprehensive docs.\" &lt;commentary&gt; Since the user needs documentation for a newly implemented feature, use the documentation-architect agent. &lt;/commentary&gt; &lt;/example&gt; 技巧 2：工具限制与安全 对于高风险操作，显式限制可用工具： --- name: read-only-auditor description: Read-only code quality checker tools: [Read, Grep, Glob] # 限制为只读工具 --- 最小权限原则 Agent 的工具访问范围应遵循最小必要原则： 只读 Agent：[Read, Grep, Glob] 代码审查 Agent：[Read]（完全不能写） 数据库 Agent：仅限 SQL 执行工具 工具越少，失误的代价越小。宁可多做几次确认，也不要给一个 Agent 赋予 “核武器”。 技巧 3：版本特定的知识 如果 Agent 需要了解特定版本的技术栈，在 Prompt 中明确： You are an expert in React 19 with TypeScript and TanStack Router. - Use React 19's new hooks (use, useOptimistic) - Follow TanStack Router's file-based routing patterns - Apply MUI v7/v8 sx prop patterns 技巧 4：返回父进程的协议 对于需要审批的 Agent，定义返回给主 Agent 的协议： **Return to Parent Process:** - Inform parent: \"Code review saved to: ./dev/active/[task-name]/code-review.md\" - Include brief summary of critical findings - IMPORTANT: Explicitly state \"Please review findings and approve before I proceed\" - Do NOT implement fixes automatically 设计优雅的交接协议 Subagent 与主 Agent 之间的接口契约是协作效率的关键： 摘要层：高层总结（3-5 句话） 详细层：分项列出（表格/列表） 证据层：引用具体文件和代码位置 好的交接 = 主 Agent 能在 10 秒内理解结果并继续工作 模板速查表 为了快速上手，我总结了一个通用模板： --- name: [agent-name] description: [Detailed description with usage examples] model: [opus|sonnet|haiku|inherit] color: [cyan|blue|green|yellow] tools: [Read, Write, Edit, Bash] # Optional --- You are a [role], a specialist in [domain]. **Core Responsibilities:** 1. [Responsibility 1] - Detail A - Detail B 2. [Responsibility 2] - Detail A **Your Process:** 1. [Phase Name] - Step 1 - Step 2 2. [Phase Name] - Step 1 **Critical Rules:** - NEVER [prohibited action] - ALWAYS [required action] **Output Format:** When completing tasks, provide: 1. [Output type 1] 2. [Output type 2] 创建 Subagent 的本质是：将领域知识、工作流程和质量标准封装成一个可复用的指令集。一旦你写好一个专家 Agent，它就像雇了一个永远在线、不知疲倦的专家团队成员。 实战心得：什么时候用 Subagent？ 并不是所有任务都需要 Subagent。经过多次实践，我总结了以下标准： Subagent 判定矩阵 在决定是否创建 Subagent 前，问自己： | 维度 | 低复杂度任务 | 高复杂度任务 | |——|————-|————-| | 领域专精 | 通用模型足以应对 | 需要领域专家知识 | | 安全风险 | 操作可撤销 | 有破坏性后果 | | 上下文密度 | 单文件/局部修改 | 全局依赖分析 | | 可复用性 | 一次性需求 | 将重复使用 | 满足 2+ 项？→ 创建 Subagent 全部否决？→ 直接让主 Agent 处理 ❌ 不用 Subagent： 简单的 Bug 修复。 单文件的逻辑修改。 跨度很小的任务。 理由：切换 Agent 有上下文切换成本（Context Switching Cost），杀鸡焉用牛刀。 ✅ 使用 Subagent： 独立性强的模块：如完全独立的微服务，或与业务逻辑分离的 UI 组件库。 高风险操作：如数据库迁移，我希望限制 Agent 只能运行只读命令，不能运行 DROP TABLE。通过 Subagent 的权限管控可以完美实现。 特定领域知识：如编写复杂的正则，或撰写符合特定法律法规的条款。 上下文密集型任务：如”重构整个项目目录结构”，这类任务需要大量搜索和依赖分析，如果放在主对话中会瞬间撑爆上下文。Subagent 在独立环境中运行，只返回最终报告。 Advanced Configuration：深度定制 除了基础的 Prompt 定义，Claude Code 还支持更高级的配置： 权限与安全模型 (Security Model) 在 CLI 环境中，安全至关重要。我们可以通过 permissionMode 精确控制 Subagent 的行为边界： default：标准模式，敏感操作（写文件、Shell 命令）会请求用户批准。 bypassPermissions：慎用。跳过所有权限检查，适合完全受信任的自动化脚本。 readOnly 模式：通过 disallowedTools: [Write, Edit] 强制创建一个只读的”审计员” Agent。 记忆与学习 (Persistent Memory) 给 Agent 装上”海马体”。通过 memory 字段，我们可以让 Agent 记住跨 Session 的知识。 工作原理 Persistent Memory 的机制是”自动加载，主动保存”： memory: project # 将记忆存储在 .claude/projects/.../memory/ 下 读取（自动）：当 Agent 启动时，系统会将 MEMORY.md 的内容自动注入到它的 System Prompt 中。这就像每次会议前都给 Agent 一张写满之前经验的小抄。 保存（主动）：Agent 需要通过 Write 或 Edit 工具显式地更新 MEMORY.md。如果你没有在 Prompt 中告诉它”把学到的东西记下来”，它可能”心里明白”但下次就忘了。 实践配置 在 System Prompt 中添加指令： --- name: sql-expert description: SQL 优化专家 memory: project --- 你是一个数据库专家。每次你学到新的优化经验或发现特定的数据库特性时， 请将总结写入 `MEMORY.md` 文件，以便下次会话时使用。 **记忆格式示例：** ## Project-specific Knowledge ### User Table Optimization - User 表的 email 列查询频繁，确保有索引 - 避免在 User 表上使用 JOIN，考虑分表策略 重要提醒：Memory 不是自动记录所有对话的”黑匣子”。它是 Agent 的”笔记本”——需要你明确告诉 Agent “把这条信息记下来”。 让 Agent 学会”记笔记” Persistent Memory 的价值取决于写入质量： 使用结构化格式（标题、分类、要点） 记录反模式和陷阱（比正确方案更有用） 包含可执行的具体信息（而非抽象总结） 定期回顾和清理过时知识 Agent 的成长速度 ≈ 你的提示频率 × 记录质量 记忆存储位置 .claude/ └── projects/ └── [project-id]/ └── memory/ └── MEMORY.md 这意味着如果 sql-expert 昨天学到了”User 表不能用这个索引”，今天它依然记得——前提是昨天的对话中它把这条经验写进了 MEMORY.md。这对于长期维护的项目至关重要。 生命周期钩子 (Lifecycle Hooks) 我们可以定义 PreToolUse 钩子来做更细粒度的控制。例如，在执行任何 SQL 之前，先运行一个脚本检查是否包含 DROP 或 TRUNCATE 关键字： 自动化验证层 Hooks 是 Agent 的最后一道防线，可用于： PreToolUse：操作前验证（如 SQL 语法检查） PostToolUse：操作后审计（如记录文件修改） PreAgentLaunch：Agent 启动前环境检查 Hook 不应该替代 Prompt 约束，而是作为补充的安全网。 hooks: PreToolUse: - matcher: \"Bash\" hooks: - type: command command: \"./scripts/validate-sql-safety.sh\" 生产力技巧 (Pro Tips) 后台执行：通过 Ctrl + B 可以让 Subagent 在后台运行。这对于运行耗时的测试套件或全库扫描非常有用，你可以继续在主线程做其他事情。 自然语言配置：你可以直接告诉 Claude “创建一个专门写文档的 Agent”，它会帮你生成配置文件，无需手动编写 YAML。 Case Study: 隔离式 Bug 修复 这是 Subagent 最强大的使用场景之一：将”调试过程”封装在子线程，只将”修复结果”返回给主线程。这既能节省 Token，又能保持主上下文的整洁。 场景描述 我在主 Agent 中开发一个新功能，遇到了一个复杂的报错。如果让主 Agent 直接尝试修复，可能会浪费 10-20 轮对话在试错上，而且这些试错记录会污染主上下文，让后续对话变得臃肿。 解决方案：委托给专门的 Subagent 我创建了一个 auto-error-resolver 专门处理 TypeScript 编译错误： --- name: auto-error-resolver description: Automatically fix TypeScript compilation errors tools: Read, Write, Edit, MultiEdit, Bash --- You are a specialized TypeScript error resolution agent. ## Your Process: 1. **Check for error information**: - Read error cache at: `~/.claude/tsc-cache/[session_id]/last-errors.txt` - Get TSC commands at: `~/.claude/tsc-cache/[session_id]/tsc-commands.txt` 2. **Analyze the errors**: - Group by type (missing imports, type mismatches, etc.) - Prioritize errors that might cascade 3. **Fix errors**: - Start with import errors - Then fix type errors - Use MultiEdit for similar issues 4. **Verify**: - Run the correct tsc command after changes - Report success when all resolved ## Return Protocol: - Summarize what was fixed (file + issue type) - Do NOT include detailed debug steps in return message - Just report: \"Fixed 5 type errors in src/components/*.tsx\" 执行流程 graph LR A[\"主 Agent 开发功能\"] --&gt; B[\"遇到 Bug\"] B --&gt; C[\"调用 auto-error-resolver Subagent\"] C --&gt; D[\"Subagent 独立调试\"] D --&gt; E[\"多轮试错与修复\"] E --&gt; F[\"Subagent 完成任务\"] F --&gt; G[\"返回简短结果给主 Agent\"] G --&gt; H[\"主 Agent 继续开发\"] style D fill:#ff9999 style E fill:#ffcccc style G fill:#99ff99 实际对话 在主对话中： 我：这个新功能有个 TypeScript 报错，请用 auto-error-resolver agent 去修复它，不要在当前会话中尝试。 主 Agent：I’ll use the auto-error-resolver agent to fix these TypeScript errors. [主 Agent 调用 Task 工具，进入等待状态] 在 Subagent 对话中（独立上下文）： Subagent：Found 12 TypeScript errors in frontend service… Error 1: Property ‘onClick’ does not exist… Error 2: Module not found… [Subagent 进行搜索、分析、修改、验证，可能需要 10 轮对话] Subagent：Fixed all errors. Modified 5 files: src/components/Button.tsx: Added onClick to ButtonProps src/services/api.ts: Fixed import path 回到主对话： 主 Agent：auto-error-resolver has completed. Fixed 5 type errors in the frontend service. The build now passes. 我：太好了，继续完成这个功能的 UI 部分。 关键优势 维度 主 Agent 直接修复 Subagent 委托修复 Token 消耗 试错过程全部进入主上下文，持续累积 试错过程留在 Subagent，主上下文只有一行调用 上下文污染 大量报错和试错记录影响后续对话 主上下文保持干净，只保留最终结果 注意力聚焦 主 Agent 需要频繁切换任务模式 主 Agent 专注于高层逻辑，Subagent 专注调试 可追溯性 难以区分哪些是开发内容，哪些是调试过程 调试历史独立记录，便于回顾 返回协议设计 对于这种用例，Subagent 的返回格式至关重要。我建议遵循以下原则： **Return Protocol:** DO: - Summarize what was fixed (high-level) - List affected files - Confirm verification passed DON'T: - Include detailed error messages - Show step-by-step debug process - Return raw tsc output Example Return: \"Fixed 5 type errors: - src/components/Button.tsx: Added missing onClick property - src/utils/helpers.ts: Fixed import path Build verified with: npx tsc --noEmit ✓\" 这样，主 Agent 收到的是一条简洁的”任务完成通知”，而不是整个调试过程的转储。 Case Study: The “Refactor Master” 让我们看一个终极案例：Code Refactor Master。重构是 LLM 最容易翻车的场景，因为它需要全局视野和极高的严谨性。 我定义了一个基于 Opus 模型的重构专家，它的工作流被严格限制为四步： Discovery：扫描依赖，建立引用图谱（Dependency Map）。 Planning：输出详细的迁移计划，包含所有受影响的文件列表。 Execution：原子化操作，移动一个文件立即更新所有 Import。 Verification：运行构建检查，确保无悬挂引用。 这比直接让主 Agent “把这个文件夹整理一下” 要靠谱得多，因为它的 System Prompt 里写死了：”NEVER move a file without first documenting ALL its importers“。 结语 Subagents 是 AI 迈向”群体智能”的第一步。通过构建这套分工体系，我实际上是在组建一个虚拟的软件开发团队。 在这个团队中，我不再是唯一的编码者，而是架构师和决策者。我定义每个 Agent 的职责边界（Role）和交互协议（Protocol），让它们协同工作，从而突破了单体模型的智力上限。 下一篇预告：Claude Code(八)MCP Server：连接万物的通用协议" }, { "title": "Claude Code(六)AgentSkills：构建高可信 AI 技能系统", "url": "/posts/claude-code-agent-skills/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent, Best Practices", "date": "2026-01-17 19:21:00 +0800", "content": "在上一章中，我介绍了由用户主动触发的 Slash Commands（斜杠命令）。但在更高级的自动化场景中，我们需要 AI 能够根据上下文自动判断何时介入并提供专业能力。 这就是 Agent Skills（技能） 的核心价值。 随着 2025 年底 Anthropic 正式将 Agent Skills 发布为开放标准 (agentskills.io)，这套机制已不仅仅是 Claude 的独门绝技，正逐渐成为 Cursor、OpenCode 等 AI 编程工具通用的”能力扩展协议”。 本文将基于行业标准与实战经验，深度剖析如何构建高可信、高性能的 Agent Skills。 Background / Problem 为什么我们需要 Agent Skills？ 在使用通用 LLM 进行软件开发时，我经常遇到三个痛点： 领域知识缺失：AI 不知道团队特定的代码规范、架构模式或数据库设计。 上下文过载：如果把所有规范都塞进 System Prompt，Token 消耗巨大且容易造成 AI 注意力分散。 执行不可靠：AI 生成的复杂操作指令容易出错，缺乏自我验证机制。 Agent Skills 通过模块化设计解决了这些问题。它允许我们定义特定的能力包，AI 只有在识别到相关意图时才会加载，并能严格遵循预定义的工具链和验证逻辑。 更重要的是，Agent Skills 正在像 LSP (Language Server Protocol) 一样标准化。掌握这套定义方法，意味着你构建的技能包未来可以在不同的 AI 宿主环境中通用。 Core Principles 核心原理与设计哲学 技能生态位：Prompt vs Skill vs MCP 在构建 AI 辅助系统时，很多开发者容易混淆 Skill 与 Prompt 或 MCP 工具的区别。我们可以用一个形象的比喻来厘清三者的生态位： 概念 比喻 定义与作用 Prompt 口令 一次性的、临时的交互指令。解决“当下做什么”。 Agent Skill 大脑/内功 “怎么做”的知识与流程。它包含最佳实践、规范文档、检查清单。它运行在 AI 的上下文中，指导 AI 如何思考和规划。 MCP 双手/工具 “用什么做”的能力。它负责连接外部世界（读数据库、操作 Git、调 API）。Skills 经常会调用 MCP 工具来执行具体操作。 协同工作流： flowchart LR User([\"User Prompt\"]) --&gt;|Trigger| Claude{\"Intent Analysis\"} Claude --&gt;|Load| Skill[\"Agent Skill&lt;br/&gt;(Protocol &amp; Knowledge)\"] Skill --&gt;|Execute| MCP[\"MCP Tools&lt;br/&gt;(Git/FS/API)\"] MCP --&gt;|Result| Claude Claude --&gt;|Response| User 用户发出指令 (Prompt) -&gt; AI 激活内功 (Skill) 规划路径 -&gt; 驱动双手 (MCP) 完成任务。 Model-Invoked vs. User-Invoked 与 Slash Command 不同，Skill 是 Model-Invoked（模型调用） 的。 Slash Command: 用户输入 /fix -&gt; 触发命令。显式、可控。 Agent Skill: 用户输入 “帮我重构这个模块” -&gt; Claude 分析意图 -&gt; 检索 Skill 库 -&gt; 发现匹配的 Refactor Skill -&gt; 自动挂载并执行。 这种机制要求 Skill 的描述（Description）必须极其精准，采用第三人称视角编写（如 “当用户请求重构代码时使用…“），以便语义检索引擎能正确匹配。 渐进式披露 (Progressive Disclosure) 这是高性能 Skill 设计的核心模式。Claude 的上下文窗口是宝贵资源。 flowchart TD subgraph Context[Context Window] L1[L1: SKILL.md Index] end subgraph Storage[File System] L2[L2: Knowledge Docs] L3[L3: Execution Scripts] end User[User Query] --&gt;|Match| L1 L1 -.-&gt;|Lazy Load| L2 L1 -.-&gt;|Execute| L3 style L1 fill:#f9f,stroke:#333 style L2 fill:#bbf,stroke:#333 style L3 fill:#bfb,stroke:#333 错误做法：在 SKILL.md 中写下几千行的详细规范。一旦加载，这些 Token 会一直占据上下文。 最佳实践： L1 索引层 (SKILL.md)：只包含元数据和导航目录。Token 占用极低。 L2 知识层 (resources/*.md)：按领域拆分的详细文档（补充文档）。 L3 执行层 (scripts/*.py)：具体的执行脚本。 AI 就像查字典一样，先看目录，再按需读取特定章节。这种架构实现了”用完即走”，极大降低了 Token 消耗和幻觉概率。 Deep Analysis 技能系统的解剖学 一个生产级的 Skill 结构通常如下（参考我的后端开发规范技能）： backend-dev-guidelines/ ├── SKILL.md # 入口与元数据 ├── resources/ # [L2] 按需加载的知识库 │ ├── architecture.md # 架构概览 │ ├── database-patterns.md # 数据库设计模式 │ └── error-handling.md # 错误处理规范 └── scripts/ # [L3] 确定性执行脚本 ├── validate_schema.py └── generate_scaffold.py SKILL.md：关键的入口 SKILL.md 的 YAML Frontmatter 中的 description 决定了技能的可见性。 --- name: backend-guidelines description: 当用户涉及后端架构设计、数据库迁移或 API 开发任务时激活。提供架构模式和代码规范指导。 allowed-tools: [Read, Grep, Bash] # 权限沙箱 --- # Backend Development Guidelines ## 知识索引 - **架构设计**：查看 [resources/architecture.md] - **数据库规范**：查看 [resources/database-patterns.md] - **错误处理**：查看 [resources/error-handling.md] ## 常用工具 - 验证 Schema：运行 `python scripts/validate_schema.py` 注意 allowed-tools 字段。对于只读性质的技能（如代码审查），我会限制它只能使用 Read 和 Grep，通过权限沙箱防止 AI 意外修改代码。 资源文件的按需加载 当 Claude 决定查阅数据库规范时，它会执行 Read resources/database-patterns.md。这个动作是动态的。如果用户只问了 API 接口，Claude 就永远不会去读数据库规范，从而保持上下文纯净。 Practical Implementation 实战：构建”确定性”的代码审查技能 单纯让 AI “Review 代码” 往往得到泛泛而谈的建议。为了保证质量，我构建了一个基于 “计划-验证-执行” (Plan-Verify-Execute) 闭环的 Review Skill。 步骤 1：定义技能结构 mkdir -p .claude/skills/code-reviewer touch .claude/skills/code-reviewer/SKILL.md mkdir .claude/skills/code-reviewer/scripts 步骤 2：编写脚本 (scripts/check_style.sh) 我不仅依赖 AI 的眼睛，更依赖脚本的确定性。 #!/bin/bash # 检查是否存在 console.log 或 TODO grep -r \"console.log\" $1 &amp;&amp; echo \"Error: Found console.log\" grep -r \"TODO\" $1 &amp;&amp; echo \"Warning: Found TODOs\" # 运行项目的 linter npm run lint --silent 步骤 3：编写 SKILL.md 将脚本集成到 AI 的工作流中。为了保证执行的严谨性，我们可以设计如下的状态流转： stateDiagram-v2 [*] --&gt; StaticAnalysis StaticAnalysis --&gt; LogicReview : Pass StaticAnalysis --&gt; ReportError : Fail LogicReview --&gt; GenerateReport ReportError --&gt; GenerateReport GenerateReport --&gt; [*] --- name: strict-code-reviewer description: 在用户请求审查代码或提交 PR 前使用。执行严格的代码质量检查。 --- # Code Review Protocol ## 工作流 (Workflow) 请严格遵循以下步骤进行审查： 1. **静态分析 (Static Analysis)** 运行验证脚本获取客观数据： ```bash ./scripts/check_style.sh src/ ``` 2. **逻辑审查 (Reasoning)** 在通过静态分析后，重点关注脚本无法检测的逻辑问题： - 变量命名是否具有语义？ - 函数是否过于复杂（超过 50 行）？ - 错误处理是否完善？ 3. **生成报告** 基于上述两步，输出 Markdown 格式的审查报告。如果脚本报错，**必须**在报告首部用红色标记。 高级技巧：本地知识库集成 在我的 backend-dev-guidelines 实践中，我发现将团队的 Wiki 导出为 Markdown 并放入 resources/ 目录极其有效。 例如，当 AI 需要写一个新的 API Controller 时： 自动激活 backend-guidelines 技能。 读取 resources/routing-and-controllers.md 了解 URL 命名规范。 读取 resources/validation-patterns.md 了解参数校验写法。 生成符合团队风格的完美代码。 这比在 Prompt 中反复强调 “请使用 RESTful 风格” 要可靠得多。 Advanced Topics 最佳实践与避坑指南 脚本优先 (Scripts over Reasoning) 原则：凡是能用代码解决的，绝不让 AI “思考”。 Bad: “请检查 JSON 是否有效。” (AI 可能看走眼，导致幻觉) Good: “运行 python -m json.tool file.json 验证。” (绝对可靠) “Claude A / Claude B” 迭代法 开发复杂 Skill 时，我通常采用双模型迭代： Claude A (Architect)：负责编写 Skill 定义和脚本。 Claude B (Tester)：加载新 Skill，在真实场景中模拟用户操作。 反馈循环：观察 Claude B 的失败路径（例如找不到文件、误解指令），将 Log 反馈给 Claude A 进行修正。 避免深层嵌套 Claude 在读取文件时，如果发现嵌套引用（文件 A 引用文件 B，文件 B 引用文件 C），可能会因为懒加载策略而导致上下文丢失。 建议：保持扁平化结构。所有核心引用最好在 SKILL.md 或一级子目录中直接可达。 路径规范 始终使用正斜杠 / (Unix Style) 编写路径，即使在 Windows 上也是如此。这能保证跨平台兼容性，避免 AI 混淆转义字符。 总结 Agent Skills 是 Claude Code 从”聊天机器人”进化为”领域专家”的关键，更代表了 AI 开发工具标准化的未来方向。通过渐进式披露架构，我们解决了上下文限制；通过Skill + MCP 的组合，我们实现了思考与行动的完美协同。 当我们将团队的知识显性化为标准化的 Skills，AI 就不再是一个外包工，而是一个熟读公司手册的、可复用的资深员工。 下一篇预告：Claude Code(七)Subagents：构建多智能体协作系统" }, { "title": "Claude Code(五)斜杠命令：从“对话框”到“控制台”的生产力跃迁", "url": "/posts/claude-code-slash-commands/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent", "date": "2026-01-16 22:08:10 +0800", "content": "在前几章中，我探讨了 Claude Code 的配置与Hooks 机制。如果说 Hooks 是 AI 的”神经系统”，那么 斜杠命令（Slash Commands） 就是它的”控制面板”。 虽然我习惯用自然语言与 AI 交流，但在 CLI（命令行界面）环境中，我发现效率往往来自于确定性和简洁性。斜杠命令既包括 Claude Code 内置的系统命令，也包括我可以自定义的项目命令和个人命令。它们正是连接”模糊对话”与”精准操作”的桥梁。 Background / Problem：为什么需要斜杠命令？ 我曾经问自己：”我直接跟 Claude 说’清理会话’不就行了吗？” 虽然自然语言非常灵活，但在以下场景中，我发现斜杠命令具有不可替代的优势： 原子操作（Atomicity）：斜杠命令是由终端程序（Claude Code Client）直接解析的，不经过 LLM 处理（针对内置命令）。这意味着它响应极快，且 100% 成功，无需等待 AI 推理。 Token 经济学（Token Economy）：当我发送 /compact 时，我不需要写一段长长的指令解释为什么要压缩上下文，从而节省了输入 Token。 状态隔离（State Isolation）：某些操作（如 /login 或 /config）涉及系统敏感信息，不应该作为上下文喂给 AI。 工作流自动化（Workflow Automation）：自定义命令可以编码复杂的多步骤操作，提升重复性任务的效率。 跨项目一致性（Consistency）：项目命令（.claude/commands/）可被团队共享，确保标准化的开发流程。 核心哲学： 我将管理权留给斜杠命令，将创造权留给自然语言。 Deep Analysis：斜杠命令的执行机制 理解斜杠命令在系统中的位置，有助于我更好地使用它。斜杠命令不仅仅是文本替换，它是一个客户端路由（Client-side Router）。 执行流全景图 graph TD A[用户输入] --&gt; B{输入路由层} B -- 以 / 开头 --&gt; C{命令解析器} B -- 普通文本 --&gt; D[LLM 推理引擎] C -- 内置系统命令 --&gt; E[本地执行环境] C -- 自定义命令 --&gt; F[命令加载器] E -- \"/compact\" --&gt; G[Context Manager: 压缩历史] E -- \"/config\" --&gt; H[Config Manager: 读写配置] F --&gt; I{元数据解析} I -- disable-model-invocation: true --&gt; J[本地工具执行器] I -- 默认模式 --&gt; K[Prompt 组装器] K --&gt; L[注入 System Prompt &amp; Context] L --&gt; D G --&gt; M[CLI 界面反馈] H --&gt; M J --&gt; M D --&gt; N[生成 Tool Calls / 文本回复] N --&gt; M 关键技术点解析： 零延迟路由（Zero-latency Routing）： 内置命令（如 /clear, /cost）在本地被拦截和执行。这避免了网络往返延迟（RTT），使得操作手感如原生 Shell 命令般流畅。 上下文无污染（Context Hygiene）： 许多内置命令执行后，不会在 AI 的上下文窗口中留下痕迹（或者只留下精简的结果）。这对于维持 Agent 的长期专注力至关重要。 混合执行模式（Hybrid Execution）： 自定义命令非常灵活。我可以通过设置 disable-model-invocation: true 让它变成一个纯粹的 Bash 脚本封装器（不消耗 LLM Token），或者让它携带 Prompt 进入 LLM 进行智能处理。 Core Principles：核心命令全景图 内置系统命令（Built-in Commands） 这些命令由 Claude Code 直接提供，开箱即用。 会话控制类 /clear： 作用：彻底清空当前会话的对话历史，回到初始状态。 注意：这会擦除 AI 对当前任务的所有短期记忆，但不会删除物理文件。 /compact [instructions]： 作用：智能压缩会话历史。它会总结之前的对话，只保留关键信息和文件变更摘要。 场景：当感觉 Claude 开始变得反应迟钝，或者收到”Context window near limit”警告时使用。 /rewind： 作用：回退到之前的会话状态，同时可选地恢复代码更改。 代码审查与开发 /review： 作用：请求 Claude 对当前代码或最近的更改进行代码审查。 关注点：自动分析代码质量、最佳实践、潜在问题。 上下文与成本追踪 /context： 作用：可视化当前会话的上下文使用情况，显示各部分（对话、文件、工具调用）对 Token 的占用。 /cost： 作用：显示当前会话的 Token 消耗统计。 场景：当我需要评估一个重构任务的成本，或者优化自己的 Prompt 策略时。 配置与系统 /config：查看或修改全局配置。 /model：切换 AI 模型（如 Haiku/Sonnet/Opus）。 /init：初始化 .claude 环境。 Practical Implementation：自定义命令实战 除了内置命令，Claude Code 真正的威力在于自定义命令。通过定义 Markdown 文件，我可以将特定的 Prompt 工程固化为可复用的工具。 基础结构 自定义命令存储在 .claude/commands/（项目级）或 ~/.claude/commands/（用户级）。 --- description: 命令描述 argument-hint: [参数提示] allowed-tools: Bash(git add:*), Bash(git commit:*) # 权限控制 model: claude-3-5-sonnet-20241022 # 指定模型 --- 这里是具体的 Prompt 内容。可以使用 $1, $2 引用参数，或者使用 `@filename` 引用文件。 场景 1：Git Workflow 自动化 (/create-pr) 创建 .claude/commands/create-pr.md： --- description: Create a pull request with automatic commits argument-hint: [branch-name] [title] allowed-tools: Bash(git add:*), Bash(git commit:*), Bash(git push:*) --- ## 任务 基于当前的代码变更创建 PR。 ## 步骤 1. 分析 @. 中所有修改过的文件 2. 将相关更改按逻辑分组（特性、组件、关注点） 3. 为每个逻辑单元创建独立的 commit（遵循 Conventional Commits） 4. 推送分支到远程 5. 生成 PR 摘要 ## PR 摘要格式 ```text ## 功能描述 &lt;简要说明&gt; ## 关键变更 - &lt;列表&gt; ## 测试计划 - [ ] &lt;测试项&gt; ### 场景 2：代码质量卫士 (`/lint-custom`) 创建 `.claude/commands/lint-custom.md`： ```markdown --- description: Check code against project standards argument-hint: [file-path] --- 检查 @$1 是否符合项目编码规范： ### 检查清单 - [ ] **命名规范**：变量驼峰，类名帕斯卡 - [ ] **函数复杂度**：圈复杂度 &lt; 10 - [ ] **错误处理**：避免空的 catch 块 - [ ] **类型安全**：TypeScript 避免使用 any 输出具体的违规实例和改进后的代码片段。 场景 3：游戏开发资产管线 (/process-asset) Game Dev Context: 游戏开发中常涉及繁琐的资源导入设置或格式转换。我们可以用命令简化这一流程。 创建 .claude/commands/process-asset.md： --- description: Pre-process game assets for Unity/Unreal argument-hint: [asset-path] [target-platform] allowed-tools: Bash(magick:*), Bash(ffmpeg:*) model: claude-3-5-haiku-20241022 --- ## 资产预处理任务 你是一个技术美术（TA）助手。请处理位于 @$1 的资源文件，目标平台为 $2。 ### 处理逻辑 1. **如果是图片 (.png, .tga)**: - 检查分辨率是否为 2 的幂次（POT）。如果不是，调整为最近的 POT。 - 如果目标是 Mobile，转换为 ASTC 友好的格式（去除 Alpha 通道如果未使用）。 - 使用 ImageMagick 执行操作。 2. **如果是音频 (.wav)**: - 转换为 44.1kHz, 16-bit Mono (对于语音) 或 Stereo (对于 BGM)。 - 使用 FFmpeg 执行。 3. **生成 .meta 建议**: - 如果是 Unity 项目，分析对应的 `.meta` 文件设置是否合理（如压缩算法）。 请先输出分析结果，再执行转换命令。 Advanced Topics：高级技巧与 FAQ 1. 快捷键与别名 在 .zshrc 或 .bashrc 中，我可以结合 echo 管道来创建系统级快捷键： # 快速查看开销 alias ccost=\"echo '/cost' | claude\" 2. 区分 Slash Commands 与 Agent Skills 这是很多开发者的困惑点，两者看似相似，实则定位不同： 特性 Slash Commands (斜杠命令) Agent Skills (技能) 定位 快速指令，单次交互为主 复杂能力，包含逻辑、知识库和多步工具调用 触发方式 用户显式输入 /command AI 根据上下文自动决策调用，或通过命令触发 文件结构 单个 Markdown 文件 包含 SKILL.md、脚本、参考文档的目录结构 复杂度 适合原子操作（Git 提交、Lint 检查） 适合复杂工作流（编写整个模块、重构遗留系统） 结论：我通常用 Slash Command 封装高频的、简单的动作；用 Agent Skills 封装复杂的、需要领域知识的业务逻辑。 FAQ Q: 自定义命令可以调用其他命令吗？ A: 目前不支持直接嵌套调用（如在命令 A 中写 /command-b）。但我可以通过 Prompt 指引 AI 去按顺序执行一系列逻辑。 Q: 如何在命令中嵌入动态的系统信息？ A: 使用 Bash 插值。例如在命令文件中写：当前 Git 分支状态：!git status（注意：这是 Markdown 里的 Bash 块执行结果，或者使用反引号插值取决于具体实现版本）。Claude Code 的命令解析器通常会将 `git status` 的输出注入到 Prompt 中。 结语 斜杠命令将 Claude Code 从一个”会聊天的机器人”转变为一个”听话的终端工具”。 内置命令 让我掌控全局（Session 管理、Cost 控制）。 自定义命令 让我将个人经验固化为工具（Prompt as Code）。 熟练掌握这些短指令，能让我在代码心流中保持专注，而不必纠结于如何措辞才能让 AI 明白我的管理意图。当我的需求超越了简单的命令，需要更复杂的逻辑判断和多工具协同运作时，我就需要下一章介绍的更强大的武器——Agent Skills。 下一篇预告：Claude Code(六)AgentSkills：AI 技能系统实践详解 - 学习如何构建包含脚本、文档和复杂逻辑的 AI 技能包。" }, { "title": "Claude Code(四)Hooks及用法：构建 Agent 的确定性神经系统", "url": "/posts/claude-code-hooks/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent", "date": "2026-01-15 21:00:00 +0800", "content": "在之前的文章中，我探讨了 Claude Code 的基本交互。如果说 Prompt 是指挥 AI 的“语言”，那么 Hooks（钩子） 就是控制 AI 行为的“神经系统”。 今天，我将跳出简单的命令配置，从架构设计的角度记录我对 Hooks 的深入研究。 核心概念：概率性 AI 与确定性系统 在软件工程中，LLM（大语言模型）本质上是一个概率性系统——我给它指令，它“很可能”会执行，但不是绝对。这在严谨的工程场景（如生产环境部署、敏感文件操作）中是不可接受的。 Hooks 的出现，是为了引入确定性。 架构视角： 我将 Hooks 想象成 Web 开发中的 Middleware（中间件） 或 Git 的 Pre-commit Hooks。它们是一道道刚性的逻辑“关卡”，包裹在柔软灵活的 AI 核心之外。 graph LR A[用户输入] --&gt; B{Pre-Hooks 拦截层} B -- 注入上下文 --&gt; C[Claude AI 核心] B -- 阻断/拒绝 --&gt; D[终止操作] C --&gt; E{Post-Hooks 处理层} E -- 格式化/审计 --&gt; F[最终输出/文件变更] E -- 触发后续任务 --&gt; G[自动化 Agent] 通过这种架构，我实现了： 确定性控制：无论 AI 多么“有创意”，它都无法绕过我硬编码的安全检查。 无感上下文：不需要我手动输入背景信息，Hooks 自动在幕后“喂”给 AI。 副作用管理：将文件清理、日志记录等脏活累活从 AI 的 Token 消耗中剥离。 生命周期全景图 Claude Code 的生命周期比简单的“问答”要复杂得多，Hook 事件贯穿了从会话启动到结束的每一个微小环节。 下图展示了一个典型指令（如“帮我修改代码”）背后的 Hook 触发流： sequenceDiagram participant User participant System as Hook System participant Claude as Claude Agent participant Tool as File/Bash Tool Note over User, Claude: 阶段一：意图注入 User-&gt;&gt;System: 提交 Prompt System-&gt;&gt;System: 触发 UserPromptSubmit Note right of System: 自动检索 Skill/文档&lt;br/&gt;注入 Hidden Context System-&gt;&gt;Claude: 增强后的 Prompt Note over Claude, Tool: 阶段二：执行控制 Claude-&gt;&gt;System: 请求使用 Tool (Edit) System-&gt;&gt;System: 触发 PreToolUse/Permission Note right of System: 检查敏感文件&lt;br/&gt;安全审计 System--&gt;&gt;Claude: Allow/Deny Claude-&gt;&gt;Tool: 执行操作 Tool--&gt;&gt;System: 触发 PostToolUse Note right of System: 记录变更文件&lt;br/&gt;更新项目索引 System--&gt;&gt;Claude: 返回执行结果 Note over Claude, User: 阶段三：收尾质量门 Claude-&gt;&gt;System: 任务完成 (Stop) System-&gt;&gt;System: 触发 Stop Note right of System: 运行 TSC 检查&lt;br/&gt;运行 Prettier System-&gt;&gt;User: 最终响应 关键事件解析 事件 (Event) 核心价值 典型应用场景 SessionStart 环境初始化 加载项目特定的环境变量、检查依赖版本、恢复上次会话记忆。 UserPromptSubmit 上下文增强 (RAG) 这是实现“智能”的关键。在此处分析用户意图，悄悄注入相关的 API 文档或编码规范，而无需我显式提供。 PreToolUse 安全卫士 防止 AI 修改 .env、锁定文件或在错误的分支上提交代码。它拥有“一票否决权”。 PostToolUse 状态追踪 AI 是无状态的，但项目是有状态的。在此处记录“AI 修改了哪些文件”，为后续的测试或回滚提供依据。 Stop 质量关口 在 AI 交差之前，强制执行编译检查。如果编译失败，甚至可以拒绝 AI 的停止请求，迫使它修复错误。 实战案例：构建“工程感知”基础设施 在 claude-code-infrastructure-showcase 项目中，我构建了一套基于 Hooks 的高级基础设施。这不仅仅是配置，而是一套自动化工作流。 Skill 自动激活 (UserPromptSubmit) 最痛点的场景：我有 50 个项目的编码规范，如果全部放入 System Prompt，上下文窗口瞬间爆炸。 解决方案：动态注入。 工作原理： 我输入：“优化这个数据库查询”。 UserPromptSubmit Hook 捕获输入，正则匹配关键词 数据库 SQL。 脚本读取本地 skills/database-best-practices.md。 将内容追加到 Prompt 尾部（对用户不可见）。 这样，Claude 在处理任务时，就像临时“回忆”起了相关的专业知识。 幽灵文件追踪器 (PostToolUse) Claude 有时会忘记自己刚刚改了什么，特别是在长会话中。 解决方案：构建外部记忆。 我利用 PostToolUse 监听所有的 Write 和 Edit 操作。每当文件发生物理变更，Hook 脚本就会将文件路径记录到 .claude/session_context.json 中。 # 伪代码逻辑 if [ \"$TOOL_NAME\" == \"Edit\" ]; then echo \"$TARGET_FILE\" &gt;&gt; .claude/context/modified_files.log # 甚至可以自动触发 git add git add \"$TARGET_FILE\" fi 当我下次问“我今天改了哪些文件？”时，Claude 可以读取这个日志，精准回答，而不是靠幻觉瞎编。 闭环质量防御 (Stop) 这是从“玩具”到“工具”的质变。 在 Stop 钩子中，我不仅运行格式化工具（Prettier），更关键的是运行编译检查。 强力模式： 如果 tsc (TypeScript Compiler) 检查失败，Hook 可以配置为拒绝停止。它会将错误日志反向喂给 Claude，并指令：“编译未通过，请修复这些错误后再结束任务。” 这构成了自动化的 ReAct (Reasoning + Acting) 循环，直到代码真正跑通。 配置指南与最佳实践 标准配置模板 我推荐在项目根目录 .claude/settings.json 中使用如下配置，兼顾扩展性与维护性： { \"hooks\": { \"UserPromptSubmit\": [ { \"hooks\": [ { \"type\": \"command\", \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/skill-injector.sh\" } ] } ], \"PostToolUse\": [ { \"matcher\": \"Edit|Write\", \"hooks\": [ { \"type\": \"command\", \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/file-tracker.sh\" } ] } ], \"Stop\": [ { \"hooks\": [ { \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/auto-format.sh\" }, { \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/build-verifier.sh\" } ] } ] } } 进阶调优技巧 性能陷阱：Hooks 是同步执行的。如果我的 PostToolUse 脚本要跑 5 秒，Claude 就会卡住 5 秒。 对策：将耗时操作（如重型分析）放入后台运行（nohup ... &amp;），或者仅对增量文件进行检查。 环境隔离：Hook 脚本中的 $PATH 可能与我的终端不同。 对策：在脚本开头显式 source ~/.zshrc 或使用绝对路径调用工具（如 /usr/local/bin/npm）。 调试黑盒：当 Hook 不工作时，Claude 不会报错，只会默默失败。 对策：在脚本头部加上 set -x，并将日志输出到标准错误流 &gt;&amp;2。Claude 会将 stderr 的内容显示在调试日志中。 结语 Claude Code Hooks 的本质，是将 Software 1.0 (明确的代码逻辑) 的力量赋予 Software 2.0 (神经网络)。 通过合理配置 Hooks，我不再是被动地“使用” AI，而是将 AI 作为一个高智商的模块，集成到我要严密的软件工程体系中。这才是 Agent 开发的终极形态。 下一篇预告：Claude Code #5-斜杠命令- 学习如何配置自定义命令工具。" }, { "title": "Claude Code(三)核心解密：从提示词工程到上下文引擎", "url": "/posts/claude-code-context-window/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent, Context Engine, Architecture", "date": "2026-01-14 23:15:10 +0800", "content": "要真正掌握 Claude Code 这样的 CLI Agent，我意识到不能只停留在”如何写好 Prompt”的层面。我需要把视角拉高，回顾 AI 交互范式的演变，并深入到底层，看看数据是如何在硅基大脑中流动的。 本文记录了我对三个维度的深度解构：从提示词工程的历史，到上下文引擎的崛起，最后直抵大模型的物理底层——向量与张量。 提示词工程 (Prompt Engineering) 的诞生与演变 在 ChatGPT 刚刚横空出世时，Prompt Engineering 被称为”未来的编程语言”。它的核心逻辑是通过自然语言的技巧，激发大模型潜在的能力。回顾这段历史，我能清晰地看到开发者是如何一步步从”玄学”走向”工程”的。 手工时代 (The Manual Era) 早期的 Prompt Engineering 就像中世纪的炼金术。我在那个阶段没有固定的范式，完全依赖直觉和反复试错，试图找到那句能让模型”点石成金”的咒语。 那个时候的模型（如 GPT-3）虽然博学但”性格顽固”。如果我直接扔给它一个复杂的指令，比如”帮我写个贪吃蛇游戏”，它往往不知所措，或者只吐出一堆毫无逻辑的代码片段。为了解决这个问题，我开始摸索 Few-Shot（少样本） 和 CoT（思维链） 等技巧。 我开始在 Prompt 中手动拼接示例，像教小学生一样引导模型：先演示把”Hello”变成”HELLO”，再演示把”World”变成”WORLD”，最后才让它处理”Claude”。这种方式虽然笨拙，但它确实有效地强迫模型进入了特定的模式，避免了它在无限的生成空间中迷路。 模板化时代 (The Template Era) 随着 LangChain 等框架的流行，我进入了模板化时代。手动拼接字符串的方式显然无法适应批量化的任务——我不可能为了分析 100 个不同的 Bug 日志而手写 100 次 Prompt。 于是，Prompt Template 应运而生。我将 Prompt 视为一个函数，通过预留的”槽位”（Slots）来动态注入外部数据。 template = \"\"\" System: 你是一个 {role} 专家。 Context: 以下是相关的代码片段： {code_snippet} Task: 请分析代码中的 {bug_type} 问题。 \"\"\" 这种变革解决了复用性的难题，让 Prompt 变成了可维护的代码。但很快，我又撞上了一个更坚硬的天花板：容量瓶颈。无论我的模板设计得多么精妙，它最终都要塞进那个有限的 Context Window 里。 为什么单纯的 Prompt Engineering 不够了？ 当我的战场从简单的脚本编写扩展到复杂的企业级项目维护时，Prompt Engineering 显得力不从心。我面临着静态 Prompt 与动态代码库之间的根本矛盾。 代码库是活的，每时每刻都在 git commit 中演进；而 Prompt 是死的。更糟糕的是，注意力稀释 (Lost in the Middle) 现象开始显现：当我试图把整个项目的文档都塞给模型时，它反而像个被大量信息淹没的实习生，开始忽略最关键的指令。 为了突破这一瓶颈，我的关注点开始向 Context Engine 转移。我不再执着于”写出完美的 Prompt”，而是转向”构建更智能的上下文管理系统”。 上下文引擎 (Context Engine) 的崛起 如果说 Prompt System 是”导演”，负责讲戏；那么 Context Engine 就是”剪辑师”，负责在海量的素材中，剪辑出最关键的片段交给导演。 核心原理：预算与取舍 Context Engine 本质上是一个动态资源分配系统。它运作的核心逻辑在于”预算”（Budget）。 Token 不仅仅是金钱，更是时间。如果每次交互都把整个项目的所有文件重新发给模型，昂贵的 API 成本和漫长的等待时间会瞬间摧毁开发体验。Context Engine 必须在毫秒级内做出残酷的决策：在 20k Token 的预算内，是保留刚才的报错日志？还是保留 README.md？还是保留我 5 分钟前的一句指令？ 工作流水线 Claude Code 的 Context Engine 并不是简单地存储历史，它像一个精密的搜索引擎，对每一条流入的信息进行清洗、排序和压缩。 Context Engine 的工作流程示意图 flowchart TD Input[原始输入源] --&gt; Filter(清洗 Filtering) Filter --&gt; Rank(优先级排序 Ranking) Rank --&gt; Budget{Token 预算检查} subgraph Sources User[用户指令] Files[加载的文件] Term[终端输出流] end Sources --&gt; Input Budget -- 溢出 --&gt; Compress[智能压缩 Compression] Budget -- 充足 --&gt; Final[构建 Context] Compress --&gt; Final Compress --&gt; A[截断头部/尾部] Compress --&gt; B[提取摘要] Compress --&gt; C[移除冗余日志] style Filter fill:#e1f5fe,stroke:#01579b style Compress fill:#fff9c4,stroke:#fbc02d 首先是清洗（Filtering）。终端输出中往往充斥着对模型无意义的噪音，比如进度条 [=====&gt;] 或是 ANSI 颜色代码。引擎会通过正则过滤器无情地剔除这些垃圾，只保留纯粹的文本信息。 接着是排序（Ranking）。并非所有文件都生而平等。Context Engine 会利用启发式算法，给最近修改的文件、报错的文件赋予更高的权重，而那些仅仅被引用的静态文件则会被降权。 最后是压缩（Compression）。当 Token 依然超标时，引擎会启动智能截断机制：保留代码文件的头部（通常包含 import）和尾部（通常包含 export），而将中间的具体实现逻辑用 ... 代替。甚至，它会将早期的多轮对话压缩成一段简短的摘要，只保留”我之前尝试修改 Auth 模块但失败了”这样的关键信息。 对抗上下文腐烂 为什么要费这么大劲做这一套系统？因为上下文腐烂 (Context Rot) 是大模型应用中的隐形杀手。 研究表明，随着上下文窗口的填充，Transformer 模型的计算复杂度呈 $O(n^2)$ 指数级增长。上下文翻倍，意味着计算成本和推理延迟翻四倍。更可怕的是，过多的干扰信息（Distractors）会显著增加模型产生幻觉的概率。 Context Engine 通过实时熵减，始终保持上下文的纯净度。它不仅仅是为了帮我省钱，更是为了在海量信息中保住模型的智商。 底层解密——从文本到张量 为了真正理解为什么我要”惜 Token 如金”，我钻进了模型的物理底层，看看数据究竟是如何被计算的。 映射：从 Token 到 向量 (Embedding) 计算机并不认识”苹果”或”if”，它只认识数字。当我输入文本时，首先发生的是人类语言向机器语言的翻译。 这个过程的第一步是 Tokenization (分词)。输入文本被切分成 Token，这也就是为什么中文比英文贵的原因：中文的信息密度极高，但 Token 密度低（通常一个汉字对应 1-2 个 Token）。表达同样的意思，中文往往需要消耗更多的 Token。 紧接着是 Embedding (向量化)。每个 Token ID 被查表映射成一个高维向量（例如 4096 维）。在这个高维向量空间中，词与词之间的语义关系被数学化了——意思相近的词，它们的向量距离也更近。 计算：张量 (Tensor) 的洪流 当这些向量进入模型内部，它们被堆叠成张量（多维数组），并在 Transformer 的层级中开始流动。 flowchart TD Text[文本输入] --&gt; Tokenizer[Tokenizer分词] Tokenizer --&gt; TokenID[Token ID] TokenID --&gt; Embedding[嵌入层转换] Embedding --&gt; Vector[向量表示] Vector --&gt; TensorStack[堆叠成张量] TensorStack --&gt; ModelProcessing[Transformer 层级计算] ModelProcessing --&gt; OutputProb[输出概率分布] OutputProb --&gt; TokenOut[Token 输出] style Text fill:#e1f5fe style Tokenizer fill:#fff9c4 style Embedding fill:#f1f8e9 style Vector fill:#ffebee style ModelProcessing fill:#f3e5f5 模型需要理解词与词之间的关系（语法、指代、逻辑），这完全依赖于核心的 Self-Attention (自注意力机制)。 在这个过程中，每一个 Token 都要和所有其他 Token “打招呼”，计算相关性分数。这种全员互动的代价是昂贵的——$O(n^2)$ 的复杂度意味着，如果上下文长度增加 10 倍，计算量将暴增 100 倍。这正是 200k 上下文不是线性的 2 倍成本，而是指数级计算压力的物理根源。 输出：概率的坍缩 模型最终输出的也不是确定的文字，而是一个概率分布表。它预测”下一个 Token”最可能是谁。 比如在”我”字后面，接”爱”的概率可能是 60%，接”是”的概率是 20%。我们通过 Temperature (温度) 参数来控制这种采样的随机性。温度越高，选词越狂野；温度越低，选词越保守。 Token 经济学的第一性原理 理解了底层的 $O(n^2)$ 计算原理，我就明白了为什么 Token 经济学是 AI 开发的第一性原理： 物理算力: 每一个 Token 的增加，都会导致 GPU 需要进行数亿次的浮点运算 (FLOPS)。 显存占用: KV Cache（键值缓存）随着上下文线性增长，过大的上下文会直接撑爆显存（OOM）。 响应延迟: First Token Time (首字延迟) 直接受限于计算量。 最佳实践 除了依赖 Context Engine 的自动压缩，作为开发者我也养成了良好习惯： 使用 /clear 定期清理上下文。 使用 /rm 移除不再需要的大文件。 在涉及大量日志分析时，先用 grep 过滤关键信息。 总结 Claude Code 的强大，不仅在于它背后有一个聪明的模型（Claude 4.5 Sonnet），更在于它构建了一套精密的工程体系： Prompt Engineering 负责设定模型的人设和知识边界。 Context Engine 负责在海量信息中提炼精华，对抗上下文腐烂。 底层计算 时刻提醒我，每一分算力都来之不易。 作为开发者，当我使用 /clear 清理上下文，或者使用 /add 精确加载文件时，我实际上是在协助这个精密的系统，进行更高效的熵减操作。 下一篇预告：Claude Code 实战：使用 Hooks 自动化你的工作流 - 学习如何配置 Pre-command 和 Post-command 钩子。" }, { "title": "Claude Code(二)环境配置", "url": "/posts/claude-code-config/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent, Configuration", "date": "2026-01-10 17:21:00 +0800", "content": "配置体系概览 Claude Code 内置了一套多层级配置架构，目的是为了平衡”个人开发习惯”与”团队项目规范”。通过全局设置、项目级配置及环境变量的组合，我可以控制 Agent 的行为、权限以及它对项目的理解。 配置作用域 配置项会按照特定的优先级生效。这种设计确保了团队规范的统一性，同时允许我在本地进行个性化调整。 作用域 配置文件位置 逻辑定义 是否同步 (Git) 典型应用场景 Managed (管理级) 系统级目录 强制约束 是 (IT 部署) 企业统一的安全红线。例如：强制禁止访问生产数据库，强制开启审计日志。此层级普通用户无法修改。 User (用户级) ~/.claude/ 个人偏好 否 跨项目的通用习惯。例如：我默认开启了思维链模式，配置了个人的 GitHub Token，并安装了常用的辅助插件。 Project (项目级) 项目根目录 .claude/ 团队规范 是 团队共享的规范。例如：统一的代码格式化命令，项目专属的 MCP 服务器配置，团队共用的 Lint 规则。 Local (本地级) .claude/*.local.* 临时环境 否 (Git 忽略) 仅针对当前机器的特定配置。例如：本地数据库的连接串，临时调试用的 API Key。 配置隔离: 我始终遵循”配置隔离”原则。将涉及机密凭证的配置严格限制在 User 或 Local 作用域；将涉及工程规范的配置显式提交到 Project 作用域。 优先级与覆盖 当同一个配置项在多个作用域中同时存在时，优先级如下： Managed (最高) &gt; Command Line Flags &gt; Local &gt; Project &gt; User (最低) 这意味着： 我在命令行指定的参数（如 --model）优先级极高，适合临时测试。 我的 Local 配置可以覆盖团队的 Project 配置（例如团队要求用 Python 3.9，但我可以在本地测试 Python 3.10）。 任何配置都无法突破 Managed 层的限制。 功能模块分布 配置系统不仅管理键值对，还管理着 Agent 的扩展能力和记忆： 功能模块 用户级 (User) 项目级 (Project) 本地级 (Local) Settings (行为) ~/.claude/settings.json .claude/settings.json .claude/settings.local.json Subagents (能力) ~/.claude/agents/ .claude/agents/ — MCP Servers (连接) ~/.claude.json .mcp.json ~/.claude.json (项目状态缓存) Memory (记忆) ~/.claude/CLAUDE.md CLAUDE.md 或 .claude/CLAUDE.md CLAUDE.local.md 核心配置文件详解 Claude Code 的”大脑”由三个核心部分组成：Settings (控制中枢)、Memory (项目记忆) 和 MCP (外部连接)。 1. Settings.json：行为控制 settings.json 定义了 Claude Code 的运行规则。它不关心我的代码业务逻辑，只关心 Agent 本身如何运作。 Permissions (权限模型): 采用”最小权限原则 (Least Privilege)”。 allow: 明确放行的命令白名单。 deny: 绝对禁止的操作（如读取 .env，上传私钥）。 ask: 需要人工审批的敏感操作（默认策略）。 Sandbox (沙箱环境): 我强烈建议开启。启用后，Bash 工具将在隔离的容器化环境中运行。这不仅防止了误删系统文件，还保证了环境的一致性。 Hooks (生命周期钩子): 类似于 Git Hooks。 PreToolUse: 在工具执行前拦截，可用于强制 Lint 检查。 PostToolUse: 工具执行后触发，可用于自动化测试反馈。 示例：构建一个安全的开发环境 { \"permissions\": { \"allow\": [\"Bash(npm run *)\", \"Bash(git status)\", \"Bash(ls)\"], \"deny\": [\"Read(.env)\", \"Bash(curl -X POST *)\"], \"ask\": [\"Edit(package.json)\"] }, \"sandbox\": { \"enabled\": true, \"timeout\": 300 }, \"autoUpdater\": { \"enabled\": false } } 权限合并逻辑：权限系统的合并策略是 “收敛合并” (Restrictive Merge)。如果在 User 层允许了 curl，但在 Project 层禁止了 curl，最终结果是禁止。安全限制总是优先于许可。 2. CLAUDE.md：项目记忆 如果说 settings.json 是大脑的结构，那么 CLAUDE.md 就是大脑中的知识。它是 Claude Code 最独特的特性之一，充当了项目级系统提示词的角色。 它与传统文档的区别在于：它是写给 AI 看的，不是给通过人看的。 构建指令: 我可以明确告诉它：”在这个项目里，运行测试必须用 make test-unit，而不是 npm test。” 架构约束: 明确”负面约束”。例如：”禁止引入新的 npm 依赖，除非经过批准”、”UI 组件必须与逻辑 Hook 分离”。 风格指南: 统一代码风格，减少 Code Review 的摩擦。 3. MCP Servers：外部能力 通过 .mcp.json，Claude Code 利用 Model Context Protocol (MCP) 协议连接外部工具。它允许 Claude 连接到： GitHub/GitLab: 直接读取 Issue 内容或 PR 评论。 PostgreSQL/MySQL: 在只读权限下查询数据库 Schema，辅助编写 SQL。 Browser: 实时抓取网页文档库。 CLAUDE.md 与 Rules 文件对比 记忆文件 (CLAUDE.md) vs 规则文件 (Rules) 记忆文件 (CLAUDE.md) 性质: 项目级系统提示词 目标: 为 Claude 提供项目背景知识和上下文信息 内容: 项目规范、构建指令、架构约束、代码风格、历史配置 工作机制: 渐进式暴露动态加载，支持 @ 引用语法 使用场景: 项目初始化、需要持续记忆、需要给予项目感 规则文件 (Rules) 性质: Agent 行为控制中心 目标: 确保 Agent 操作安全可控，防止超权限或危险操作 内容: 权限模型、允许/禁止操作清单、安全策略、沙箱环境 工作机制: 层叠优先级机制，安全限制优先于允许 使用场景: 需要强制执行安全策略、需要控制权限、需要管理共享规范 核心差异对比 特征 记忆文件 (CLAUDE.md) 规则文件 (Rules) 主要用途 知识传递和上下文建立 行为控制和安全管理 内容类型 描述性、知识性 配置性、约束性 更新方式 动态热更新 静态配置更新 优先级机制 子目录优先 管理层最高 安全性要求 低 高 适用范围 项目特定 系统全局 Claude Code 中 Rules 的使用要求和规范 作用域管理 管理层 (Managed): 最高权限，企业级强制约束 用户层 (User): 个人偏好配置，不同步到 Git 项目层 (Project): 团队共享规范，同步到 Git 本地层 (Local): 机器特定配置，Git 忽略 权限模型规范 允许列表 (allow): 明确放行的操作白名单 禁止列表 (deny): 绝对禁止的高危操作 询问列表 (ask): 需要人工审批的敏感操作 安全策略要求 最小权限原则: 仅授予完成任务所必需的最小权限 收敛合并策略: 安全限制总是优先于允许 配置隔离: 敏感信息仅放在 User 或 Local 层 沙箱环境: 强烈建议启用隔离环境运行 配置优先级链 Managed (最高) &gt; 命令行参数 &gt; Local &gt; Project &gt; User (最低) 最佳实践 使用 /init 命令生成基础配置 当 Claude 出错时，立即使用 /memory 更新规则 逐步放开必要权限，同时守住安全红线 在 Project 层维护团队共享规范 在 User/Local 层管理个人偏好和敏感信息 记忆系统工作原理 Memory 机制是 Claude Code 区别于普通 Copilot 的核心。它通过结构化上下文注入，让 AI 理解项目结构。 上下文加载机制 Claude Code 通过 CLAUDE.md 文件来获取项目上下文。它不会一次性读取所有文件，而是根据我当前的工作目录 (CWD) 动态加载相关的 CLAUDE.md 文件。 加载逻辑： 定位: 确定当前 Shell 所在的路径。 回溯: 从当前路径向上查找至根目录，收集沿途所有的 CLAUDE.md。 合并: 将收集到的 Markdown 文件按”子目录优先”的顺序合并。 注入: 将合并后的文本作为 System Context 的一部分发送给模型。 场景演示： /workspace/ ├── CLAUDE.md # [Layer 1] 全局规范：Java 项目通用配置 └── services/ ├── payment-service/ │ ├── CLAUDE.md # [Layer 2] 服务规范：支付网关接口定义 │ └── src/ └── user-service/ └── CLAUDE.md # [Layer 2] 服务规范：用户数据隐私标准 当我 cd services/payment-service 后，Claude 脑子里装的是 Layer 1 + Layer 2 (Payment)。它完全不知道 User Service 的隐私标准，这完美避免了上下文污染，并节省了 Token。 动态维护 记忆不是静态的。 /init: 初始化项目记忆文件。分析 package.json 等元数据，自动生成”第一份记忆”。 /memory: 热更新记忆文件。当我在对话中纠正了 Claude 的一个错误（比如”不要用 Log4j，用 SLF4J”），我会立即使用 /memory 将这条规则写入文件。这实现了从”纠正”到”教会”的质变。 环境变量与模型调优 环境变量 (Env Vars) 环境变量不仅用于鉴权，还用于微调 Runtime 行为： ANTHROPIC_API_KEY: 必须。 BASH_DEFAULT_TIMEOUT_MS: 调整 Shell 命令的”耐心值”。对于大型编译任务，我通常会调大此值。 CLAUDE_LOG_LEVEL: 设置为 debug 可查看详细的 Prompt 交互日志，用于排查为何 Claude “不听话”。 Thinking Mode Claude 3.7+ 引入的 Thinking Mode 通过增加推理时间来提高输出质量。 原理: 模型在输出最终代码前，会进行一段内部推理，检查逻辑漏洞。 适用场景: 复杂的重构、算法设计、涉及多个文件联动的修改。 不适用场景: 简单的语法修正、文档补全（耗时且消耗 Token）。 高效协作技巧 1. 结构化需求 (Structured Prompting) 我把 Claude 当作一名高级工程师。使用 DOD (Definition of Done) 清单： “请实现用户登录功能。” (❌ 弱指令) “请实现用户登录 API，要求如下： 接口: POST /api/login，接收 JSON。 验证: 使用 Zod 进行 Schema 校验。 安全: 密码必须使用 Argon2 哈希。 测试: 编写对应的 Vitest 单元测试。 规范: 遵循 @docs/api-guide.md 中的错误码定义。” (✅ 强指令) 2. 模块化引用 (@References) CLAUDE.md 的 @ 语法支持引用文件，这实际上构建了一个知识索引。 @package.json: 告诉 Claude 当前项目的依赖版本。 @src/types.ts: 告诉 Claude 全局的数据结构定义。 善用引用，可以让 Claude 在不读取整个文件的情况下，精准获取关键上下文。 结语 配置 Claude Code 的过程，实际上就是把项目知识教给 AI 的过程。 起步: 运行 /init 生成基础配置。 调优: 发现 Claude 犯错时，立刻更新 CLAUDE.md，避免重复纠正。 定界: 通过 settings.json 管理权限。 做好配置后，每次开启新会话，Claude 都能直接理解项目背景，产出符合团队规范的代码，从而减少在”解释背景”上花费的时间。 下一篇预告：Claude Code(三)核心解密：揭秘上下文引擎与提示词系统-了解Claude Code如何管理上下文，了解如何节省token" }, { "title": "Claude Code(一)介绍与安装", "url": "/posts/claude-code-introduction/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent", "date": "2026-01-10 16:21:00 +0800", "content": "Claude Code是什么 Claude Code 是 Anthropic 推出的基于 CLI（命令行界面）的下一代 AI 编程助手。不同于传统的 IDE 插件，它作为一个独立的 Agent 运行在终端中，能够直接访问本地文件系统、执行 Shell 命令、管理 Git 版本控制，并理解整个代码库的上下文结构。我从2024年9月开始一直是重症患者般地使用Cursor IDE，直到2025年12月才真正认识到CLI Agent的威力。 它不只是在帮我写代码，而是在”代理”我进行开发。我可以直接告诉它”修复这个报错”或者”重构这个模块”，它会自动分析报错信息、定位文件、修改代码、运行测试验证，直到问题解决。 flowchart TD A[用户指令] --&gt; B[分析需求并制定计划] B --&gt; C[执行相应操作] C --&gt; D[进行验证与调整] D --&gt; E[交付最终结果] C -.-&gt;|git commit| F[版本控制] C -.-&gt;|npm install| G[包管理] C -.-&gt;|pytest| H[测试执行] style A fill:#e1f5fe style E fill:#f1f8e9 Claude Code的核心能力 虽然官方文档把它描述成agentic coding tool，它不仅具有强大的编码能力，还能理解整个项目上下文、自主执行终端命令、Git操作、调用外部工具。甚至基于多模态大模型，可以干非常非常多的事，比如图片识别、生图、语音交互等等。 功能场景 具体表现 核心价值 全项目理解 能够通过 ls, grep, glob 等工具自主探索文件结构，阅读代码依赖，分析配置文件。 它的上下文不仅限于我打开的文件，而是整个工程的架构逻辑。 终端自主权 可以直接执行 Bash 命令，包括安装依赖 (npm install)、运行构建 (make build)、执行测试 (pytest)。 打通了”编写代码”到”验证代码”的闭环，拥有了执行力。 任务规划 面对复杂需求（如”重构认证模块”），能拆解为多个步骤（Plan -&gt; Act -&gt; Verify），并维护任务列表。 具备了任务拆解能力，遇到错误时会根据报错信息尝试自主修正，而不是把问题抛回给我。 版本控制集成 理解 Git 状态，能查看 git diff，分析变更，甚至帮我撰写 Commit Message 和提交代码。 将”写代码”和”提交代码”整合到了单一的对话流中，保持心流不被打断。 与Cursor的区别 Cursor 的本质依然是一个极其强大的 IDE 编辑器（Fork 自 VS Code），它的优势在于无缝的编辑器体验，通过LLM大模型极其顺滑地预测我的下一个按键，或者通过 Composer 窗口进行多文件编辑。它的核心交互依然离不开”光标”和”编辑器窗口”，就像一个坐在旁边的副驾驶。 相比之下，Claude Code 彻底抛弃了 GUI 的束缚。我不再是操作一个编辑器，而是在与一个拥有 Shell 权限的 AI 结对编程。Cursor 擅长”写”（Writing），而 Claude Code 擅长”做”（Doing）。Cursor 适合在这个文件里快速实现一个函数，而 Claude Code 适合在整个项目中从头搭建一个功能、修复一个棘手的跨文件 Bug 或进行一次大规模重构。 维度 Cursor (IDE + AI) Claude Code (CLI Agent) 核心形态 基于 VS Code 的定制编辑器 运行在终端的命令行工具 交互逻辑 补全(Tab)、聊天(Cmd+L)、内联编辑(Cmd+K) 对话式指令 (Prompt -&gt; Action -&gt; Result) 上下文范围 打开的文件、引用的代码块、向量索引 整个文件系统、终端输出、Git 历史 操作权限 主要是读写文件 读写文件 + 执行任意系统命令 适用场景 快速编码、浏览代码、即时修改 复杂重构、Debug调试、自动化脚本编写、脚手架生成 思维模式 辅助驾驶 (Copilot) 代理执行 (Agent) quadrantChart title \"AI编程工具位置对比\" x-axis \"代码编写\" --&gt; \"项目执行\" y-axis \"局部优化\" --&gt; \"全局管理\" \"Cursor\": [0.2, 0.8] \"Claude Code\": [0.8, 0.2] \"GitHub Copilot\": [0.3, 0.3] \"Tabnine\": [0.1, 0.1] 安装 官方提供了多种安装途径，但我强烈建议选择 Native Install（原生安装脚本）。相比于常见的 npm install -g，原生安装包内置了独立的运行时环境，彻底避免了因本地 Node.js 版本差异导致的各种诡异报错，且升级维护更加稳定。 # 官方推荐的安装命令（示例） curl -sL https://code.claude.com/install.sh | bash 由于众所周知的网络原因，在国内网络环境下安装或使用时，大概率会遇到连接超时或区域限制的问题。 仅仅开启系统代理软件（VPN）往往是不够的，终端（Terminal）的流量默认可能不走系统代理。我必须在 Shell 中显式 export 代理变量，否则安装和使用过程中会频发连接超时错误。 通常需要在终端显式声明代理环境变量，确保流量正确转发： # 替换为自己的代理端口，通常是 7890 或 10808 export https_proxy=http://127.0.0.1:7890 export http_proxy=http://127.0.0.1:7890 # 建议执行以下命令验证代理是否生效 curl -I https://www.google.com 模型配置 Anthropic 原生模型 (Claude 4.5 Sonnet) 这是 Claude Code 的默认且最佳体验配置。直接登录即可使用，无需复杂设置，原生支持开箱即用。 OpenAI (GPT-5.x) 虽然可以通过适配层使用 GPT-4o，但实际使用下来的感受是：模型定价较高。对于高频的 Agent 交互（通常伴随着大量的 Input Token 读取）来说，成本控制是不得不考虑的因素。 DeepSeek (V3.1) 国内之光 DeepSeek 也是很多人的首选。在配置时，通常需要指向兼容 OpenAI 格式的端点。实测下来，DeepSeek-V3 能满足日常代码分析需求，对于常见 Bug 的定位也很准确。但在处理跨多个文件的复杂逻辑或需要长上下文推理的任务时，稳定性稍逊于 Claude 4.5 Sonnet。 xAI Grok (Grok 4.1 Fast) + LiteLLM 这是一个非常有趣的非官方方案。由于 Grok 的 API 格式不兼容 Anthropic 标准，我们需要引入 LiteLLM 作为中间代理。 我现在就是用的这套方案，可以胜任95%以上的编码任务。但 LiteLLM 毕竟是代理转发工具，部分高级参数映射可能不完全。尽管如此，Grok 的推理速度和代码能力令人印象深刻。 Google Gemini 3 (Pro/Flash) + LiteLLM 理论上 Gemini 的超长上下文非常适合全库分析，但现阶段通过 LiteLLM 接入的兼容性问题较多，常出现参数校验错误。虽然 Pro 版本性能强悍且 Flash 价格诱人，但在工具链完善之前暂不推荐作为主力。可以使用Gemini CLI 直连。 其他模型 目前不推荐使用对 Tool Use（工具调用）支持不完善的模型。Claude Code 强在依赖于自家模型准确输出特定格式的工具调用指令，如果模型遵循指令能力较弱，会导致 Agent 反复尝试或卡死。 Anthropic 的风控极其严格，大陆及港澳地区 IP 直连会被立即封号，且申诉极难通过。务必确保终端流量走在靠谱的海外节点上，并保持网络环境稳定。 使用 在终端中进入项目根目录，只需输入简单的命令即可唤醒这个强大的助手： claude 首次运行时，它会花一点时间对当前目录进行索引（Indexing），建立对项目结构的大局观。这个过程非常快，且索引完全在本地构建。一旦就绪，我就进入了一个交互式的 REPL（Read-Eval-Print Loop）环境，可以像和同事聊天一样与代码库对话。 Claude Code 具有执行终端命令的权限（如 rm, git push）。虽然它在执行敏感操作前会请求批准，但在授予权限（输入 y）之前，我都会仔细阅读它打算执行的命令，避免误删文件。 工作流示范 参考官方的最佳实践，以下是两种最能体现 Claude Code 价值的经典工作流，也是我最常用的方式： 沉浸式代码探索 (Exploration) 当我接手一个遗留项目或面对复杂的开源库时，传统的做法是手动翻阅文件，试图在脑海中构建依赖关系图。而在 Claude Code 中，我可以直接提问： &gt; \"请解释一下 `auth` 模块的认证流程是如何实现的？\" &gt; \"找出所有使用了 `User` 模块的地方，并检查是否有潜在的类型安全问题。\" 它会自动运行 grep、阅读相关文件、分析引用关系，最后给出一份详尽的分析报告。这不仅是搜索，更是理解。 交互式重构与修复 (Refactoring &amp; Fixing) 这是 CLI Agent 最迷人的地方。我不再需要手动在一个个文件中复制粘贴。 &gt; \"运行测试，修复所有失败的用例。\" &gt; \"把 `src/utils` 下的大文件拆分成多个小的工具函数文件，并为它们补充单元测试。\" Claude Code 会生成一个执行计划（Plan），告诉我它准备读哪些文件、改哪些代码。在我批准后，它会像一个熟练的工程师一样，自动执行编辑、运行测试、根据报错再次修正代码，直到测试全部通过。我只需要扮演 “Tech Lead” 的角色进行 Code Review 和验收。 总结 Claude Code 的出现标志着 AI 辅助编程从”副驾驶”（Co-pilot）向”代理人”（Agent）的实质性跨越。如果说 Cursor 是让我写代码更爽的跑车，那么 Claude Code 就是能帮我自动驾驶的司机。尽管目前在模型成本和区域访问上仍有门槛，但它展示的终端自主权和全项目理解力，确实解决了很多 IDE 插件无法处理的复杂场景（如环境配置、跨模块重构）。从手动逐行编码到指挥 Agent 执行任务，我现在的角色更像是一个进行 Code Review 和技术决策的 Manager。 下一篇预告：Claude Code(二)环境配置 - 了解 Claude Code 的详细配置项" }, { "title": "Claude Code学习总结：目录导航", "url": "/posts/claude-code-guide/", "categories": "AI, Claude Code", "tags": "AI, CLI Agent", "date": "2026-01-10 15:21:00 +0800", "content": "预览 从2024年9月开始，公司内部已经开始强推Cursor IDE，今年12月中公司做AI产品的部门分享了Claude Code CLI工程实践。听完之后就开始详细了解了Claude Code是如何使用，如何利用CLI Agent提升速度与效率。经过了大约20来天断断续续的学习使用，对Claude Code有了基本认知。现在开始梳理、细化、总结这个Agent工具使用的各个部分。 系列文章目录 文章导航 简介 Claude Code #1-介绍 如何安装、设置新项目、添加上下文 Claude Code #2-初始化配置 CLAUDE.md 文件与 /init Claude Code #3-上下文窗口 理解如何管理上下文，如何节省token Claude Code #4-hooks hooks能干什么，以及如何使用钩子 Claude Code #5-斜杠命令 创建自定义斜杠命令来简化工作流程 Claude Code #6-Agent Skills 介绍agent技能，如何使用 Claude Code #7-Subagents 使用专门的字AI处理特定任务的工作流程 Claude Code #8-Plugins 插件的介绍、使用、开发 Claude Code #9-MCP服务 MCP服务配置、安装、使用 Claude Code #10-思考与技巧 克服空白瘫痪，不断向前演进 参考来源 参考资料(截至2026年1月)： 参考信息 简介 Claude Code docs Claude Code的官方文档 包含了所有的基本用法 Claude Code Github Claude Code官方仓库 不定时发布Claude Code重要更新说明 Claude Code tutorail 知名Youtube博主的入门教程 Claude Code handbbok 非常高质量的用AI学习并总结Claude Code的博客教程 prompts-tools 获得极高star的提示词工程学习仓库 ShareAI-learn-claude-code 获得非常高star的Claude Code学习仓库 Claude-Code-Infrastructure-Showcase 获得非常高star的Claude Code学习仓库 claude-code-templates 获得非常高star的Claude Code学习仓库 SuperClaude 值得肯定的Claude Code学习仓库 awesome-claude-code 收集了学习Claude Code一揽子仓库的仓库 下一篇预告：Claude Code(一)基本介绍-了解Claude Code如何管理上下文，了解如何节省token" }, { "title": "自定义管线:方向光 (翻译三)", "url": "/posts/directional-lights/", "categories": "Unity3D, ScriptRenderPipeline", "tags": "SRP, Shader", "date": "2019-11-30 00:00:00 +0800", "content": "增加对多个方向光着色（shading）的支持。 已升级至 2022.3.62f2 1 光照 (Lighting) 如果我们想创建一个更真实的场景，那么我们就必须模拟光如何与表面相互作用。这需要一个比我们目前拥有的不发光（unlit）着色器更复杂的着色器。 1.1 受光着色器 (Lit Shader) 复制 UnlitPass.hlsl 文件并将其重命名为 LitPass.hlsl。调整重命名（include guard define）以及顶点和片元函数名称以匹配。我们稍后会添加光照计算。 #ifndef CUSTOM_LIT_PASS_INCLUDED #define CUSTOM_LIT_PASS_INCLUDED … Varyings LitPassVertex (Attributes input) { … } float4 LitPassFragment (Varyings input) : SV_TARGET { … } #endif 同时复制 Unlit 着色器并将其重命名为 Lit。更改其菜单名称、它包含的文件以及它使用的函数。让我们也将默认颜色更改为灰色，因为在光照充足的场景中，全白表面可能会显得非常亮。通用管线（URP）默认也使用灰色。 Shader \"Custom RP/Lit\" { Properties { _BaseMap(\"Texture\", 2D) = \"white\" {} _BaseColor(\"Color\", Color) = (0.5, 0.5, 0.5, 1.0) … } SubShader { Pass { … #pragma vertex LitPassVertex #pragma fragment LitPassFragment #include \"LitPass.hlsl\" ENDHLSL } } } 我们将使用自定义光照方法，我们将通过将着色器的 LightMode 设置为 CustomLit 来指示这一点。在 Pass 中添加一个 Tags 块，包含 \"LightMode\" = \"CustomLit\"。 Pass { Tags { \"LightMode\" = \"CustomLit\" } … } 为了渲染使用此 Pass 的对象，我们必须在 CameraRenderer 中包含它。首先为其添加一个着色器标签标识符。 static ShaderTagId unlitShaderTagId = new ShaderTagId(\"SRPDefaultUnlit\"), litShaderTagId = new ShaderTagId(\"CustomLit\"); 然后将其添加到 DrawVisibleGeometry 中要渲染的 Pass 中，就像我们在 DrawUnsupportedShaders 中所做的一样。 var drawingSettings = new DrawingSettings( unlitShaderTagId, sortingSettings ) { enableDynamicBatching = useDynamicBatching, enableInstancing = useGPUInstancing }; drawingSettings.SetShaderPassName(1, litShaderTagId); 现在我们可以创建一个新的不透明材质，尽管此时它的结果与不发光材质相同。 默认不透明材质 1.2 法线向量 (Normal Vectors) 物体的受光程度取决于多种因素，包括光线与表面之间的相对角度。为了了解表面的朝向，我们需要访问表面法线（surface normal），这是一个垂直于表面的单位长度向量。该向量是顶点数据的一部分，在对象空间中定义，就像位置一样。因此，在 LitPass 的 Attributes 中添加它。 struct Attributes { float3 positionOS : POSITION; float3 normalOS : NORMAL; float2 baseUV : TEXCOORD0; UNITY_VERTEX_INPUT_INSTANCE_ID }; 由于需要逐像素级计算的，所以我们也必须在 Varyings 中添加法线向量。我们将在世界空间中执行计算，因此将其命名为 normalWS。 struct Varyings { float4 positionCS : SV_POSITION; float3 normalWS : VAR_NORMAL; float2 baseUV : VAR_BASE_UV; UNITY_VERTEX_INPUT_INSTANCE_ID }; 我们可以使用 SpaceTransforms 中的 TransformObjectToWorldNormal 在 LitPassVertex 中将法线转换为世界空间。 output.positionWS = TransformObjectToWorld(input.positionOS); output.positionCS = TransformWorldToHClip(positionWS); output.normalWS = TransformObjectToWorldNormal(input.normalOS); TransformObjectToWorldNormal 是如何工作的？ 当你检查代码时，你会看到它根据是否定义了 UNITY_ASSUME_UNIFORM_SCALING 宏的分支代码。 1.当定义了 UNITY_ASSUME_UNIFORM_SCALING 时，它会调用 TransformObjectToWorldDir，它的作用与 TransformObjectToWorld 相同，只是它忽略了平移部分，因为我们处理的是方向向量而不是位置。但向量也会被均匀缩放，因此稍后应该对其进行归一化。 2.在另一种情况下，不假设均匀缩放。这更复杂，因为当对象受到非均匀缩放变形时，法线向量必须反向缩放以匹配新的表面朝向。这需要乘以转置的 UNITY_MATRIX_I_M 矩阵，再加上归一化。 不正确和正确的法线转换 使用 UNITY_ASSUME_UNIFORM_SCALING 是一种微小的优化，你可以通过自己定义它来启用。但是，当使用 GPU 实例化时，它会产生更大的差异，因为那样就不必将 UNITY_MATRIX_I_M 矩阵数组发送到 GPU。在不需要时避免这样做是值得的。你可以通过在着色器中添加 #pragma instancing_options assumeuniformscaling 指令来启用它，但只有在你专门渲染具有均匀缩放的对象时才这样做。 为了验证我们是否在 LitPassFragment 中获得了正确的法线向量，我们可以将其用作颜色。 base.rgb = input.normalWS; return base; 世界空间法线向量 负值无法可视化，因此它们被约束到0。 1.3 插值法线 (Interpolated Normals) 虽然法线向量在顶点程序中是单位长度的，但跨三角形的线性插值会影响它们的长度。我们可以通过渲染 1 与向量长度之间的差异（放大十倍使其更明显）来可视化误差。 base.rgb = abs(length(input.normalWS) - 1.0) * 10.0; 插值法线误差，夸张处理 我们可以通过在 LitPassFragment 中归一化法线向量来平滑插值畸变。在只看法线向量时，差异并不明显，但在用于光照时会更加明显。 base.rgb = normalize(input.normalWS); 插值后的归一化 1.4 表面属性 (Surface Properties) 着色器中的光照是关于模拟光线照射到表面的相互作用，这意味着我们必须跟踪表面的属性。现在我们有一个法线向量和一个基础颜色。我们可以将后者分为两部分：RGB 颜色和 alpha 值。我们将在几个地方使用这些数据，所以让我们定义一个方便的 Surface 结构体来包含所有相关数据。将其放在 ShaderLibrary 文件夹中一个单独的 Surface.hlsl 文件中。 #ifndef CUSTOM_SURFACE_INCLUDED #define CUSTOM_SURFACE_INCLUDED struct Surface { float3 normal; float3 color; float alpha; }; #endif 我们不应该将法线定义为 normalWS 吗？ 可以，但表面并不关心法线是在什么空间定义的。光照计算可以在任何适当的 3D 空间中执行。所以我们让空间未定义。在填充数据时，我们只需要在任何地方使用相同的空间即可。我们将使用世界空间，但稍后我们可以切换到另一个空间，一切仍然可以正常工作。 将它包含在 LitPass 中，位于 Common 之后。这样我们可以保持 LitPass 简洁。从现在开始，我们将把专业代码放在它自己的 HLSL 文件中，以便更容易找到相关功能。 #include \"../ShaderLibrary/Common.hlsl\" #include \"../ShaderLibrary/Surface.hlsl\" 在 LitPassFragment 中定义一个 surface 变量并填充它。然后最终结果变成表面的颜色和 alpha。 Surface surface; surface.normal = normalize(input.normalWS); surface.color = base.rgb; surface.alpha = base.a; return float4(surface.color, surface.alpha); 这不是低效的代码吗？ 没关系，因为着色器编译器会生成高度优化的程序，完全重写我们的代码。结构体纯粹是为了我们的方便。你可以通过着色器检查器中的 Compile and show code 按钮检查编译器的工作。 1.5 计算光照 (Calculating Lighting) 为了计算实际光照，我们将创建一个具有 Surface 参数的 GetLighting 函数。最初让它返回表面法线的 Y 分量。由于这是光照功能，我们将把它放在 ShaderLibrary 文件夹中一个单独的 Lighting.hlsl 文件中。 #ifndef CUSTOM_LIGHTING_INCLUDED #define CUSTOM_LIGHTING_INCLUDED float3 GetLighting (Surface surface) { return surface.normal.y; } #endif 将它包含在 LitPass 中，在包含 Surface 之后，因为 Lighting 依赖于它。 #include \"../ShaderLibrary/Surface.hlsl\" #include \"../ShaderLibrary/Lighting.hlsl\" 为什么不在 Lighting 中包含 Surface？ 我们可以这样做，但最终会导致多个文件依赖于多个其他文件。我选择将所有包含语句放在一个地方，这使依赖关系清晰。这也使得用另一个文件替换一个文件变得容易，以更改着色器的工作方式，只要新文件定义了其他文件所依赖的相同功能即可。 现在我们可以在 LitPassFragment 中获取光照，并将其用于片元的 RGB 部分。 float3 color = GetLighting(surface); return float4(color, surface.alpha); 来自上方的漫反射光照 此时，结果是表面法线的 Y 分量，因此它在球体顶部为 1，在侧面下降到 0。再往下，结果变为负值，在底部达到 -1，但我们看不见负值。它匹配法线和向上向量之间夹角的余弦值。忽略负数部分，这在视觉上匹配指向正下方的方向光的漫反射光照（diffuse lighting）。最后的修饰是在 GetLighting 中将表面颜色计入结果，将其解释为表面反照率（albedo）。 float3 GetLighting (Surface surface) { return surface.normal.y * surface.color; } 应用反照率 反照率（albedo）是什么意思？ Albedo 在拉丁语中意为“白度”。它是衡量表面散射反射多少光的一个指标。如果反照率不是全白，那么部分光能就会被吸收而不是反射。 2 灯光 (Lights) 为了执行正确的光照，我们还需要知道灯光的属性。在本教程中，我们将仅限制在方向光上。方向光代表一个距离非常远的光源，以至于其位置无关紧要，只有其方向重要。这是一个简化，但足以模拟地球上的太阳光以及入射光或多或少是单向的其他情况。 2.1 灯光结构 (Light Structure) 我们将使用一个结构体来存储灯光数据。目前，颜色和方向就足够了。将其放在一个单独的 Light.hlsl 文件中。同时定义一个 GetDirectionalLight 函数，返回一个配置好的方向光。最初使用白色和向上向量，匹配我们当前使用的灯光数据。请注意，灯光的方向被定义为光线来自的方向，而不是它去的方向。 #ifndef CUSTOM_LIGHT_INCLUDED #define CUSTOM_LIGHT_INCLUDED struct Light { float3 color; float3 direction; }; Light GetDirectionalLight () { Light light; light.color = 1.0; light.direction = float3(0.0, 1.0, 0.0); return light; } #endif 在 Lighting 之前将文件包含在 LitPass 中。 #include \"../ShaderLibrary/Light.hlsl\" #include \"../ShaderLibrary/Lighting.hlsl\" 2.2 光照函数 (Lighting Functions) 在 Lighting 中添加一个 IncomingLight 函数，用于计算给定表面和光源的入射光量。对于任意光照方向，我们必须计算表面法线和方向的点积。我们可以使用 dot 函数。结果应该由灯光的颜色调制。 float3 IncomingLight (Surface surface, Light light) { return dot(surface.normal, light.direction) * light.color; } 什么是点积（dot product） 什么是点积（dot product）？ 两个向量之间的点积在几何上定义为 $A \\cdot B = ||A|| ||B|| \\cos{\\theta}$。这意味着它是向量之间夹角的余弦值，再乘以它们的长度。所以在两个单位长度向量的情况下，$A \\cdot B = \\cos{\\theta}$。 在代数上，它被定义为 $A \\cdot B = \\sum_{i=1}^{n} A_i B_i = A_1 B_1 + A_2 B_2 + \\dots + A_n B_n$。这意味着你可以通过将所有分量相乘并求和来计算它。 float dotProduct = a.x * b.x + a.y * b.y + a.z * b.z; 点积 在视觉上，此操作将一个向量直接投影到另一个向量上，就像在其上投射阴影一样。这样做，你最终会得到一个直角三角形，其底边的长度就是点积的结果。如果两个向量都是单位长度，那就是它们夹角的余弦值。 但这只有在表面朝向灯光时才正确。当点积为负时，我们必须将其约束为零，我们可以通过 saturate 函数来实现。 float3 IncomingLight (Surface surface, Light light) { return saturate(dot(surface.normal, light.direction)) * light.color; } saturate 的作用是什么？ 它将值约束在 0 和 1 之间（含 0 和 1）。我们只需要指定最小值，因为点积永远不应该大于 1，但饱和（saturation）是着色器的一种常见操作，通常是免费的操作修饰符。 添加另一个 GetLighting 函数，它返回表面和灯光的最终光照。目前，它是入射光乘以表面颜色。在另一个函数上方定义它。 float3 GetLighting (Surface surface, Light light) { return IncomingLight(surface, light) * surface.color; } 最后，调整只有一个 Surface 参数的 GetLighting 函数，使其调用另一个函数，使用 GetDirectionalLight 提供灯光数据。 float3 GetLighting (Surface surface) { return GetLighting(surface, GetDirectionalLight()); } 2.3 发送灯光数据到 GPU (Sending Light Data to the GPU) 我们不应该总是使用来自上方的白光，而应该使用当前场景的灯光。默认场景自带一个代表太阳的方向光，颜色略带黄色——十六进制 FFF4D6——并且绕 X 轴旋转 50°，绕 Y 轴旋转 -30°。如果这种灯光不存在，请创建一个。 为了使灯光的数据在着色器中可访问，我们必须为其创建统一值（uniform values），就像着色器属性一样。在这种情况下，我们将定义两个 float3 向量：_DirectionalLightColor 和 _DirectionalLightDirection。将它们放在 Light 顶部定义的 _CustomLight 缓冲区中。 CBUFFER_START(_CustomLight) float3 _DirectionalLightColor; float3 _DirectionalLightDirection; CBUFFER_END 在 GetDirectionalLight 中使用这些值而不是常量。 Light GetDirectionalLight () { Light light; light.color = _DirectionalLightColor; light.direction = _DirectionalLightDirection; return light; } 现在我们的 RP 必须将灯光数据发送到 GPU。我们将为此创建一个新的 Lighting 类。它的工作方式类似于 CameraRenderer，但用于灯光。给它一个带有 context 参数的公共 Setup 方法，在其中调用一个单独的 SetupDirectionalLight 方法。虽然不是严格必要，但我们也给它一个专门的命令缓冲区，我们在完成后执行它，这对于调试很方便。另一种选择是添加一个缓冲区参数。 using UnityEngine; using UnityEngine.Rendering; public class Lighting { const string bufferName = \"Lighting\"; CommandBuffer buffer = new CommandBuffer { name = bufferName }; public void Setup (ScriptableRenderContext context) { buffer.BeginSample(bufferName); SetupDirectionalLight(); buffer.EndSample(bufferName); context.ExecuteCommandBuffer(buffer); buffer.Clear(); } void SetupDirectionalLight () {} } 跟踪这两个着色器属性的标识符。 static int dirLightColorId = Shader.PropertyToID(\"_DirectionalLightColor\"), dirLightDirectionId = Shader.PropertyToID(\"_DirectionalLightDirection\"); 我们可以通过 RenderSettings.sun 访问场景的主光源。默认情况下，这会得到最重要的方向光，也可以在 Window / Rendering / Lighting Settings 中显式配置。使用 CommandBuffer.SetGlobalVector 将灯光数据发送到 GPU。颜色是灯光在线性空间中的颜色，而方向是灯光变换的前向向量取反。 void SetupDirectionalLight () { Light light = RenderSettings.sun; buffer.SetGlobalVector(dirLightColorId, light.color.linear); buffer.SetGlobalVector(dirLightDirectionId, -light.transform.forward); } SetGlobalVector 不需要 Vector4 吗？ 是的，发送到 GPU 的向量始终有四个分量，即使我们定义的分量较少。额外的分量在着色器中被隐式掩码。同样，存在从 Vector3 到 Vector4 的隐式转换，尽管反向不行。 灯光的颜色属性是其配置的颜色，但灯光也有一个单独的强度因子。最终颜色是两者相乘的结果。 buffer.SetGlobalVector( dirLightColorId, light.color.linear * light.intensity ); 为 CameraRenderer 提供一个 Lighting 实例，并在绘制可见几何体之前使用它来设置光照。 Lighting lighting = new Lighting(); public void Render ( ScriptableRenderContext context, Camera camera, bool useDynamicBatching, bool useGPUInstancing ) { … Setup(); lighting.Setup(context); DrawVisibleGeometry(useDynamicBatching, useGPUInstancing); DrawUnsupportedShaders(); DrawGizmos(); Submit(); } 被照亮 2.4 可见光 (Visible Lights) 在剔除时，Unity 还会确定哪些灯光影响摄像机可见的空间。我们可以依靠这些信息而不是全局太阳。为此，Lighting 需要访问剔除结果，因此为 Setup 添加一个参数并在字段中存储它以便于使用。然后我们可以支持多个灯光，因此将 SetupDirectionalLight 的调用替换为新的 SetupLights 方法。 CullingResults cullingResults; public void Setup ( ScriptableRenderContext context, CullingResults cullingResults ) { this.cullingResults = cullingResults; buffer.BeginSample(bufferName); //SetupDirectionalLight(); SetupLights(); … } void SetupLights () {} 在 CameraRenderer.Render 中调用 Setup 时添加剔除结果作为参数。 lighting.Setup(context, cullingResults); 现在 Lighting.SetupLights 可以通过剔除结果的 visibleLights 属性检索所需数据。它是以 Unity.Collections.NativeArray 形式提供的，元素类型为 VisibleLight。 using Unity.Collections; using UnityEngine; using UnityEngine.Rendering; public class Lighting { … void SetupLights () { NativeArray&lt;VisibleLight&gt; visibleLights = cullingResults.visibleLights; } … } 什么是 NativeArray？ 它是一个行为类似于数组的结构体，但提供了与本机内存缓冲区的连接。它使得在托管 C# 代码和本机 Unity 引擎代码之间高效共享数据成为可能。 2.5 多个方向光 (Multiple Directional Lights) 使用可见光数据使得支持多个方向光成为可能，但我们必须将所有这些灯光的数据发送到 GPU。因此，我们将使用两个 Vector4 数组加上一个用于灯光数量的整数，而不是一对向量。我们还将定义方向光的最大数量，我们可以使用它来初始化两个数组字段以缓冲数据。让我们将最大值设置为 4，这对于大多数场景应该足够了。 const int maxDirLightCount = 4; static int //dirLightColorId = Shader.PropertyToID(\"_DirectionalLightColor\"), //dirLightDirectionId = Shader.PropertyToID(\"_DirectionalLightDirection\"); dirLightCountId = Shader.PropertyToID(\"_DirectionalLightCount\"), dirLightColorsId = Shader.PropertyToID(\"_DirectionalLightColors\"), dirLightDirectionsId = Shader.PropertyToID(\"_DirectionalLightDirections\"); static Vector4[] dirLightColors = new Vector4[maxDirLightCount], dirLightDirections = new Vector4[maxDirLightCount]; 为什么不使用结构化缓冲区（structured buffers）？ 这是可能的，但我不会使用，因为着色器对结构化缓冲区的支持还不够好。要么根本不支持，要么仅在片元程序中支持，或者性能比常规数组差。好消息是，数据在 CPU 和 GPU 之间传递方式的细节仅在少数地方起作用，因此很容易更改。这是使用 Light 结构体的另一个好处。 为 SetupDirectionalLight 添加一个索引和一个 VisibleLight 参数。让它使用提供的索引设置颜色和方向元素。在这种情况下，最终颜色通过 visibleLight.finalColor 属性提供。前向向量可以通过 visibleLight.localToWorldMatrix 属性找到。它是矩阵的第三列，并且再次需要取反。 void SetupDirectionalLight (int index, VisibleLight visibleLight) { dirLightColors[index] = visibleLight.finalColor; dirLightDirections[index] = -visibleLight.localToWorldMatrix.GetColumn(2); } 最终颜色已经应用了灯光的强度，但默认情况下 Unity 不会将其转换为线性空间。我们必须将 GraphicsSettings.lightsUseLinearIntensity 设置为 true，我们可以在 CustomRenderPipeline 的构造函数中执行此操作。 public CustomRenderPipeline ( bool useDynamicBatching, bool useGPUInstancing, bool useSRPBatcher ) { this.useDynamicBatching = useDynamicBatching; this.useGPUInstancing = useGPUInstancing; GraphicsSettings.useScriptableRenderPipelineBatching = useSRPBatcher; GraphicsSettings.lightsUseLinearIntensity = true; } 接下来，循环遍历 Lighting.SetupLights 中的所有可见光并为每个元素调用 SetupDirectionalLight。然后在缓冲区上调用 SetGlobalInt 和 SetGlobalVectorArray 以将数据发送到 GPU。 NativeArray&lt;VisibleLight&gt; visibleLights = cullingResults.visibleLights; for (int i = 0; i &lt; visibleLights.Length; i++) { VisibleLight visibleLight = visibleLights[i]; SetupDirectionalLight(i, visibleLight); } buffer.SetGlobalInt(dirLightCountId, visibleLights.Length); buffer.SetGlobalVectorArray(dirLightColorsId, dirLightColors); buffer.SetGlobalVectorArray(dirLightDirectionsId, dirLightDirections); 但我们只支持最多四个方向光，所以当我们达到该最大值时应该中止循环。让我们保持方向光索引与循环迭代器分开。 int dirLightCount = 0; for (int i = 0; i &lt; visibleLights.Length; i++) { VisibleLight visibleLight = visibleLights[i]; SetupDirectionalLight(dirLightCount++, visibleLight); if (dirLightCount &gt;= maxDirLightCount) { break; } } buffer.SetGlobalInt(dirLightCountId, dirLightCount); 因为我们只支持方向光，所以我们应该忽略其他类型的灯光。我们可以通过检查可见光的 lightType 属性是否等于 LightType.Directional 来实现这一点。 VisibleLight visibleLight = visibleLights[i]; if (visibleLight.lightType == LightType.Directional) { SetupDirectionalLight(dirLightCount++, visibleLight); if (dirLightCount &gt;= maxDirLightCount) { break; } } 这可行，但 VisibleLight 结构体相当大。理想情况下，我们只从原生数组中检索它一次，而且不要将其作为常规参数传递给 SetupDirectionalLight，因为这会复制它。我们可以使用 Unity 对 ScriptableRenderContext.DrawRenderers 方法使用的相同技巧，即通过引用传递参数。 SetupDirectionalLight(dirLightCount++, ref visibleLight); 这要求我们也将会参数定义为引用。 void SetupDirectionalLight (int index, ref VisibleLight visibleLight) { … } 2.6 着色器循环 (Shader Loop) 调整 Light 中的 _CustomLight 缓冲区以使其匹配我们的新数据格式。在这种情况下，我们将明确地为数组类型使用 float4。数组在着色器中具有固定大小，无法调整大小。确保使用我们在 Lighting 中定义的相同最大值。 #define MAX_DIRECTIONAL_LIGHT_COUNT 4 CBUFFER_START(_CustomLight) int _DirectionalLightCount; float4 _DirectionalLightColors[MAX_DIRECTIONAL_LIGHT_COUNT]; float4 _DirectionalLightDirections[MAX_DIRECTIONAL_LIGHT_COUNT]; CBUFFER_END 添加一个获取方向光数量的函数，并调整 GetDirectionalLight 以使其检索特定灯光索引的数据。 int GetDirectionalLightCount () { return _DirectionalLightCount; } Light GetDirectionalLight (int index) { Light light; light.color = _DirectionalLightColors[index].rgb; light.direction = _DirectionalLightDirections[index].xyz; return light; } rgb 和 xyz 之间有区别吗？ 它们是语义别名。使用 rgba 和 xyzw 进行混写（Swizzling）是等效的。 然后调整表面的 GetLighting，使其使用 for 循环来累加所有方向光的贡献。 float3 GetLighting (Surface surface) { float3 color = 0.0; for (int i = 0; i &lt; GetDirectionalLightCount(); i++) { color += GetLighting(surface, GetDirectionalLight(i)); } return color; } 四个方向光 现在我们的着色器支持最多四个方向光。通常只需要一个方向光来代表太阳或月亮，但也许在拥有多个太阳的行星上有一个场景。方向光也可以用来近似多个大型灯光装置，例如大型体育场的灯光。 如果你的游戏始终只有一个方向光，那么你可以去掉循环，或者制作多个着色器变体。但在本教程中，我们将保持简单，坚持使用一个通用的循环。最好的性能始终是通过剥离所有你不需要的东西来实现的，尽管它并不总是产生显著差异。 2.7 着色器目标级别 (Shader Target Level) 使用变量长度的循环过去对着色器来说是个问题，但现代 GPU 可以毫无问题地处理它们，特别是当绘制调用的所有片元都以相同的方式迭代相同的数据时。但是，OpenGL ES 2.0 和 WebGL 1.0 图形 API 默认无法处理此类循环。我们可以通过合并硬编码的最大值来使其工作，例如让 GetDirectionalLight 返回 min(_DirectionalLightCount, MAX_DIRECTIONAL_LIGHT_COUNT)。这使得展开（unroll）循环成为可能，将其转换为一系列条件代码块。不幸的是，生成的着色器代码一团糟，性能会迅速下降。在非常陈旧的硬件上，所有代码块将始终执行，它们的贡献通过条件分配来控制。虽然我们可以使其工作，但它会使代码变得更复杂，因为我们也必须做出其他调整。所以我选择忽略这些限制，为了简单起见在构建中关闭 WebGL 1.0 和 OpenGL ES 2.0 支持。它们无论如何都不支持线性光照。我们还可以通过 #pragma target 3.5 指令将着色器 Pass 的目标级别提高到 3.5 来避免为它们编译 OpenGL ES 2.0 着色器变体。让我们保持一致，对两个着色器都这样做。 HLSLPROGRAM #pragma target 3.5 … ENDHLSL 3 BRDF 我们目前使用一个非常简单的光照模型，仅适用于完全漫反射表面。我们可以通过应用双向反射分布函数（BRDF）来实现更多样化、更真实的光照。此类函数有很多。我们将使用通用管线（URP）所使用的那一个，它权衡了一些真实感以换取性能。 3.1 入射光 (Incoming Light) 当光束正面照射到表面片元时，其所有能量都将影响该片元。为简单起见，我们将假设光束的宽度与片元的宽度匹配。这就是光线方向 $L$ 和表面法线 $N$ 对齐的情况，因此 $N \\cdot L = 1$。当它们不对齐时，光束的至少一部分会错过表面片元，因此影响片元的能量较少。影响片元的能量部分是 $N \\cdot L$。负值结果意味着表面背离光源，因此不受其影响。 入射光部分 3.2 出射光 (Outgoing Light) 我们不会直接看到到达表面的光。我们只看到从表面反弹并到达摄像机或我们眼睛的那部分。如果表面是一个完全平坦的镜子，那么光线会反射出去，出射角等于入射角。只有当摄像机与该光线对齐时，我们才能看到此光线。这被称为高光反射（specular reflection）。这是光交互的简化，但对于我们的目的来说已经足够了。 完全高光反射 但如果表面不是完全平坦的，那么光线就会被散射，因为该片元实际上由许多具有不同方向的更小片元组成。这会将光束分解成走向不同方向的小光束，从而有效地模糊了高光反射。即使没有与完美反射方向对齐，我们最终也可能会看到一些散射光。 散射高光反射 除此之外，光线还会穿透表面，到处反弹，并以不同的角度射出，以及其他我们不需要考虑的事情。取极端情况，我们最终会得到一个完美的漫反射表面，它向所有可能的方向均匀散射光。这就是我们目前在着色器中计算的光照。 完美漫反射反射 无论摄像机位于何处，从表面接收到的漫反射光量都是相同的。但这意味着我们观察到的光能远小于到达表面 fragment 的光能。这表明我们应该用某个系数来缩放进入的光线。然而，由于该系数始终保持不变，我们可以将其直接合并到光的颜色和强度中。因此，我们使用的最终光色代表了在正面照射下，从一个完美的白色漫反射表面 fragment 反射时观察到的光量。这只是实际发射的总光量的一小部分。还有其他配置灯光的方法，例如通过指定流明（lumen）或（lux），这使得配置现实的光源更加容易，但我们将坚持目前的方法。 3.3 表面属性 (Surface Properties) 表面可以是完全漫反射、完美镜面反射或介于两者之间的任何状态。我们可以通过多种方式控制这一点。我们将使用金属度工作流（metallic workflow），这要求我们在 Lit 着色器中添加两个表面属性。 第一个属性是表面是金属还是非金属，也称为电介质（dielectric）。由于表面可以包含两者的混合，我们将为其添加一个 $0-1$ 范围的滑块，1 表示完全金属。默认值为完全电介质。 第二个属性控制表面的平滑程度。我们也将为此使用 $0-1$ 范围的滑块，0 表示完全粗糙，1 表示完全平滑。我们将使用 0.5 作为默认值。 _Metallic (\"Metallic\", Range(0, 1)) = 0 _Smoothness (\"Smoothness\", Range(0, 1)) = 0.5 带有金属度和光滑度滑块的材质 将这些属性添加到 UnityPerMaterial 缓冲区中。 UNITY_INSTANCING_BUFFER_START(UnityPerMaterial) UNITY_DEFINE_INSTANCED_PROP(float4, _BaseMap_ST) UNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor) UNITY_DEFINE_INSTANCED_PROP(float, _Cutoff) UNITY_DEFINE_INSTANCED_PROP(float, _Metallic) UNITY_DEFINE_INSTANCED_PROP(float, _Smoothness) UNITY_INSTANCING_BUFFER_END(UnityPerMaterial) 同时也添加到 Surface 结构体中。 struct Surface { float3 normal; float3 color; float alpha; float metallic; float smoothness; }; 在 LitPassFragment 中将它们复制到表面。 Surface surface; surface.normal = normalize(input.normalWS); surface.color = base.rgb; surface.alpha = base.a; surface.metallic = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _Metallic); surface.smoothness = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _Smoothness); 并为 PerObjectMaterialProperties 添加对它们的支持。 static int baseColorId = Shader.PropertyToID(\"_BaseColor\"), cutoffId = Shader.PropertyToID(\"_Cutoff\"), metallicId = Shader.PropertyToID(\"_Metallic\"), smoothnessId = Shader.PropertyToID(\"_Smoothness\"); … [SerializeField, Range(0f, 1f)] float alphaCutoff = 0.5f, metallic = 0f, smoothness = 0.5f; … void OnValidate () { … block.SetFloat(metallicId, metallic); block.SetFloat(smoothnessId, smoothness); GetComponent&lt;Renderer&gt;().SetPropertyBlock(block); } 3.4 BRDF 属性 (BRDF Properties) 我们将使用表面属性来计算 BRDF 方程。它告诉我们最终看到的从表面反射了多少光，这是漫反射和高光反射的组合。我们需要将表面颜色分为漫反射部分和高光部分，我们还需要知道表面的粗糙程度。让我们在 ShaderLibrary 文件夹中一个单独的 BRDF.hlsl 文件中跟踪这三个值。 #ifndef CUSTOM_BRDF_INCLUDED #define CUSTOM_BRDF_INCLUDED struct BRDF { float3 diffuse; float3 specular; float roughness; }; #endif 添加一个获取给定表面的 BRDF 数据的函数。从完美的漫反射表面开始，因此漫反射部分等于表面颜色，而高光部分为黑色，粗糙度为 1。 BRDF GetBRDF (Surface surface) { BRDF brdf; brdf.diffuse = surface.color; brdf.specular = 0.0; brdf.roughness = 1.0; return brdf; } 在 Light 之后、Lighting 之前包含 BRDF。 #include \"../ShaderLibrary/Common.hlsl\" #include \"../ShaderLibrary/Surface.hlsl\" #include \"../ShaderLibrary/Light.hlsl\" #include \"../ShaderLibrary/BRDF.hlsl\" #include \"../ShaderLibrary/Lighting.hlsl\" 为两个 GetLighting 函数添加一个 BRDF 参数，然后将入射光乘以漫反射部分而不是整个表面颜色。 float3 GetLighting (Surface surface, BRDF brdf, Light light) { return IncomingLight(surface, light) * brdf.diffuse; } float3 GetLighting (Surface surface, BRDF brdf) { float3 color = 0.0; for (int i = 0; i &lt; GetDirectionalLightCount(); i++) { color += GetLighting(surface, brdf, GetDirectionalLight(i)); } return color; } 最后，在 LitPassFragment 中获取 BRDF 数据并将其传递给 GetLighting。 BRDF brdf = GetBRDF(surface); float3 color = GetLighting(surface, brdf); 3.5 反射率 (Reflectivity) 表面的反射程度各不相同，但一般金属通过高光反射反射所有光线，漫反射反射为零。所以我们将声明反射率等于金属度表面属性。被反射的光不会被漫反射，所以我们应该在 GetBRDF 中通过 1 减去反射率来缩放漫反射颜色。 float oneMinusReflectivity = 1.0 - surface.metallic; brdf.diffuse = surface.color * oneMinusReflectivity; 金属度为 0、0.25、0.5、0.75 和 1 的白色球体 实际上，一些光也会从电介质表面反弹，这给它们带来了亮点。非金属的反射率各不相同，但平均约为 0.04。让我们将其定义为最小反射率，并添加一个 OneMinusReflectivity 函数，将范围从 0-1 调整为 0-0.96。此范围调整与通用管线（URP）的方法匹配。 #define MIN_REFLECTIVITY 0.04 float OneMinusReflectivity (float metallic) { float range = 1.0 - MIN_REFLECTIVITY; return range - metallic * range; } 在 GetBRDF 中使用该函数来强制执行最小值。在仅渲染漫反射反射时差异几乎察觉不到，但在我们添加高光反射时会很重要。没有它，非金属将不会获得高光。 float oneMinusReflectivity = OneMinusReflectivity(surface.metallic); 3.6 高光颜色 (Specular Color) 被反射走的光线不能再以另一种方式被反射。这被称为能量守恒，这意味着出射光量不能超过入射光量。这表明高光颜色应该等于表面颜色减去漫反射颜色。 brdf.diffuse = surface.color * oneMinusReflectivity; brdf.specular = surface.color - brdf.diffuse; 但是，这忽略了金属影响高光反射颜色而非金属不影响这一事实。电介质表面的高光颜色应该是白色的，我们可以通过使用金属度属性在最小反射率和表面颜色之间进行插值来实现。 brdf.specular = lerp(MIN_REFLECTIVITY, surface.color, surface.metallic); 3.7 粗糙度 (Roughness) 粗糙度是平滑度的相反面，所以我们可以简单地用 1 减去平滑度。Core RP 库有一个执行此操作的函数，名为 PerceptualSmoothnessToPerceptualRoughness。我们将使用此函数，以明确平滑度以及粗糙度都被定义为感知的。我们可以通过 PerceptualRoughnessToRoughness 函数转换为实际的粗糙度值，该函数对感知值进行平方。这匹配 Disney 光照模型。之所以这样做，是因为在编辑材质时调整感知版本更直观。 float perceptualRoughness = PerceptualSmoothnessToPerceptualRoughness(surface.smoothness); brdf.roughness = PerceptualRoughnessToRoughness(perceptualRoughness); 这些函数在 Core RP 库的 CommonMaterial.hlsl 文件中定义。在包含核心的 Common 之后将其包含在我们的 Common 文件中。 #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/CommonMaterial.hlsl\" #include \"UnityInput.hlsl\" 3.8 视角方向 (View Direction) 为了确定摄像机与完美反射方向的对齐程度，我们需要知道摄像机的位置。Unity 通过 float3 _WorldSpaceCameraPos 提供此数据，所以将其添加到 UnityInput 中。 float3 _WorldSpaceCameraPos; 为了获得视角方向——从表面到摄像机的方向——在 LitPassFragment 中，我们需要在 Varyings 中添加世界空间表面位置。 struct Varyings { float4 positionCS : SV_POSITION; float3 positionWS : VAR_POSITION; … }; Varyings LitPassVertex (Attributes input) { … output.positionWS = TransformObjectToWorld(input.positionOS); output.positionCS = TransformWorldToHClip(output.positionWS); … } 我们将视角方向视为表面数据的一部分，因此将其添加到 Surface 中。 struct Surface { float3 normal; float3 viewDirection; float3 color; float alpha; float metallic; float smoothness; }; 在 LitPassFragment 中分配它。它等于摄像机位置减去片元位置，再归一化。 surface.normal = normalize(input.normalWS); surface.viewDirection = normalize(_WorldSpaceCameraPos - input.positionWS); 3.9 高光强度 (Specular Strength) 我们观察到的高光反射强度取决于我们的视角方向与完美反射方向的匹配程度。我们将使用与通用管线（URP）相同的公式，它是 Minimalist CookTorrance BRDF 的变体。该公式包含一些平方，所以让我们首先为 Common 添加一个方便的 Square 函数。 float Square (float v) { return v * v; } 然后为 BRDF 添加一个以表面、BRDF 数据和灯光为参数的 SpecularStrength 函数。它应该计算 $\\frac{r^2}{d^2 \\max (0.1, (L \\cdot H)^2) n}$，其中 $r$ 是粗糙度，所有点积都应该饱和。此外，$d = (N \\cdot H)^2(r^2 - 1) + 1.0001$，$N$ 是表面法线，$L$ 是灯光方向，$H = L + V$ 归一化，它是灯光和视角方向之间的半程向量（halfway vector）。使用 SafeNormalize 函数来归一化该向量，以避免在向量相反的情况下除以零。最后，$n = 4r + 2$ 且是一个归一化项。 float SpecularStrength (Surface surface, BRDF brdf, Light light) { float3 h = SafeNormalize(light.direction + surface.viewDirection); float nh2 = Square(saturate(dot(surface.normal, h))); float lh2 = Square(saturate(dot(light.direction, h))); float r2 = Square(brdf.roughness); float d2 = Square(nh2 * (r2 - 1.0) + 1.0001); float normalization = brdf.roughness * 4.0 + 2.0; return r2 / (d2 * max(0.1, lh2) * normalization); } 那个函数是如何工作的？ BRDF 理论太复杂，无法简短地完全解释，而且也不是本教程的重点。你可以查看 URP 的 Lighting.hlsl 文件以获取一些代码文档和参考。 接下来，添加一个 DirectBRDF，它返回通过直接光照获得颜色，给定表面、BRDF 和灯光。结果是高光颜色由高光强度调制，加上漫反射颜色。 float3 DirectBRDF (Surface surface, BRDF brdf, Light light) { return SpecularStrength(surface, brdf, light) * brdf.specular + brdf.diffuse; } GetLighting 然后必须将入射光乘以该函数的结果。 float3 GetLighting (Surface surface, BRDF brdf, Light light) { return IncomingLight(surface, light) * DirectBRDF(surface, brdf, light); } 平滑度从上到下分别为 0、0.25、0.5、0.75 和 0.95 我们现在获得了高光反射，这为我们的表面增加了亮点。对于完全粗糙的表面，亮点模拟漫反射反射。平滑的表面获得更集中的亮点。完全平滑的表面获得极其微小的亮点，我们看不见。需要一些散射才能使其可见。 由于能量守恒，平滑表面的亮点可能会变得非常亮，因为到达表面片元的大部分光都变得集中了。因此，我们最终看到的光比漫反射反射可能看到的光要多得多。你可以通过将最终渲染颜色大幅缩小来验证这一点。 最终颜色除以 100 你也可以通过使用白色以外的基础颜色来验证金属会影响高光反射的颜色而非金属不影响。 蓝色基础颜色 我们现在有了功能性的、可信的直接光照，尽管目前结果太暗了——特别是对于金属——因为我们还不支持环境反射。此时均匀的黑色环境会比默认的天空盒更真实，但这会使我们的对象更难看到。添加更多灯光也有效。 四个灯光 3.10 网格球 (Mesh Ball) 让我们也为 MeshBall 添加对变化的金属度和光滑度属性的支持。这需要添加两个 float 数组。 static int baseColorId = Shader.PropertyToID(\"_BaseColor\"), metallicId = Shader.PropertyToID(\"_Metallic\"), smoothnessId = Shader.PropertyToID(\"_Smoothness\"); … float[] metallic = new float[1023], smoothness = new float[1023]; … void Update () { if (block == null) { block = new MaterialPropertyBlock(); block.SetVectorArray(baseColorId, baseColors); block.SetFloatArray(metallicId, metallic); block.SetFloatArray(smoothnessId, smoothness); } Graphics.DrawMeshInstanced(mesh, 0, material, matrices, 1023, block); } 让我们在 Awake 中让 25% 的实例具有金属感，并让平滑度从 0.05 变化到 0.95。 baseColors[i] = new Vector4( Random.value, Random.value, Random.value, Random.Range(0.5f, 1f) ); metallic[i] = Random.value &lt; 0.25f ? 1f : 0f; smoothness[i] = Random.Range(0.05f, 0.95f); 然后让网格球使用受光材质。 受光网格球 4 透明度 (Transparency) 让我们再次考虑透明度。对象仍然基于其 alpha 值淡出，但现在是反射光淡出。这对于漫反射反射是有意义的，因为只有一部分光被反射，而其余光穿过表面。 淡出球体 但是，高光反射也会淡出。在完全透明的玻璃的情况下，光线要么穿过要么被反射。高光反射不会淡出。我们目前的做法无法表示这一点。 4.1 预乘 Alpha (Premultiplied Alpha) 解决方案是只淡出漫反射光，同时保持高光反射为全强度。由于源混合模式应用于所有内容，我们无法使用它，所以让我们将其设置为 1，同时仍然为目标混合模式使用 one-minus-source-alpha。 源混合模式设置为 one 这恢复了高光反射，但漫反射反射不再淡出。我们通过将表面 alpha 因子计入漫反射颜色来解决这个问题。因此，我们对漫反射进行预乘 alpha 处理，而不是稍后依赖 GPU 混合。这种方法被称为预乘 alpha 混合（premultiplied alpha blending）。在 GetBRDF 中执行此操作。 brdf.diffuse = surface.color * oneMinusReflectivity; brdf.diffuse *= surface.alpha; 预乘漫反射 4.2 预乘切换 (Premultiplication Toggle) 将 alpha 与漫反射预乘有效地将对象变成了玻璃，而常规 alpha 混合使对象有效地仅部分存在。让我们通过为 GetBRDF 添加一个布尔参数来支持两者，以控制我们是否预乘 alpha，默认设置为 false。 BRDF GetBRDF (inout Surface surface, bool applyAlphaToDiffuse = false) { … if (applyAlphaToDiffuse) { brdf.diffuse *= surface.alpha; } … } 我们可以在 LitPassFragment 中使用 _PREMULTIPLY_ALPHA 关键字来决定使用哪种方法，类似于我们控制 alpha 裁剪的方式。 #if defined(_PREMULTIPLY_ALPHA) BRDF brdf = GetBRDF(surface, true); #else BRDF brdf = GetBRDF(surface); #endif float3 color = GetLighting(surface, brdf); return float4(color, surface.alpha); 为 Lit 的 Pass 添加关键字的着色器特征（shader feature）。 #pragma shader_feature _CLIPPING #pragma shader_feature _PREMULTIPLY_ALPHA 并为着色器也添加一个切换属性。 [Toggle(_PREMULTIPLY_ALPHA)] _PremulAlpha (\"Premultiply Alpha\", Float) = 0 预乘 alpha 切换 5 着色器 GUI (Shader GUI) 我们现在支持多种渲染模式，每种模式都需要特定设置。为了更轻松地在模式之间切换，让我们为材质检查器添加一些按钮以应用预设配置。 5.1 自定义着色器 GUI (Custom Shader GUI) 在 Lit 着色器的主块底部添加一个 CustomEditor \"CustomShaderGUI\" 语句。 Shader \"Custom RP/Lit\" { … CustomEditor \"CustomShaderGUI\" } 这指示 Unity 编辑器使用 CustomShaderGUI 类的实例来绘制使用 Lit 着色器的材质的检查器。创建一个该类的脚本资产并将其放在一个新的 Custom RP / Editor 文件夹中。 我们需要使用 UnityEditor、UnityEngine 和 UnityEngine.Rendering 命名空间。该类必须继承 ShaderGUI 并重写公共 OnGUI 方法，该方法具有 MaterialEditor 和 MaterialProperty 数组参数。让它调用基类方法，这样我们最终得到默认的检查器。 using UnityEditor; using UnityEngine; using UnityEngine.Rendering; public class CustomShaderGUI : ShaderGUI { public override void OnGUI ( MaterialEditor materialEditor, MaterialProperty[] properties ) { base.OnGUI(materialEditor, properties); } } 5.2 设置属性和关键字 (Setting Properties and Keywords) 为了完成我们的工作，我们需要访问三样东西，我们将它们存储在字段中。首先是材质编辑器，它是负责显示和编辑材质的基础编辑器对象。其次是对正在编辑的材质的引用，我们可以通过编辑器的 targets 属性检索它。它被定义为 Object 数组，因为 targets 是通用 Editor 类的属性。第三是可以编辑的属性数组。 MaterialEditor editor; Object[] materials; MaterialProperty[] properties; public override void OnGUI ( MaterialEditor materialEditor, MaterialProperty[] properties ) { base.OnGUI(materialEditor, properties); editor = materialEditor; materials = materialEditor.targets; this.properties = properties; } 为什么有多个材质？ 可以同时编辑使用相同着色器的多个材质，就像你可以选择并编辑多个游戏对象一样。 要设置属性，我们首先必须在数组中找到它，为此我们可以使用 ShaderGUI.FindProperty 方法，传递名称和属性数组。然后我们可以通过给其 floatValue 属性赋值来调整其值。将其封装在一个方便的、带有名称和值参数的 SetProperty 方法中。 void SetProperty (string name, float value) { FindProperty(name, properties).floatValue = value; } 设置关键字稍微复杂一些。我们将为此创建一个 SetKeyword 方法，它带有一个名称和一个布尔参数，指示是启用还是禁用该关键字。我们必须在所有材质上调用 EnableKeyword 或 DisableKeyword，并向它们传递关键字名称。 void SetKeyword (string keyword, bool enabled) { if (enabled) { foreach (Material m in materials) { m.EnableKeyword(keyword); } } else { foreach (Material m in materials) { m.DisableKeyword(keyword); } } } 让我们还创建一个 SetProperty 变体，用于切换属性-关键字组合。 void SetProperty (string name, string keyword, bool value) { SetProperty(name, value ? 1f : 0f); SetKeyword(keyword, value); } 现在我们可以定义简单的 Clipping、PremultiplyAlpha、SrcBlend、DstBlend 和 ZWrite 设置器属性。 bool Clipping { set =&gt; SetProperty(\"_Clipping\", \"_CLIPPING\", value); } bool PremultiplyAlpha { set =&gt; SetProperty(\"_PremulAlpha\", \"_PREMULTIPLY_ALPHA\", value); } BlendMode SrcBlend { set =&gt; SetProperty(\"_SrcBlend\", (float)value); } BlendMode DstBlend { set =&gt; SetProperty(\"_DstBlend\", (float)value); } bool ZWrite { set =&gt; SetProperty(\"_ZWrite\", value ? 1f : 0f); } 最后，渲染队列通过分配给所有材质的 renderQueue 属性来设置。我们可以为此使用 RenderQueue 枚举。 RenderQueue RenderQueue { set { foreach (Material m in materials) { m.renderQueue = (int)value; } } } 5.3 预设按钮 (Preset Buttons) 可以通过 GUILayout.Button 方法创建一个按钮，并向其传递一个标签，该标签将是预设的名称。如果该方法返回 true，则表示它被按下了。在应用预设之前，我们应该向编辑器注册一个撤消步骤，这可以通过调用 RegisterPropertyChangeUndo 并向其传递名称来完成。由于此代码对于所有预设都是相同的，因此将其放在一个 PresetButton 方法中，该方法返回是否应应用预设。 bool PresetButton (string name) { if (GUILayout.Button(name)) { editor.RegisterPropertyChangeUndo(name); return true; } return false; } 我们将为每个预设创建一个单独的方法，从默认的 Opaque 模式开始。让它在激活时适当地设置属性。 void OpaquePreset () { if (PresetButton(\"Opaque\")) { Clipping = false; PremultiplyAlpha = false; SrcBlend = BlendMode.One; DstBlend = BlendMode.Zero; ZWrite = true; RenderQueue = RenderQueue.Geometry; } } 第二个预设是 Clip，它是 Opaque 的副本，打开了裁剪并将队列设置为 AlphaTest。 void ClipPreset () { if (PresetButton(\"Clip\")) { Clipping = true; PremultiplyAlpha = false; SrcBlend = BlendMode.One; DstBlend = BlendMode.Zero; ZWrite = true; RenderQueue = RenderQueue.AlphaTest; } } 第三个预设是标准透明度，它会淡出对象，所以我们将它命名为 Fade。它是 Opaque 的另一个副本，调整了混合模式和队列，另外没有深度写入。 void FadePreset () { if (PresetButton(\"Fade\")) { Clipping = false; PremultiplyAlpha = false; SrcBlend = BlendMode.SrcAlpha; DstBlend = BlendMode.OneMinusSrcAlpha; ZWrite = false; RenderQueue = RenderQueue.Transparent; } } 第四个预设是 Fade 的变体，它应用预乘 alpha 混合。我们将它命名为 Transparent，因为它适用于具有正确光照的半透明表面。 void TransparentPreset () { if (PresetButton(\"Transparent\")) { Clipping = false; PremultiplyAlpha = true; SrcBlend = BlendMode.One; DstBlend = BlendMode.OneMinusSrcAlpha; ZWrite = false; RenderQueue = RenderQueue.Transparent; } } 在 OnGUI 末尾调用预设方法，这样它们就会显示在默认检查器下方。 public override void OnGUI ( MaterialEditor materialEditor, MaterialProperty[] properties ) { … OpaquePreset(); ClipPreset(); FadePreset(); TransparentPreset(); } 预设按钮 5.4 预设折叠 (Preset Foldout) 预设按钮不会经常使用，所以让我们把它们放在一个默认折叠的折叠栏（foldout）里。这通过调用 EditorGUILayout.Foldout 来完成，传入当前的折叠状态、标签和 true 以指示点击它应该切换其状态。它返回新的折叠状态，我们应该将其存储在一个字段中。仅当折叠栏打开时才绘制按钮。 bool showPresets; … public override void OnGUI ( MaterialEditor materialEditor, MaterialProperty[] properties ) { … EditorGUILayout.Space(); showPresets = EditorGUILayout.Foldout(showPresets, \"Presets\", true); if (showPresets) { OpaquePreset(); ClipPreset(); FadePreset(); TransparentPreset(); } } 预设折叠 5.5 不发光预设 (Presets for Unlit) 我们也可以为我们的 Unlit 着色器使用自定义着色器 GUI。 Shader \"Custom RP/Unlit\" { … CustomEditor \"CustomShaderGUI\" } 但是，激活预设将导致错误，因为我们正试图设置着色器没有的属性。我们可以通过调整 SetProperty 来防范这种情况。让它通过 false 作为额外参数调用 FindProperty，指示如果找不到属性，则不应记录错误。结果将为 null，所以只有在这种情况下才设置值。同时返回属性是否存在。 bool SetProperty (string name, float value) { MaterialProperty property = FindProperty(name, properties, false); if (property != null) { property.floatValue = value; return true; } return false; } 然后调整 SetProperty 的关键字版本，使其仅在相关属性存在时才设置关键字。 void SetProperty (string name, string keyword, bool value) { if (SetProperty(name, value ? 1f : 0f)) { SetKeyword(keyword, value); } } 5.6 无透明度 (No Transparency) 现在预设也适用于使用 Unlit 着色器的材质，尽管在这种情况下 Transparent 模式没有太大意义，因为相关属性不存在。让我们在不相关时隐藏此预设。 首先，添加一个返回属性是否存在的 HasProperty 方法。 bool HasProperty (string name) =&gt; FindProperty(name, properties, false) != null; 其次，创建一个方便的属性来检查 _PremultiplyAlpha 是否存在。 bool HasPremultiplyAlpha =&gt; HasProperty(\"_PremulAlpha\"); 最后，通过在 TransparentPreset 中首先检查该属性，使 Transparent 预设的所有内容都以该属性为条件。 if (HasPremultiplyAlpha &amp;&amp; PresetButton(\"Transparent\")) { … } 不发光材质缺少透明预设 下一篇方向阴影 (Directional Shadows)。" }, { "title": "自定义渲染管线:Draw Calls、Shaders、 Batches (翻译二)", "url": "/posts/drawcalls-shaders-batches/", "categories": "Unity3D, ScriptRenderPipeline", "tags": "SRP, Shader", "date": "2019-10-31 00:00:00 +0800", "content": "编写一个 HLSL shader 支持 SRP batcher、GPU instancing 以及 dynamic batching 为每个对象配置材质属性，并随机绘制多个对象 创建透明（transparent）和镂空（cutout）材质 Shaders 为了绘制物体，CPU 必须告诉 GPU 绘制什么以及如何绘制。绘制的内容通常是一个 mesh。如何绘制则由 shader 定义，它是一组供 GPU 执行的指令。除了 mesh 之外，shader 还需要额外的信息来完成工作，包括物体的变换矩阵（transformation matrices）和材质属性。 Unity 的 LW/Universal 和 HD RP 允许你使用 Shader Graph package 来设计 shader，它会为你生成 shader 代码。但我们的自定义 RP 不支持该功能，因此我们必须亲手编写 shader 代码。这让我们能够完全控制并理解 shader 的工作原理。 Unlit Shader 我们的第一个 shader 将简单地以纯色绘制 mesh，不包含任何光照。可以通过 Assets / Create / Shader 菜单中的选项之一创建 shader asset。 Unlit Shader 是最合适的选择，但我们将从头开始，删除创建的 shader 文件中所有的默认代码。将该 asset 命名为 Unlit ，并将其存放在 Custom RP 下新建的 Shaders 文件夹中。 Unlit shader asset. Shader 的定义类似于一个类，但只需使用 Shader 关键字，后跟一个字符串，该字符串用于在材质的 Shader 下拉菜单中为其创建一个条目。让我们使用 Custom RP/Unlit 。紧随其后的是一个代码块，其中包含更多带有关键字的前缀块。有一个 Properties 块用于定义材质属性，接着是一个 SubShader 块，其中必须包含一个 Pass 块，用于定义一种渲染方式。请创建该结构，内部块暂时保持为空。 Shader \"Custom RP/Unlit\" { Properties {} SubShader { Pass {} } } 这定义了一个最小化的 Shader，它可以编译并允许我们创建一个使用它的材质。 自定义 Unlit 材质 默认的 Shader 实现会将网格渲染为纯白色。材质显示了渲染队列（render queue）的默认属性，它是从 Shader 中自动获取的，并被设置为 2000，这是不透明几何体的默认值。它还有一个启用双面全局光照（double-sided global illumination）的开关，但这对我们来说并不重要。 HLSL Programs 用于编写 shader 代码的语言是高级着色语言（High-Level Shading Language），简称 HLSL。我们需要将其放置在 Pass 代码块中，位于 HLSLPROGRAM 和 ENDHLSL 关键字之间。这样做是因为在 Pass 代码块中也可以放置其他非 HLSL 代码。 Pass { HLSLPROGRAM ENDHLSL } 为了绘制 mesh，GPU 必须对其所有三角形进行光栅化，将其转换为像素数据。它通过将顶点坐标从 3D 空间变换到 2D 可视化空间，然后填充被所得三角形覆盖的所有像素来实现这一点。这两个步骤由两个独立的 shader 程序控制，我们必须同时定义它们。第一个被称为顶点内核/程序/着色器（vertex kernel/program/shader），第二个被称为片元内核/程序/着色器（fragment kernel/program/shader）。一个片元（fragment）对应一个显示像素或纹理纹素（texel），尽管它可能不代表最终结果，因为稍后当有东西绘制在它上面时，它可能会被覆盖。 HLSLPROGRAM #pragma vertex UnlitPassVertex #pragma fragment UnlitPassFragment ENDHLSL shader 编译器现在会提示找不到声明的 shader kernels。我们需要编写同名的 HLSL 函数来定义它们的实现。我们可以直接在 pragma 指令下方编写，但我们将把所有 HLSL 代码放在一个单独的文件中。具体来说，我们将在同一个 asset 文件夹中使用一个 UnlitPass.hlsl 文件。我们可以通过添加一个带有文件相对路径的 #include 指令，来指示 shader 编译器插入该文件的内容。 HLSLPROGRAM ... #include \"UnlitPass.hlsl\" ENDHLSL Unity 没有创建 HLSL 文件的便捷菜单选项，因此你必须执行类似以下的操作：复制 shader 文件，将其重命名为 UnlitPass ，在外部将其文件扩展名更改为 hlsl 并清空其内容。 UnlitPass HLSL asset file. Include Guard HLSL 文件用于对代码进行分组，就像 C# 类一样，尽管 HLSL 并没有类的概念。除了代码块的局部作用域外，只有一个全局作用域。因此，所有内容在任何地方都是可以访问的。包含（include）一个文件也不等同于使用命名空间。它会在 include 指令所在的位置插入文件的全部内容，所以如果你多次包含同一个文件，就会得到重复的代码，这极有可能导致编译错误。为了防止这种情况，我们将为 UnlitPass.hlsl 添加一个 include guard。 可以使用 #define 指令来定义任何标识符，通常使用大写字母。我们将使用它在文件顶部定义 CUSTOM_UNLIT_PASS_INCLUDED。 #ifndef CUSTOM_UNLIT_PASS_INCLUDED #define CUSTOM_UNLIT_PASS_INCLUDED #endif 如果宏已经被定义，那么 #ifndef 之后的所有代码都将被跳过，从而不会被编译。我们必须在文件末尾添加 #endif 指令来结束其作用域。 Shader Functions 我们在包含保护（include guard）的作用域内定义 shader 函数。它们的写法就像没有访问修饰符的 C# 方法。先从什么都不做的简单 void 函数开始。 #ifndef CUSTOM_UNLIT_PASS_INCLUDED #define CUSTOM_UNLIT_PASS_INCLUDED void UnlitPassVertex () {} void UnlitPassFragment () {} #endif 为了生成有效的输出，我们必须让 fragment 函数返回一个颜色。该颜色由一个包含红、绿、蓝和 alpha 分量的四分量 $float4$ 向量定义。我们可以通过 $float4(0.0, 0.0, 0.0, 0.0)$ 来定义纯黑色，但也可以只写一个零，因为单个数值会自动扩展为完整的向量。由于我们正在创建一个不透明的 shader，因此 alpha 值并不重要，写零即可。 此时着色器编译器会报错，因为我们的函数缺少语义（semantics）。我们必须指明返回值的含义，因为我们可能会产生许多具有不同含义的数据。在这种情况下，我们提供渲染目标的默认系统值，方法是在 UnlitPassFragment 的参数列表后写一个冒号，后跟 SV_TARGET。 float4 UnlitPassFragment () : SV_TARGET { \treturn 0.0; } UnlitPassVertex 负责变换顶点位置，因此应该返回一个位置。这同样是一个 $float4$ 向量，因为它必须被定义为齐次裁剪空间位置，我们稍后会详细介绍。我们再次从零向量开始，在这种情况下，我们必须指明其含义为 SV_POSITION。 float4 UnlitPassVertex () : SV_POSITION { \treturn 0.0; } Space Transformation 当所有顶点都被设置为$0$时，mesh 会塌陷为一个点，并且不会渲染任何内容。vertex function 的主要任务是将原始顶点位置转换到正确的空间。调用该函数时，如果我们提出请求，它将获得可用的顶点数据。我们通过向 UnlitPassVertex 添加参数来实现这一点。我们需要在对象空间（object space）中定义的顶点位置，因此我们将其命名为 positionOS，采用与 Unity 新 RP 相同的命名约定。该位置的类型是 $float3$，因为它是一个 3D 点。让我们最初先返回它，并通过 $float4(positionOS, 1.0)$ 添加 1 作为所需的第四个分量。 float4 UnlitPassVertex (float3 positionOS) : SV_POSITION { \treturn float4(positionOS, 1.0); } 我们还需要为输入添加语义，因为顶点数据可以包含的不仅仅是位置。在这种情况下，我们需要在参数名称后直接加上一个冒号来添加 POSITION。 float4 UnlitPassVertex (float3 positionOS : POSITION) : SV_POSITION { \treturn float4(positionOS, 1.0); } 对象空间位置 网格再次显示出来，但这是错误的，因为我们输出的位置是在错误的空间中。空间转换需要矩阵，这些矩阵在绘制时会被发送到 GPU。我们必须将这些矩阵添加到我们的着色器中，但由于它们总是相同的，我们将 Unity 提供的标准输入放在一个单独的 HLSL 文件中，既是为了保持代码结构化，也是为了能够将代码包含在其他着色器中。添加一个 UnityInput.hlsl 文件，并将其放在直接位于 Custom RP 下的 ShaderLibrary 文件夹中，以镜像 Unity 资源包的文件夹结构。 ShaderLibrary 文件夹与 UnityInput 文件 文件开头先编写一个 include guard CUSTOM_UNITY_INPUT_INCLUDED ，然后在全局作用域内定义一个名为 unity_ObjectToWorld 的 $float4x4$ 矩阵。在 C# 类中这相当于定义一个字段，但在着色器中它被称为 uniform 值。它由 GPU 在每次 draw 时设置一次，并在该次 draw 期间的所有 vertex 和 fragment 函数调用中保持不变（即 uniform，统一的）。 #ifndef CUSTOM_UNITY_INPUT_INCLUDED #define CUSTOM_UNITY_INPUT_INCLUDED float4x4 unity_ObjectToWorld; #endif 我们可以使用矩阵将坐标从对象空间转换到世界空间。由于这是常见功能，让我们为此创建一个函数，并将其放在另一个文件中，这次是放在同一个 ShaderLibrary 文件夹的 Common.hlsl 文件里。我们在其中包含 UnityInput ，然后声明一个 TransformObjectToWorld 函数，将 float3 作为输入和输出。 #ifndef CUSTOM_COMMON_INCLUDED #define CUSTOM_COMMON_INCLUDED #include \"UnityInput.hlsl\" float3 TransformObjectToWorld (float3 positionOS) { \treturn 0.0; } \t #endif 空间转换是通过调用带有矩阵和向量的 mul 函数来完成的。在这种情况下，我们确实需要一个 4D 向量，但由于其第四个分量始终为 1，我们可以通过使用 $float4(positionOS, 1.0)$ 自行添加。结果同样是一个第四分量始终为 1 的 4D 向量。我们可以通过访问向量的 xyz 属性从中提取前三个分量，这被称为 swizzle 操作。 float3 TransformObjectToWorld (float3 positionOS) { \treturn mul(unity_ObjectToWorld, float4(positionOS, 1.0)).xyz; } 我们现在可以在 UnlitPassVertex 中转换到世界空间。首先在函数正上方包含 Common.hlsl 。由于它存在于不同的文件夹中，我们可以通过相对路径 ../ShaderLibrary/Common.hlsl 访问它。然后使用 TransformObjectToWorld 计算 positionWS 变量，并返回它以替代对象空间位置。 #include \"../ShaderLibrary/Common.hlsl\" float4 UnlitPassVertex (float3 positionOS : POSITION) : SV_POSITION { \tfloat3 positionWS = TransformObjectToWorld(positionOS.xyz); \treturn float4(positionWS, 1.0); } 结果仍然错误，因为我们需要一个在齐次裁剪空间中的位置。这个空间定义了一个立方体，其中包含相机视野内的一切，在透视相机的情况下，它会变形为梯形。从世界空间转换到这个空间可以通过乘以视图投影矩阵来完成，该矩阵考虑了相机的位置、方向、投影、视场和近远裁剪平面。unity_ObjectToWorld矩阵已经提供，所以将其添加到UnityInput.hlsl中。 float4x4 unity_ObjectToWorld; float4x4 unity_MatrixVP; 向 Common.hlsl 添加一个 TransformWorldToHClip ，其工作原理与 TransformObjectToWorld 相同，不同之处在于其输入位于世界空间（world space），使用另一个矩阵，并生成一个 float4 。 float3 TransformObjectToWorld (float3 positionOS) { \treturn mul(unity_ObjectToWorld, float4(positionOS, 1.0)).xyz; } float4 TransformWorldToHClip (float3 positionWS) { \treturn mul(unity_MatrixVP, float4(positionWS, 1.0)); } 让 UnlitPassVertex 使用该函数返回正确空间中的位置。 float4 UnlitPassVertex (float3 positionOS : POSITION) : SV_POSITION { \tfloat3 positionWS = TransformObjectToWorld(positionOS.xyz); \treturn TransformWorldToHClip(positionWS); } 修正黑色球体 我们刚刚定义的这两个函数非常常用，因此它们也被包含在 Core RP Pipeline package 中。核心库定义了更多有用且必不可少的内容，所以让我们安装该 package，删除我们自己的定义，并改为包含相关文件，在本例中是 Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl。 //float3 TransformObjectToWorld (float3 positionOS) { //\treturn mul(unity_ObjectToWorld, float4(positionOS, 1.0)).xyz; //} //float4 TransformWorldToHClip (float3 positionWS) { //\treturn mul(unity_MatrixVP, float4(positionWS, 1.0)); //} #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl\" 这会导致编译失败，因为 SpaceTransforms.hlsl 中的代码并不假定 unity_ObjectToWorld 存在。相反，它期望相关的矩阵通过宏定义为 UNITY_MATRIX_M，所以让我们在包含文件之前，在单独的一行编写 #define UNITY_MATRIX_M unity_ObjectToWorld 来实现这一点。之后，所有出现的 UNITY_MATRIX_M 都会被替换为 unity_ObjectToWorld。这样做是有原因的，我们稍后会发现。 #define UNITY_MATRIX_M unity_ObjectToWorld #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl\" 对于逆矩阵 unity_WorldToObject 也是如此，它应该通过 UNITY_MATRIX_I_M 定义；unity_MatrixV 矩阵通过 UNITY_MATRIX_V 定义；而 unity_MatrixVP 通过 UNITY_MATRIX_VP 定义。最后，还有通过 UNITY_MATRIX_P 定义的投影矩阵，它以 glstate_matrix_projection 的形式提供。我们不需要这些额外的矩阵，但如果不包含它们，代码将无法编译。 #define UNITY_MATRIX_M unity_ObjectToWorld #define UNITY_MATRIX_I_M unity_WorldToObject #define UNITY_MATRIX_V unity_MatrixV #define UNITY_MATRIX_VP unity_MatrixVP #define UNITY_MATRIX_P glstate_matrix_projection Unity 2022 需要额外的三个矩阵。 #define UNITY_MATRIX_M unity_ObjectToWorld #define UNITY_MATRIX_I_M unity_WorldToObject #define UNITY_MATRIX_V unity_MatrixV #define UNITY_MATRIX_VP unity_MatrixVP #define UNITY_MATRIX_P glstate_matrix_projection #define UNITY_MATRIX_I_V unity_MatrixInvV #define UNITY_PREV_MATRIX_M unity_prev_MatrixM #define UNITY_PREV_MATRIX_I_M unity_prev_MatrixIM 也将额外的矩阵添加到 UnityInput 。 float4x4 unity_ObjectToWorld; float4x4 unity_WorldToObject; float4x4 unity_MatrixVP; float4x4 unity_MatrixV; float4x4 unity_MatrixInvV; float4x4 unity_prev_MatrixM; float4x4 unity_prev_MatrixIM; float4x4 glstate_matrix_projection; 最后缺失的是一个非矩阵的项。它是 unity_WorldTransformParams，其中包含了一些我们在这里同样不需要的变换信息。它是一个定义为 real4 的向量，这本身不是一个有效的类型，而是根据目标平台指向 float4 或 half4 的别名。 float4x4 unity_ObjectToWorld; float4x4 unity_WorldToObject; real4 unity_WorldTransformParams; 该别名和许多其他基础宏是根据图形 API 定义的，我们可以通过包含 Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl 来获取所有这些内容。在包含 UnityInput.hlsl 之前，在我们的 Common.hlsl 文件中执行此操作。如果你对这些文件的内容感到好奇，可以在导入的 package 中检查它们。 #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\" #include \"UnityInput.hlsl\" 颜色 可以通过调整 UnlitPassFragment 来更改渲染对象的颜色。例如，我们可以通过返回 $float4(1.0, 1.0, 0.0, 1.0)$ 而不是零来使其变为黄色。 float4 UnlitPassFragment () : SV_TARGET { \treturn float4(1.0, 1.0, 0.0, 1.0); } 黄色球体 为了能够按材质配置颜色，我们必须将其定义为统一值。在 include 指令下方，UnlitPassVertex 函数之前执行此操作。我们需要一个 float4，并将其命名为 _BaseColor。前导下划线是表示材质属性的标准方式。在 UnlitPassFragment 中返回此值，而不是硬编码的颜色。 #include \"../ShaderLibrary/Common.hlsl\" float4 _BaseColor; float4 UnlitPassVertex (float3 positionOS : POSITION) : SV_POSITION { \tfloat3 positionWS = TransformObjectToWorld(positionOS); \treturn TransformWorldToHClip(positionWS); } float4 UnlitPassFragment () : SV_TARGET { \treturn _BaseColor; } 我们又回到了黑色，因为默认值为零。要将其链接到材质，我们必须将 _BaseColor 添加到 Unlit shader中的 Properties 块。 Properties { _BaseColor } 属性名称后必须跟着一个用于检查器的字符串和一个 Color 类型标识符，就像为方法提供参数一样。 _BaseColor(\"Color\", Color) 最后，我们必须提供一个默认值，在本例中是为其分配一个包含四个数字的列表。我们使用白色。 _BaseColor(\"Color\", Color) = (1.0, 1.0, 1.0, 1.0) 红色无光材质 现在可以使用我们的着色器创建多个材质，每个材质都有不同的颜色。 批处理 每次绘制调用都需要 CPU 和 GPU 之间的通信。如果大量数据必须发送到 GPU，那么它可能会因为等待而浪费时间。当 CPU 忙于发送数据时，它无法执行其他操作。这两个问题都可能降低帧率。目前我们的方法很简单：每个对象都有自己的绘制调用。这是最糟糕的做法，尽管我们最终发送的数据量很少，所以目前还可以。 举个例子，我创建了一个包含76个球体的场景，每个球体使用四种材质之一：红色、绿色、黄色和蓝色。它需要78次绘制调用来渲染，其中76次用于球体，一次用于天空盒，一次用于清除渲染目标。 76个球体，78次绘制调用 如果你打开 Stats 窗口的 Game 面板，你就可以看到渲染帧所需的概览。这里有趣的事实是，它显示了 77 个批次——忽略清除——其中零个通过批处理节省。 游戏窗口统计。 SRP Batcher 批处理是合并绘制调用的过程，减少 CPU 和 GPU 之间通信所花费的时间。最简单的方法是启用 SRP batcher。然而，这只适用于兼容的着色器，而我们的 Unlit 着色器不兼容。你可以在 Inspector 中选择它来验证。有一行 SRP Batcher 指示不兼容，下面给出了一个原因。 不兼容。 SRP 批处理并没有减少绘制调用的数量，而是使其更精简。它在 GPU 上缓存材质属性，这样就不必在每次绘制调用时都发送它们。这减少了必须通信的数据量以及 CPU 在每次绘制调用中必须完成的工作。但这仅在着色器遵循严格的统一数据结构时才有效。 所有材质属性都必须在具体的内存缓冲区中定义，而不是在全局级别。这是通过将_BaseColor声明包装在名为UnityPerMaterial的cbuffer块中来完成的。这类似于结构体声明，但必须以分号结尾。它通过将_BaseColor放入特定的常量内存缓冲区来隔离它，尽管它仍然可以在全局级别访问。 cbuffer UnityPerMaterial { \tfloat _BaseColor; }; 常量缓冲区并非在所有平台（如 OpenGL ES 2.0）上都受支持，因此我们不直接使用cbuffer，而是可以使用我们从 Core RP Library 中包含的CBUFFER_START和CBUFFER_END宏。第一个宏将缓冲区名称作为参数，就像它是一个函数一样。在这种情况下，我们得到的结果与之前完全相同，只是cbuffer代码不会存在于不支持它的平台上。 CBUFFER_START(UnityPerMaterial) \tfloat4 _BaseColor; CBUFFER_END 我们还必须对 unity_ObjectToWorld、unity_WorldToObject 和 unity_WorldTransformParams 执行此操作，只是它们必须分组到 UnityPerDraw 缓冲区中。 CBUFFER_START(UnityPerDraw) \tfloat4x4 unity_ObjectToWorld; \tfloat4x4 unity_WorldToObject; \treal4 unity_WorldTransformParams; CBUFFER_END 在这种情况下，如果使用其中一个值，则需要定义特定的值组。对于转换组，我们还需要包含 $float4 unity_LODFade$，即使我们不使用它。确切的顺序无关紧要，但 Unity 将其直接放在 unity_WorldToObject 之后，所以我们也这样做。 CBUFFER_START(UnityPerDraw) \tfloat4x4 unity_ObjectToWorld; \tfloat4x4 unity_WorldToObject; \tfloat4 unity_LODFade; \treal4 unity_WorldTransformParams; CBUFFER_END 与 SRP batcher 兼容。 在我们的着色器兼容后，下一步是启用 SRP batcher，这通过将 GraphicsSettings.useScriptableRenderPipelineBatching 设置为 true 来完成。我们只需执行一次此操作，因此让我们在创建RP实例时执行此操作，方法是向 CustomRenderPipeline 添加一个构造函数。 public CustomRenderPipeline () { GraphicsSettings.useScriptableRenderPipelineBatching = true; } 负批次已保存。 Stats 面板显示保存了 76 个批次，尽管它显示的是负数。帧调试器现在在 RenderLoopNewBatcher.Draw 下显示一个 SRP Batch 条目，但请记住，它不是一个单独的绘制调用，而是一系列经过优化的绘制调用。 一个 SRP 批次。 多种颜色 尽管我们使用了四种材质，但我们只获得了一个批次。这是因为所有数据都缓存在 GPU 上，每个绘制调用只需包含一个指向正确内存位置的偏移量。唯一的限制是每种材质的内存布局必须相同，这在本例中是成立的，因为我们对所有材质都使用了相同的着色器，每个着色器只包含一个颜色属性。Unity 不会比较材质的精确内存布局，它只会批处理使用完全相同着色器变体的绘制调用。 如果我们想要几种不同的颜色，这种方法很有效，但如果想让每个球体都有自己的颜色，我们就不得不创建更多的材质。如果能按对象设置颜色会更方便。这在默认情况下是不可能的，但我们可以通过创建自定义组件类型来支持它。将其命名为 PerObjectMaterialProperties。由于它是一个示例，我将其放在 Examples 文件夹下的 Custom RP 中。 其理念是，一个游戏对象可以附加一个 PerObjectMaterialProperties 组件，该组件有一个 Base Color 配置选项，用于设置其 _BaseColor 材质属性。它需要知道着色器属性的标识符，我们可以通过 Shader.PropertyToID 检索并存储在一个静态变量中，就像我们在 CameraRenderer 中为着色器通道标识符所做的那样，尽管在这种情况下它是一个整数。 using UnityEngine; [DisallowMultipleComponent] public class PerObjectMaterialProperties : MonoBehaviour { \t \tstatic int baseColorId = Shader.PropertyToID(\"_BaseColor\"); \t \t[SerializeField] \tColor baseColor = Color.white; } PerObjectMaterialProperties 组件。 通过 MaterialPropertyBlock 对象设置每个对象的材质属性。我们只需要一个所有 PerObjectMaterialProperties 实例都可以重用的对象，因此为其声明一个静态字段。 static MaterialPropertyBlock block; 创建一个 MaterialPropertyBlock，然后使用属性标识符和颜色在其上调用 SetColor，然后通过 SetPropertyBlock 将该块应用于游戏对象的 Renderer 组件，该方法会复制其设置。在 OnValidate 中执行此操作，以便结果立即显示在编辑器中。 void OnValidate () { if (block == null) { block = new MaterialPropertyBlock(); } block.SetColor(baseColorId, baseColor); GetComponent&lt;Renderer&gt;().SetPropertyBlock(block); } 我将该组件添加到了 24 个任意球体上，并给它们赋予了不同的颜色。 多种颜色。 不幸的是，SRP batcher 无法处理每个对象的材质属性。因此，这 24 个球体各自回退到一个常规的绘制调用，并且由于排序，可能会将其他球体也分成多个批次。 24个非批处理绘制调用。 此外，OnValidate 不会在构建中被调用。为了让单独的颜色出现在那里，我们还必须在 Awake 中应用它们，我们可以通过简单地在那里调用 OnValidate 来实现。 \tvoid Awake () { \t\tOnValidate(); \t} GPU Instancing 还有另一种合并绘制调用的方法，它适用于每个对象的材质属性。这被称为 GPU 实例化，其工作原理是为多个具有相同网格的对象一次性发出一个绘制调用。CPU 收集所有每个对象的变换和材质属性，并将它们放入数组中，然后发送到 GPU。GPU 随后遍历所有条目，并按照提供的顺序渲染它们。 由于 GPU 实例需要通过数组提供数据，我们当前的着色器尚不支持它。要实现此功能，第一步是在着色器的 Pass 块中，在vertex和fragment前添加 #pragma multi_compile_instancing 指令。 #pragma multi_compile_instancing #pragma vertex UnlitPassVertex #pragma fragment UnlitPassFragment 这将使 Unity 生成我们着色器的两个变体:支持 GPU instance和不支持GPU 实例。材质检查器中也出现了一个切换选项，允许我们为每个材质选择要使用的版本。 已启用 GPU 实例化的材质。 支持 GPU 实例化需要改变方法，为此我们必须从核心着色器库中包含 UnityInstancing.hlsl 文件。这必须在定义 UNITY_MATRIX_M 和其他宏之后、包含 SpaceTransforms.hlsl 之前完成。 #define UNITY_MATRIX_P glstate_matrix_projection #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/UnityInstancing.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl\" UnityInstancing.hlsl 所做的是重新定义这些宏，以访问实例化数据数组。但要使其工作，它需要知道当前正在渲染的对象的索引。该索引通过顶点数据提供，因此我们必须使其可用。 UnityInstancing.hlsl 定义了宏来简化此操作，但它们假设我们的顶点函数有一个结构体参数。 可以声明一个 struct（就像 cbuffer 一样），并将其用作函数的输入参数。我们还可以在结构体内部定义语义。这种方法的优点是比长参数列表更具可读性。因此，将 UnlitPassVertex 的 positionOS 参数封装在一个 Attributes 结构体中，表示顶点输入数据。 struct Attributes { \tfloat3 positionOS : POSITION; }; float4 UnlitPassVertex (Attributes input) : SV_POSITION { \tfloat3 positionWS = TransformObjectToWorld(input.positionOS); \treturn TransformWorldToHClip(positionWS); } 当使用 GPU 实例化时，对象索引也可以作为顶点属性使用。我们只需在Attributes中放入 UNITY_VERTEX_INPUT_INSTANCE_ID 即可在适当的时候添加它。 struct Attributes { \tfloat3 positionOS : POSITION; \tUNITY_VERTEX_INPUT_INSTANCE_ID }; 接下来，在 UnlitPassVertex 的开头添加 ` UNITY_SETUP_INSTANCE_ID(input); `。这会从输入中提取索引并将其存储在一个全局静态变量中，其他实例化宏都依赖于此变量。 float4 UnlitPassVertex (Attributes input) : SV_POSITION { \tUNITY_SETUP_INSTANCE_ID(input); \tfloat3 positionWS = TransformObjectToWorld(input.positionOS); \treturn TransformWorldToHClip(positionWS); } 这足以让 GPU 实例化工作，尽管 SRP batcher 优先，所以我们现在没有得到不同的结果。但是我们还不支持每个实例的材质数据。要添加这个，我们需要在需要时用数组引用替换 _BaseColor。这是通过将 CBUFFER_START 替换为 UNITY_INSTANCING_BUFFER_START，将 CBUFFER_END 替换为 UNITY_INSTANCING_BUFFER_END 来完成的，这现在也需要一个参数。这不必与开头相同，但没有令人信服的理由让它们不同。 //CBUFFER_START(UnityPerMaterial) //\tfloat4 _BaseColor; //CBUFFER_END UNITY_INSTANCING_BUFFER_START(UnityPerMaterial) \tfloat4 _BaseColor; UNITY_INSTANCING_BUFFER_END(UnityPerMaterial) 然后将 _BaseColor 的定义替换为 UNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor) 。 UNITY_INSTANCING_BUFFER_START(UnityPerMaterial) \t//\tfloat4 _BaseColor; \tUNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor) UNITY_INSTANCING_BUFFER_END(UnityPerMaterial) 当使用实例化时，我们现在还必须在 UnlitPassFragment 中提供实例索引。为了简化此操作，我们将使用一个结构体让 UnlitPassVertex 同时输出位置和索引，并使用 UNITY_TRANSFER_INSTANCE_ID(input, output); 在索引存在时复制它。我们将此结构体命名为 Varyings，就像 Unity 所做的那样，因为它包含的数据在同一三角形的不同片段之间可能会有所不同。 struct Varyings { \tfloat4 positionCS : SV_POSITION; \tUNITY_VERTEX_INPUT_INSTANCE_ID }; Varyings UnlitPassVertex (Attributes input) { \tVaryings output; \tUNITY_SETUP_INSTANCE_ID(input); \tUNITY_TRANSFER_INSTANCE_ID(input, output); \tfloat3 positionWS = TransformObjectToWorld(input.positionOS); \toutput.positionCS = TransformWorldToHClip(positionWS); \treturn output; } 将此结构体作为参数添加到 UnlitPassFragment。然后像之前一样使用 UNITY_SETUP_INSTANCE_ID 来使索引可用。现在必须通过 UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseColor) 访问材质属性。 float4 UnlitPassFragment (Varyings input) : SV_TARGET { \tUNITY_SETUP_INSTANCE_ID(input); \treturn UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseColor); } 实例化绘制调用 Unity 现在能够将 24 个球体与每个对象的颜色结合起来，从而减少了绘制调用的数量。我最终得到了四个实例化绘制调用，因为这些球体之间仍然使用了四种材质。GPU 实例化仅适用于共享相同材质的对象。由于它们覆盖了材质颜色，因此它们都可以使用相同的材质，这使得它们可以在一个批次中绘制。 一个实例化材质 批处理大小存在限制 具体取决于目标平台以及每个实例需要提供的数据量。如果超出此限制，则会生成多个批处理。此外，如果使用了多种材质，排序仍然会拆分批处理。 绘制大量实例网格 当数百个对象可以组合在一个绘制调用中时，GPU 实例化就成为一个显著的优势。但是手动编辑场景中的这么多对象是不切实际的。所以让我们随机生成一堆。创建一个 MeshBall 示例组件，它将在唤醒时生成大量对象。让它缓存 _BaseColor 着色器属性，并添加网格和材质的配置选项，这些材质必须支持实例化。 using UnityEngine; public class MeshBall : MonoBehaviour { \tstatic int baseColorId = Shader.PropertyToID(\"_BaseColor\"); \t[SerializeField] \tMesh mesh = default; \t[SerializeField] \tMaterial material = default; } 创建一个带有此组件的游戏对象。我给它指定了默认的球体网格来绘制。 用于球体的网格球体组件。 我们可以生成许多新的游戏对象，但我们不必这样做。相反，我们将填充一个变换矩阵和颜色数组，并告诉 GPU 渲染一个带有这些数据的网格。这就是 GPU 实例化最有用之处。我们可以一次提供多达 1023 个实例，因此让我们添加长度为该值的数组字段，以及一个我们需要传递颜色数据的 MaterialPropertyBlock。在这种情况下，颜色数组的元素类型必须是 Vector4。 \tMatrix4x4[] matrices = new Matrix4x4[1023]; \tVector4[] baseColors = new Vector4[1023]; \tMaterialPropertyBlock block; 创建一个 Awake 方法，用半径为 10 的球体内的随机位置和随机 RGB 颜色数据填充数组。 \tvoid Awake () { \t\tfor (int i = 0; i &lt; matrices.Length; i++) { \t\t\tmatrices[i] = Matrix4x4.TRS( \t\t\t\tRandom.insideUnitSphere * 10f, Quaternion.identity, Vector3.one \t\t\t); \t\t\tbaseColors[i] = \t\t\t\tnew Vector4(Random.value, Random.value, Random.value, 1f); \t\t} \t} 在 Update 中，如果块尚不存在，我们会创建一个新块，并对其调用 SetVectorArray 来配置颜色。之后，调用 Graphics.DrawMeshInstanced，并将网格、子网格索引零、材质、矩阵数组、元素数量和属性块作为参数。我们在此处设置块，以便网格球在热重载后仍然存在。 \tvoid Update () { \t\tif (block == null) { \t\t\tblock = new MaterialPropertyBlock(); \t\t\tblock.SetVectorArray(baseColorId, baseColors); \t\t} \t\tGraphics.DrawMeshInstanced(mesh, 0, material, matrices, 1023, block); \t} 1023 个球体，3 次绘制调用。 Play Game，现在会生成一个密集的球体。渲染所需的绘制调用次数取决于平台，因为每个绘制调用的最大缓冲区大小不同。在我的情况下，渲染需要三次绘制调用。 单个网格的绘制顺序与我们提供数据的顺序相同。除此之外，没有任何排序或剔除，尽管整个批次一旦超出视锥体就会消失。 动态批处理 还有第三种减少绘制调用的方法，称为动态批处理。这是一种旧技术，它将共享相同材质的多个小网格组合成一个更大的网格进行绘制。但是当使用MaterialPropertyBlock时，此方法也无法有效和批。 较大的网格是按需生成的，因此只适用于小型网格。球体太大，但它适用于立方体。要查看其效果，请禁用 GPU 实例化，并在 CameraRenderer.DrawVisibleGeometry 中将 enableDynamicBatching 设置为 true。 var drawingSettings = new DrawingSettings( unlitShaderTagId, sortingSettings ) { enableDynamicBatching = true, enableInstancing = false }; 同时禁用 SRP 批处理器，因为它具有优先合批权。 GraphicsSettings.useScriptableRenderPipelineBatching = false; 改为绘制立方体。 通常情况下，GPU 实例化比动态批处理效果更好。这种方法也有一些注意事项，例如当涉及不同比例时，较大网格的法线向量不保证是normalize化单位长度。此外，由于现在是一个网格而不是多个网格，绘制顺序也会发生变化。 还有静态批处理，它的工作方式类似，但会提前对标记为批处理静态的对象进行处理。除了需要更多内存和存储空间外，它没有其他注意事项。RP 不用关注这一点，所以我们不必担心。 配置批处理 哪种方法最好可能会有所不同，因此我们将其配置化。首先，添加布尔参数来控制是否使用动态批处理和 GUI 实例化来 DrawVisibleGeometry，而不是硬编码。 \tvoid DrawVisibleGeometry (bool useDynamicBatching, bool useGPUInstancing) { \t\tvar sortingSettings = new SortingSettings(camera) { \t\t\tcriteria = SortingCriteria.CommonOpaque \t\t}; \t\tvar drawingSettings = new DrawingSettings( \t\t\tunlitShaderTagId, sortingSettings \t\t) { \t\t\tenableDynamicBatching = useDynamicBatching, \t\t\tenableInstancing = useGPUInstancing \t\t}; \t\t… \t} Render 现在必须提供此配置，而此配置又依赖于 RP 提供。 \tpublic void Render ( \t\tScriptableRenderContext context, Camera camera, \t\tbool useDynamicBatching, bool useGPUInstancing \t) { \t\t… \t\tDrawVisibleGeometry(useDynamicBatching, useGPUInstancing); \t\t… \t} CustomRenderPipeline 将通过在其构造函数方法中设置的字段来跟踪选项，并在 Render 中传递它们。同时，为构造函数添加一个布尔参数用于 SRP batcher，而不是始终启用它。 \tbool useDynamicBatching, useGPUInstancing; \tpublic CustomRenderPipeline ( \t\tbool useDynamicBatching, bool useGPUInstancing, bool useSRPBatcher \t) { \t\tthis.useDynamicBatching = useDynamicBatching; \t\tthis.useGPUInstancing = useGPUInstancing; \t\tGraphicsSettings.useScriptableRenderPipelineBatching = useSRPBatcher; \t} \t… \t \tprotected override void Render ( \t\tScriptableRenderContext context, List&lt;Camera&gt; cameras \t) { \t\tfor (int i = 0; i &amp;;t cameras.Count; i++) { \t\t\trenderer.Render( \t\t\t\tcontext, cameras[i], useDynamicBatching, useGPUInstancing \t\t\t); \t\t} \t} 最后，将所有三个选项作为配置字段添加到 CustomRenderPipelineAsset，并在 CreatePipeline 中将它们传递给构造函数调用。 \t[SerializeField] \tbool useDynamicBatching = true, useGPUInstancing = true, useSRPBatcher = true; \tprotected override RenderPipeline CreatePipeline () { \t\treturn new CustomRenderPipeline( \t\t\tuseDynamicBatching, useGPUInstancing, useSRPBatcher \t\t); \t} RP 配置。 现在可以更改我们的 RP 使用的方法。切换选项会立即生效，因为 Unity 编辑器在检测到资产更改时会创建一个新的 RP 实例。 透明度 我们的着色器可用于创建无光不透明材质。可以更改颜色的 alpha 分量，这通常表示透明度，但目前没有效果。我们还可以将渲染队列设置为 Transparent ，但这只改变对象何时以及以何种顺序绘制，而不是如何绘制。 降低了 alpha 并使用了透明渲染队列。 我们不需要编写单独的着色器来支持透明材质。稍作修改，我们的 Unlit 着色器就可以支持不透明和透明渲染。 混合模式 不透明和透明渲染的主要区别在于我们是否替换之前绘制的内容，或者将之前的结果组合起来以产生透明效果。我们可以通过设置源和目标混合模式来控制这一点。在这里，源指的是现在绘制的内容，而目标指的是之前绘制的内容以及结果将最终出现在哪里。为此添加两个着色器属性：_SrcBlend 和 _DstBlend。它们是混合模式的枚举，但我们能使用的最佳类型是 Float，默认情况下，源设置为 1，目标设置为 0。 \tProperties { \t\t_BaseColor(\"Color\", Color) = (1.0, 1.0, 1.0, 1.0) \t\t_SrcBlend (\"Src Blend\", Float) = 1 \t\t_DstBlend (\"Dst Blend\", Float) = 0 \t} 为了便于编辑，我们可以将 Enum 属性添加到属性中，并以完全限定的 UnityEngine.Rendering.BlendMode 枚举类型作为参数。 \t\t[Enum(UnityEngine.Rendering.BlendMode)] _SrcBlend (\"Src Blend\", Float) = 1 \t\t[Enum(UnityEngine.Rendering.BlendMode)] _DstBlend (\"Dst Blend\", Float) = 0 不透明混合模式。 默认值代表我们已经使用的不透明混合配置。源设置为1，表示它被完全添加，而目标设置为零，表示它被忽略。 标准透明度的源混合模式是 SrcAlpha，这意味着渲染颜色的 RGB 分量会乘以其 alpha 分量。因此，alpha 越低，它就越弱。然后将目标混合模式设置为相反的模式：OneMinusSrcAlpha，以达到总权重为 1。 透明混合模式。 混合模式可以在 Pass 块中使用 Blend 语句后跟两个模式来定义。我们想使用着色器属性，可以通过将它们放在方括号中来访问。这是可编程着色器时代之前的旧语法。 Pass { Blend [_SrcBlend] [_DstBlend] HLSLPROGRAM … ENDHLSL } 半透明黄色球体。 不写入深度 透明渲染通常不写入深度缓冲区，因为它从中得不到好处，甚至可能产生不希望的结果。我们可以通过 ZWrite 语句控制是否写入深度。同样，我们可以使用着色器属性，这次使用 _ZWrite。 Blend [_SrcBlend] [_DstBlend] ZWrite [_ZWrite] 使用自定义的 Enum(Off, 0, On, 1) 属性定义着色器属性，以创建一个默认开启的、值为 0 和 1 的开关。 [Enum(UnityEngine.Rendering.BlendMode)] _SrcBlend (\"Src Blend\", Float) = 1 [Enum(UnityEngine.Rendering.BlendMode)] _DstBlend (\"Dst Blend\", Float) = 0 [Enum(Off, 0, On, 1)] _ZWrite (\"Z Write\", Float) = 1 深度写入已关闭。 纹理 之前我们使用 alpha 贴图创建了不均匀的半透明材质。我们通过向着色器添加 _BaseMap 纹理属性来支持这一点。在这种情况下，类型是 2D，我们将使用 Unity 的标准白色纹理作为默认值，用 ` white ` 字符串表示。此外，我们必须以一个空的 code 块结束纹理属性。它很久以前用于控制纹理设置，但今天仍应包含在内，以防止在某些情况下出现奇怪的错误。 _BaseMap(\"Texture\", 2D) = \"white\" {} _BaseColor(\"Color\", Color) = (1.0, 1.0, 1.0, 1.0) 带纹理的材质。 纹理必须上传到 GPU 内存，Unity 会为我们完成此操作。着色器需要一个相关纹理的句柄，我们可以像定义 uniform 值一样定义它，只不过我们使用 TEXTURE2D 宏并将名称作为参数。我们还需要为纹理定义一个采样器状态，它控制纹理应如何采样，同时考虑其环绕和过滤模式。这是通过 SAMPLER 宏完成的，类似于 TEXTURE2D，但名称前缀为 sampler。这与 Unity 自动提供的采样器状态的名称匹配。 纹理和采样器状态是着色器资源。它们不能按实例提供，必须在全局范围声明。在 UnlitPass.hlsl 中的着色器属性之前完成此操作。 TEXTURE2D(_BaseMap); SAMPLER(sampler_BaseMap); UNITY_INSTANCING_BUFFER_START(UnityPerMaterial) \tUNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor) UNITY_INSTANCING_BUFFER_END(UnityPerMaterial) 除此之外，Unity 还通过一个 float4 提供纹理的平铺和偏移，该 float4 的名称与纹理属性相同，但附加了 _ST，代表缩放和变换或类似含义。此属性应是 UnityPerMaterial 缓冲区的一部分，因此可以按实例设置。 UNITY_INSTANCING_BUFFER_START(UnityPerMaterial) \tUNITY_DEFINE_INSTANCED_PROP(float4, _BaseMap_ST) \tUNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor) UNITY_INSTANCING_BUFFER_END(UnityPerMaterial) 为了能采样纹理，我们需要纹理坐标，它是顶点属性的一部分。具体来说，我们需要第一对坐标，因为可能有更多对。这通过向 Attributes 添加一个具有 TEXCOORD0 含义的 float2 字段来完成。由于它是用于我们的基础贴图，并且纹理空间维度普遍命名为 U 和 V，我们将其命名为 baseUV。 struct Attributes { \tfloat3 positionOS : POSITION; \tfloat2 baseUV : TEXCOORD0; \tUNITY_VERTEX_INPUT_INSTANCE_ID }; 我们需要将坐标传递给片段函数，因为纹理是在那里采样的。因此，也要将 $float2 baseUV$ 添加到 Varyings 中。这次我们不需要添加特殊含义，它只是我们传递的数据，不需要 GPU 的特殊关注。但是，我们仍然必须赋予它一些含义。我们可以应用任何未使用的标识符，我们简单地使用 VAR_BASE_UV。 struct Varyings { \tfloat4 positionCS : SV_POSITION; \tfloat2 baseUV : VAR_BASE_UV; \tUNITY_VERTEX_INPUT_INSTANCE_ID }; 当我们在 UnlitPassVertex 中复制坐标时，我们也可以应用存储在 _BaseMap_ST 中的缩放和偏移。这样，我们就可以按顶点而不是按片段进行操作。缩放存储在 XY 中，偏移存储在 ZW 中，我们可以通过 swizzle 属性访问它们。 Varyings UnlitPassVertex (Attributes input) { \t… \tfloat4 baseST = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseMap_ST); \toutput.baseUV = input.baseUV * baseST.xy + baseST.zw; \treturn output; } UV 坐标现在可用于 UnlitPassFragment，并在三角形上进行插值。在这里通过使用 SAMPLE_TEXTURE2D 宏并以纹理、采样器状态和坐标作为参数来采样纹理。最终颜色是纹理和统一颜色通过乘法组合而成的。将两个相同大小的向量相乘会导致所有匹配分量相乘，因此在这种情况下是红色乘以红色，绿色乘以绿色，依此类推。 float4 UnlitPassFragment (Varyings input) : SV_TARGET { \tUNITY_SETUP_INSTANCE_ID(input); \tfloat4 baseMap = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, input.baseUV); \tfloat4 baseColor = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseColor); \treturn baseMap * baseColor; } 带纹理的黄色球体。 因为我们纹理的 RGB 数据是统一的白色，所以颜色不受影响。但 alpha 通道是变化的，因此透明度不再统一。 Alpha 裁剪 另一种看穿表面的方法是在其中打孔。着色器也可以做到这一点，通过丢弃一些它们通常会渲染的片段。这会产生硬边，而不是我们目前看到的平滑过渡。这种技术被称为 alpha 裁剪。通常的做法是定义一个截止阈值。alpha 值低于此阈值的片段将被丢弃，而所有其他片段则被保留。 添加一个 _Cutoff 属性，默认设置为 0.5。由于 alpha 始终介于零和 1 之间，我们可以使用 Range(0.0, 1.0) 作为其类型。 _BaseColor(\"Color\", Color) = (1.0, 1.0, 1.0, 1.0) _Cutoff (\"Alpha Cutoff\", Range(0.0, 1.0)) = 0.5 也将其添加到 UnlitPass.hlsl 中的材质属性。 \tUNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor) \tUNITY_DEFINE_INSTANCED_PROP(float, _Cutoff) 我们可以在 UnlitPassFragment 中调用 clip 函数来丢弃片段。如果传入的值为零或更小，它将中止并丢弃该片段。因此，将最终的 alpha 值（可通过 a 或 w 属性访问）减去截止阈值后传递给它。 \tfloat4 baseMap = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, input.baseUV); \tfloat4 baseColor = UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseColor); \tfloat4 base = baseMap * baseColor; \tclip(base.a - UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _Cutoff)); \treturn base; Alpha 截止值设为 0.2。 一种材质通常使用透明度混合或 Alpha 裁剪，而不是同时使用两者。典型的裁剪材质除了被丢弃的片段外是完全不透明的，并且会写入深度缓冲区。它使用AlphaTest渲染队列，这意味着它会在所有完全不透明对象之后渲染。这样做是因为丢弃片段会使一些 GPU 优化变得不可能，因为三角形不能再被假定完全覆盖它们后面的内容。通过首先绘制完全不透明的对象，它们最终可能会覆盖 Alpha 裁剪对象的一部分，这样就不需要处理其隐藏的片段。 Alpha 裁剪材质。 但要使此优化生效，我们必须确保仅在需要时才使用 clip。我们将通过添加一个功能切换着色器属性来实现这一点。它是一个默认为零的 Float 属性，带有一个 Toggle 属性，用于控制着色器关键字，我们将使用 _CLIPPING。属性本身的名称无关紧要，因此只需使用 _Clipping。 \t\t_Cutoff (\"Alpha Cutoff\", Range(0.0, 1.0)) = 0.5 \t\t[Toggle(_CLIPPING)] _Clipping (\"Alpha Clipping\", Float) = 0 Alpha 裁剪已关闭，按理说。 着色器功能 启用此开关会将 _CLIPPING 关键字添加到材质的活动关键字列表中，禁用则会将其移除。但这本身并不会产生任何效果。我们必须告诉 Unity 根据关键字是否已定义来编译我们着色器的不同版本。我们通过在其 Pass 中的指令中添加 #pragma shader_feature _CLIPPING 来实现这一点。 #pragma shader_feature _CLIPPING #pragma multi_compile_instancing 现在，Unity 将会编译我们的着色器代码，无论是否定义了 _CLIPPING。它将生成一个或两个变体，具体取决于我们如何配置材质。因此，我们可以根据定义使代码具有条件性，就像包含守卫一样，但在这种情况下，我们只希望在定义了 _CLIPPING 时才包含裁剪行。我们可以使用 #ifdef _CLIPPING 来实现，但我更喜欢 #if defined(_CLIPPING)。 \t#if defined(_CLIPPING) \t\tclip(base.a - UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _Cutoff)); \t#endif Cutoff Object1 由于Cutoff值是 UnityPerMaterial 缓冲区的一部分，因此可以按实例配置。所以让我们将该功能添加到 PerObjectMaterialProperties 中。它的工作方式与颜色相同，只是我们需要在属性块上调用 SetFloat 而不是 SetColor。 \tstatic int baseColorId = Shader.PropertyToID(\"_BaseColor\"); \tstatic int cutoffId = Shader.PropertyToID(\"_Cutoff\"); \tstatic MaterialPropertyBlock block; \t[SerializeField] \tColor baseColor = Color.white; \t[SerializeField, Range(0f, 1f)] \tfloat cutoff = 0.5f; \t… \tvoid OnValidate () { \t\t… \t\tblock.SetColor(baseColorId, baseColor); \t\tblock.SetFloat(cutoffId, cutoff); \t\tGetComponent&lt;Renderer&gt;().SetPropertyBlock(block); \t} 每个实例化对象的 Alpha 截止值。 Alpha 裁剪球体球 MeshBall 也是如此。现在我们可以使用剪裁材质，但所有实例最终都会有完全相同的孔洞。 近距离观察 Alpha 剪裁的网格球。 让我们通过给每个实例一个随机旋转，加上 0.5-1.5 范围内的随机统一缩放来增加一些多样性。但是，我们不会为每个实例设置截止值，而是将其颜色的 alpha 通道在 0.5-1 范围内变化。这给了我们不太精确的控制，但这只是一个随机示例。 matrices[i] = Matrix4x4.TRS( Random.insideUnitSphere * 10f, Quaternion.Euler( Random.value * 360f, Random.value * 360f, Random.value * 360f ), Vector3.one * Random.Range(0.5f, 1.5f) ); baseColors[i] = new Vector4( Random.value, Random.value, Random.value, Random.Range(0.5f, 1f) ); 更多样化的网格球。 请注意，Unity 仍然会向 GPU 发送一个cutoff数组，每个实例一个，即使它们都相同。该值是材质的副本，因此通过改变它，可以一次性改变所有球体的孔洞，即使它们仍然不同。 下一个篇是方向光。" }, { "title": "自定义渲染管线:掌控渲染流程 (翻译一)", "url": "/posts/rustom-render-pipeline/", "categories": "Unity3D, ScriptRenderPipeline", "tags": "SRP, Shader", "date": "2019-09-26 00:00:00 +0800", "content": "这是关于创建自定义脚本化渲染管线（Scriptable Render Pipeline, SRP）的系列教程的第一部分。它涵盖了我们将要在未来扩展的准系统渲染管线的初始创建。 创建渲染管线资源和实例 渲染摄像机视图 执行剔除、过滤和排序 分离不透明、透明和无效阶段 处理多个摄像机。 1 Render Pipeline 为了渲染任何内容，Unity 必须确定需要绘制哪些形状，以及在何时、何地、使用何种设置进行绘制。根据涉及效果的多寡，这一过程可能会变得非常复杂。光照、阴影、透明度、图像效果、体积效果等都必须按正确的顺序处理，才能得到最终的图像。这就是 render pipeline 的职责。 过去，Unity 仅支持几种内置的渲染方式。Unity 2018 引入了 scriptable render pipelines（简称 RPs），使我们能够随心所欲地实现自定义渲染，同时仍能依靠 Unity 完成剔除（culling）等基础步骤。Unity 2018 还添加了两个使用这种新方法制作的实验性 RP：Lightweight RP 和 High Definition RP。在 Unity 2019 中，Lightweight RP 不再是实验性的，并在 Unity 2019.3 中更名为 Universal RP。 Universal RP 旨在取代当前的 legacy RP 成为默认选项。其理念是将其打造为一个适用于大多数场景且易于自定义的 RP。本系列教程不会去自定义该 RP，而是将从头开始创建一个完整的 RP。 本教程将通过一个使用 forward rendering 进行绘制 unlit 形状的最小化的最基础的RPs。一旦该流程正常运行，我们就可以在后续教程中扩展我们的 pipeline，添加光照、阴影、不同的渲染方法以及更高级的功能。 1.1 项目设置 在 Unity 2022.3.5f1（以前是 2019.2.6）或更高版本中创建一个新的 3D 项目。我们将创建自己的管线，因此不要选择任何 RP 项目模板。项目打开后，你可以前往包管理器 (Package Manager) 并删除所有不需要的包。在本教程中，我们只会使用 Unity UI 包来试验绘制 UI，因此你可以保留该包。 我们将专门在线性颜色空间 linear color space 中工作，但 Unity 2019.2 仍将 Gamma 空间作为默认设置。通过 Edit / Project Settings 进入 Player 设置，然后在 Other Settings 部分下将 Color Space 切换为 Linear。 色彩空间设置 在默认场景中填充一些物体，混合使用标准、不透明无光照（unlit opaque）以及透明材质。由于 Unlit/Transparent shader 仅支持纹理，这里提供了一张用于该材质的 UV 球体贴图。 UV球体 Alpha 贴图，黑色背景 我在我的测试场景中放了几个不透明的立方体。红色的使用标准着色器材质，而绿色和黄色的使用 Unlit/Color 着色器材质。蓝色球体使用标准着色器，并将 Rendering Mode 设置为 Transparent，而白色球体使用 Unlit/Transparent 着色器。 测试场景 1.2 管线资源 (Pipeline Asset) 目前，Unity 使用默认渲染管线。要将其替换为自定义渲染管线，我们首先必须为其创建一个资产类型。我们将使用与 Unity 用于通用 RP 大致相同的文件夹结构。创建一个 Custom RP 资产文件夹，其中包含一个 Runtime 子文件夹。在其中放入一个新的 C# 脚本，用于 CustomRenderPipelineAsset 类型。 文件夹结构 资产类型必须扩展 UnityEngine.Rendering 命名空间中的 RenderPipelineAsset。 using UnityEngine; using UnityEngine.Rendering; [CreateAssetMenu(menuName = \"Rendering/Custom Render Pipeline\")] public class CustomRenderPipelineAsset : RenderPipelineAsset { protected override RenderPipeline CreatePipeline () { return null; } } RP 资产的主要目的是让 Unity 能够获得负责渲染的管线对象实例。资产本身只是一个句柄和存储设置的地方。我们还没有任何设置，所以现在能做的就是先通过覆盖抽象的 CreatePipeline 方法来让 Unity 能够获取我们的管线对象实例。该方法应该返回一个 RenderPipeline 实例。但我们还没有定义自定义 RP 类型，所以先返回 null。 CreatePipeline 方法使用 protected 访问修饰符定义，这意味着只有定义该方法的类（即 RenderPipelineAsset）以及扩展它的类才能访问它。 为了让创建该资产成为可能，我们将 CreateAssetMenu 属性添加到 CustomRenderPipelineAsset。 这将会在 Asset / Create 菜单中放置一个条目。让我们保持整洁，将其放在 Rendering 子菜单中。我们通过将属性的 menuName 属性设置为 Rendering/Custom Render Pipeline 来实现这一点。 [CreateAssetMenu(menuName = \"Rendering/Custom Render Pipeline\")] public class CustomRenderPipelineAsset : RenderPipelineAsset { … } 使用新的菜单项将资产添加到项目中，然后转到 Graphics 项目设置并在 Scriptable Render Pipeline Settings 下选择它。 选择了自定义RP 替换默认 RP 改变了一些事情。首先，图形设置中消失了很多选项，这在信息面板中有所提及。其次，我们禁用了默认 RP 但没有提供有效的替代品，因此不再渲染任何内容。游戏窗口、场景窗口和材质预览不再起作用。如果你打开帧调试器 (Frame Debugger)——通过 Window / Analysis / Frame Debugger——并启用它，你会看到游戏窗口中确实没有绘制任何内容。 1.3 渲染管线实例 (Render Pipeline Instance) 创建一个 CustomRenderPipeline 类，并将其脚本文件放在与 CustomRenderPipelineAsset 相同的文件夹中。这将是我们的资产返回的 RP 实例所使用的类型，因此它必须扩展 RenderPipeline。 using UnityEngine; using UnityEngine.Rendering; public class CustomRenderPipeline : RenderPipeline { //unity 2022之前的老版本API，每帧分配相机数组 protected override void Render ( ScriptableRenderContext context, Camera[] cameras ) {} } RenderPipeline 定义了一个受保护的抽象 Render 方法，我们必须覆盖它来创建一个具体的管线。它有两个参数：一个 ScriptableRenderContext 和一个List&lt;Camera&gt;,先让该方法保持为空。 protected override void Render ( ScriptableRenderContext context, System.Collections.Generic.List&lt;Camera&gt; cameras ) {} 此方法曾是自定义 SRP 定义的入口点，但由于相机数组参数需要每帧分配内存，因此引入了一个使用列表参数的替代方案。我们可以在 Unity 2022 中使用该方案，但仍必须保留旧版本，因为它被声明为抽象方法，尽管它不会被使用。请注意，后续的 profiler 截图仍会包含旧版相机数组的内存分配。 让 CustomRenderPipelineAsset.CreatePipeline 返回 CustomRenderPipeline 的新实例。这将让我们获得一个有效且功能完备的管线，尽管它还不会渲染任何内容。 protected override RenderPipeline CreatePipeline () { return new CustomRenderPipeline(); } 2 渲染 (Rendering) 每一帧 Unity 都会在 RP 实例上调用 Render 。它会传递一个 context 结构体，该结构体提供了与原生引擎的连接，我们可以使用它进行渲染。它还会传递一个 camera 数组，因为场景中可能存在多个处于激活状态的 camera。RP 的职责是按照提供的顺序渲染所有这些 camera。 2.1 摄像机渲染器 (Camera Renderer) 每个 camera 都是独立渲染的。因此，我们不让 CustomRenderPipeline 渲染所有 camera，而是将该职责转发给一个专门负责渲染单个 camera 的新类。将其命名为 CameraRenderer ，并为其提供一个带有 context 和 camera 参数的公开 Render 方法。为了方便起见，我们将这些参数存储在字段中。 using UnityEngine; using UnityEngine.Rendering; public class CameraRenderer { ScriptableRenderContext context; Camera camera; public void Render (ScriptableRenderContext context, Camera camera) { this.context = context; this.camera = camera; } } 让 CustomRenderPipeline 在创建时创建一个渲染器的实例，然后在循环中使用它来渲染所有摄像机。 public class CustomRenderPipeline : RenderPipeline { CameraRenderer renderer = new CameraRenderer(); protected override void Render ( ScriptableRenderContext context, Camera[] cameras ) {} protected override void Render ( ScriptableRenderContext context, System.Collections.Generic.List&lt;Camera&gt; cameras ) { for (int i = 0; i &lt; cameras.Count; i++) { renderer.Render(context, cameras[i]); } } } 我们的 camera renderer 大致相当于 Universal RP 的 scriptable renderer。这种方法可以方便未来为每个 camera 支持不同的渲染方式，例如一个用于第一人称视角，另一个用于 3D 地图叠加，或者前向渲染与延迟渲染的对比。但目前我们将以相同的方式渲染所有 camera。 2.2 绘制天空盒 (Drawing the Skybox) CameraRenderer.Render 的任务是绘制其 camera 可见的所有几何体。为了清晰起见，将该特定任务隔离在一个单独的 DrawVisibleGeometry 方法中。我们首先让它绘制默认的 skybox，这可以通过在 context 上调用 DrawSkybox 并将 camera 作为参数来实现。 public void Render (ScriptableRenderContext context, Camera camera) { this.context = context; this.camera = camera; //这里默认场景只有一个相机，所以外部for循环调用Render后续再优化这个多相机流程 DrawVisibleGeometry(); } void DrawVisibleGeometry () { context.DrawSkybox(camera); } 这还不能让 skybox 出现。这是因为我们向 context 发出的命令是缓冲处理的。我们必须通过调用 context 的 Submit 来提交排队的工作以执行。让我们在一个单独的 Submit 方法中完成此操作，该方法在 DrawVisibleGeometry 之后调用。 public void Render (ScriptableRenderContext context, Camera camera) { this.context = context; this.camera = camera; DrawVisibleGeometry(); Submit(); } void Submit () { context.Submit(); } skybox 终于出现在 game 窗口和 scene 窗口中了。启用 frame debugger 后，你也可以在其中看到它的条目。它被列为 Camera.RenderSkybox ，其下方有一个 Draw Mesh 项，代表实际的 draw call。这对应于 game 窗口的渲染。frame debugger 不会报告其他窗口的绘制情况。 天空盒绘制 请注意，摄像机的方向目前不会影响天空盒的渲染方式。我们将摄像机传递给 DrawSkybox，但这仅用于根据摄像机的清除标志 (clear flags) 确定是否应该绘制天空盒。 为了正确渲染天空盒以及整个场景，我们必须设置视图投影矩阵（view-projection matrix）。该变换矩阵结合了摄像机的位置和朝向（即视图矩阵 view matrix）与摄像机的透视或正交投影（即投影矩阵 projection matrix）。在 shader 中，它被称为 unity_MatrixVP ，是绘制几何体时使用的 shader 属性之一。当选中某个 draw call 时，你可以在 frame debugger 的 ShaderProperties 部分检查该矩阵。 目前，unity_MatrixVP 矩阵始终保持不变。我们必须通过 SetupCameraProperties 方法将摄像机的属性应用于 context。该方法会设置矩阵以及其他一些属性。请在调用 DrawVisibleGeometry 之前，在一个独立的 Setup 方法中执行此操作。 public void Render (ScriptableRenderContext context, Camera camera) { this.context = context; this.camera = camera; Setup(); DrawVisibleGeometry(); Submit(); } // 渲染之前把相机的属性传递给context void Setup () { context.SetupCameraProperties(camera); } 天空盒，正确对齐 2.3 命令缓冲区 (Command Buffers) Context 会延迟实际的渲染，直到我们提交它。在此之前，我们会对其进行配置并添加命令以便稍后执行。某些任务（例如绘制 skybox）可以通过专用方法发布，但其他命令必须通过单独的 command buffer 间接发布。我们需要这样一个 buffer 来绘制场景中的其他几何体。 为了获取 buffer，我们必须创建一个新的 CommandBuffer 对象实例。我们只需要一个 buffer，因此默认在 CameraRenderer 中创建一个，并将其引用存储在字段中。同时给 buffer 起一个名字，以便我们在 frame debugger 中识别它。 命名Custom Render Camera 即可。 const string bufferName = \"Render Camera\"; CommandBuffer buffer = new CommandBuffer { name = bufferName }; 我们可以使用 command buffers 来注入 (profiler samples)，这些样本将同时显示在profiler 和 frame debugger 中。这通过在适当的时间点调用 BeginSample 和 EndSample 来完成，在我们的例子中是在 Setup 和 Submit 的开头。这两个方法必须提供相同的 sample 名称，我们将使用 buffer 的名称。 void Setup () { buffer.BeginSample(bufferName); context.SetupCameraProperties(camera); } void Submit () { buffer.EndSample(bufferName); context.Submit(); } 若要执行 buffer，请调用 context 上的 ExecuteCommandBuffer 并将 buffer 作为参数传入。这会从 buffer 中拷贝命令，但不会将其清除，如果我们要重复使用它，之后必须显式地进行清除。执行和清除操作总是连在一起进行的，因此添加一个同时完成这两项操作的方法会很方便。 void Setup () { buffer.BeginSample(bufferName); ExecuteBuffer(); context.SetupCameraProperties(camera); } void Submit () { buffer.EndSample(bufferName); ExecuteBuffer(); context.Submit(); } void ExecuteBuffer () { context.ExecuteCommandBuffer(buffer); buffer.Clear(); } Camera.RenderSkybox 示例现在嵌套在 Render Camera 内部。 摄像机渲染截帧 2.4 清除渲染目标 (Clearing the Render Target) 我们绘制的所有内容最终都会渲染到摄像机的渲染目标中，默认情况下是帧缓冲区，但也可能是渲染纹理。之前绘制到该目标的内容仍然存在，这可能会干扰我们现在渲染的图像。为了保证正确的渲染，我们必须清除渲染目标以移除其旧内容。这是通过在 command buffer 上调用 ClearRenderTarget 来完成的，该调用应位于 Setup 方法中。 CommandBuffer.ClearRenderTarget 至少需要三个参数。前两个指示是否应该清除深度和颜色数据，这两者都设置为 true。第三个参数是用于清除的颜色，我们将使用 Color.clear。 清除buff void Setup () { buffer.BeginSample(bufferName); buffer.ClearRenderTarget(true, true, Color.clear); ExecuteBuffer(); context.SetupCameraProperties(camera); } Frame Debugger 现在会为清除操作显示一个 Draw GL 条目，该条目嵌套在额外一层的 Render Camera 中。这是因为 ClearRenderTarget 会使用 Command Buffer 的名称将清除操作包装在一个 Sample 中。我们可以通过在开始自定义 Sample 之前进行清除来消除冗余的嵌套。这样会产生两个相邻的 Render Camera Sample 作用域，它们会被合并。 void Setup () { //换一个位置 buffer.ClearRenderTarget(true, true, Color.clear); buffer.BeginSample(bufferName); ExecuteBuffer(); context.SetupCameraProperties(camera); } Draw GL 条目表示使用 Hidden/InternalClear shader 绘制一个全屏四边形并写入 render target，这并不是清除它的最有效方式。之所以采用这种方法，是因为我们在设置 camera 属性之前进行了清除操作。如果我们交换这两个步骤的顺序，就可以使用更快速的清除方式。 void Setup () { //把参数传递提前 context.SetupCameraProperties(camera); buffer.ClearRenderTarget(true, true, Color.clear); buffer.BeginSample(bufferName); ExecuteBuffer(); } 现在我们看到 Clear (color+Z+stencil)，这表明颜色和深度缓冲区都被清除了。Z 代表深度缓冲区，模板数据是同一缓冲区的一部分。 正确清除 Draw GL vs Clear (color+Z+stencil) 方式 显示 原理 性能 软件清除 Draw GL / Hidden/InternalClear shader 绘制全屏四边形，每个像素走完整渲染管线（VS+FS+光栅化） 慢 硬件清除 Clear (color+Z+stencil) GPU 快速清除指令，直接操作帧缓冲区内存 快 原理对比: # 软件清除（先 ClearRenderTarget） ClearRenderTarget → GPU 未配置 → 降级为绘制全屏四边形 ↓ 每个像素执行 shader # 硬件清除（先 SetupCameraProperties） SetupCameraProperties → 配置渲染目标（分辨率/格式/视口） ↓ ClearRenderTarget → 使用硬件 Fast-Clear 指令 ↓ CPU 发送指令，GPU 直接填充内存 关键: SetupCameraProperties(camera) 必须先调用，为 GPU 提供渲染目标的完整配置信息，才能触发硬件快速清除指令。 2.5 剔除 (Culling) 我们目前可以看到天空盒，但看不到场景中放置的任何物体。与其绘制每一个物体，我们应该只渲染那些对摄像机可见的物体。为此，我们首先获取场景中所有带有 renderer 组件的物体，然后剔除那些位于摄像机视锥体 (view frustum) 之外的物体。 确定哪些物体可以被剔除需要我们跟踪多个摄像机设置和矩阵，为此我们可以使用 ScriptableCullingParameters 结构体。我们无需手动填充它，而是可以在摄像机上调用 TryGetCullingParameters 。它会返回参数是否成功获取，因为对于某些退化的摄像机设置可能会失败。为了获取参数数据，我们必须将其作为输出参数提供，即在它前面加上 out 。请在一个独立的 Cull 方法中执行此操作，该方法返回成功或失败。 bool Cull () { if (camera.TryGetCullingParameters(out ScriptableCullingParameters p)) { return true; } return false; } 在 Render 中的 Setup 之前调用 Cull，如果失败则中止。 public void Render (ScriptableRenderContext context, Camera camera) { this.context = context; this.camera = camera; if (!Cull()) { return; } Setup(); DrawVisibleGeometry(); Submit(); } 实际的剔除是通过在上下文上调用 Cull 来完成的，这会产生一个 CullingResults 结构。如果成功，在 Cull 中执行此操作并将结果存储在字段中。在这种情况下，我们必须通过在前面写 ref 将剔除参数作为引用参数传递。 CullingResults cullingResults; bool Cull () { if (camera.TryGetCullingParameters(out ScriptableCullingParameters p)) { cullingResults = context.Cull(ref p); return true; } return false; } 2.6 绘制几何图形 (Drawing Geometry) 一旦我们知道了哪些物体是可见的，就可以开始渲染它们。这是通过调用 context 的 DrawRenderers 方法并传入 culling results 作为参数来完成的，以此告知它使用哪些 renderer。除此之外，我们还必须提供 drawing settings 和 filtering settings。两者都是结构体—— DrawingSettings 和 FilteringSettings ——最初我们将使用它们的默认构造函数。两者都必须以引用方式传递。请在 DrawVisibleGeometry 中，绘制 skybox 之前执行此操作。 void DrawVisibleGeometry () { var drawingSettings = new DrawingSettings(); var filteringSettings = new FilteringSettings(); context.DrawRenderers( cullingResults, ref drawingSettings, ref filteringSettings ); context.DrawSkybox(camera); } 我们还看不到任何东西，因为我们还必须指示允许哪些类型的着色器通道 (shader passes)。由于我们在本教程中仅支持无光照 (unlit) 着色器，因此我们必须获取 SRPDefaultUnlit 通道的着色器标签 ID，我们可以执行一次并将其缓存在静态字段中。 static ShaderTagId unlitShaderTagId = new ShaderTagId(\"SRPDefaultUnlit\"); 将其作为第一个参数传递给 DrawingSettings 构造函数，同时传入一个新的 SortingSettings 结构体值。将 camera 传递给排序设置的构造函数，因为它被用于确定是应用正交排序还是基于距离的排序。 void DrawVisibleGeometry () { var sortingSettings = new SortingSettings(camera); var drawingSettings = new DrawingSettings( unlitShaderTagId, sortingSettings ); var filteringSettings = new FilteringSettings(); context.DrawRenderers( cullingResults, ref drawingSettings, ref filteringSettings ); context.DrawSkybox(camera); } 除此之外，我们还必须指示允许哪些渲染队列（render queues）。将 RenderQueueRange.all 传递给 FilteringSettings 构造函数，以便我们包含所有内容。 var filteringSettings = new FilteringSettings(RenderQueueRange.all); 绘制无光照几何体 只有使用了 unlit shader 的可见对象才会被绘制。所有的 draw calls 都会列在 frame debugger 中，并被分组在 RenderLoop.Draw 下。透明对象似乎出现了一些异常，但让我们先来看看对象的绘制顺序。frame debugger 会显示这一顺序，你可以通过依次选择或使用方向键来逐步查看各个 draw calls。 Your browser does not support the video tag. Here is a link to the video file instead. 逐步查看 frame debugger 绘制顺序是杂乱无章的。我们可以通过设置排序设置的 criteria 属性来强制执行特定的绘制顺序。让我们使用 SortingCriteria.CommonOpaque。 void DrawVisibleGeometry () { var sortingSettings = new SortingSettings(camera) { criteria = SortingCriteria.CommonOpaque }; … } Your browser does not support the video tag. Here is a link to the video file instead. 逐步查看 frame debugger 普通不透明排序 现在物体大致按照从前到后的顺序进行绘制，这对于不透明物体来说是最理想的。如果某个物体最终被绘制在另一个物体后面，其隐藏的片元（fragments）就可以被跳过，从而加快渲染速度。通用的不透明排序选项还会考虑其他一些标准，包括 render queue 和 materials。 2.7 分别绘制不透明和透明几何体 Frame debugger 显示透明对象已被绘制，但天空盒（skybox）却覆盖了所有未处于不透明对象前方的区域。天空盒在不透明几何体之后绘制，以便跳过所有被遮挡的片段，但它同时也会覆盖透明几何体。这是因为透明 shader 不会写入深度缓冲区（depth buffer）。它们不会遮挡背后的内容，因为我们可以看穿它们。解决方案是先绘制不透明对象，接着绘制天空盒，最后再绘制透明对象。 我们可以通过切换到 RenderQueueRange.opaque 来从初始的 DrawRenderers 调用中排除透明物体。 var filteringSettings = new FilteringSettings(RenderQueueRange.opaque); 然后在绘制天空盒之后再次调用 DrawRenderers。但在这样做之前，将渲染队列范围更改为 RenderQueueRange.transparent。还要将排序标准更改为 SortingCriteria.CommonTransparent 并再次设置绘制设置的排序。这将反转透明对象的绘制顺序。 void DrawVisibleGeometry () { var sortingSettings = new SortingSettings(camera) { criteria = SortingCriteria.CommonOpaque }; var drawingSettings = new DrawingSettings( unlitShaderTagId, sortingSettings ); var filteringSettings = new FilteringSettings(RenderQueueRange.opaque); context.DrawRenderers( cullingResults, ref drawingSettings, ref filteringSettings ); context.DrawSkybox(camera); sortingSettings.criteria = SortingCriteria.CommonTransparent; drawingSettings.sortingSettings = sortingSettings; filteringSettings.renderQueueRange = RenderQueueRange.transparent; context.DrawRenderers( cullingResults, ref drawingSettings, ref filteringSettings ); } Your browser does not support the video tag. Here is a link to the video file instead. 逐步查看 frame debugger 不透明，然后天空盒，最后透明 不透明物体不会写入深度缓冲区，因此将它们从前到后排序没有性能优势。但当不透明物体在视觉上相互遮挡时，必须从后到前绘制以正确混合。 不幸的是，从后到前的排序并不能保证正确的混合，因为排序是针对单个物体，并且仅基于物体的位置。相交和大的不透明物体仍然可能产生错误的结果。这有时可以通过将几何体切割成更小的部分来解决。 3 编辑器渲染 (Editor Rendering) 我们的 RP 可以正确绘制 unlit 对象，但我们还可以做一些工作来提升在 Unity editor 中使用它的体验。 3.1 绘制旧版着色器 (Drawing Legacy Shaders) 由于我们的 pipeline 仅支持 unlit shader pass，使用其他 pass 的对象将不会被渲染，从而变得不可见。虽然这在逻辑上是正确的，但它掩盖了场景中某些对象使用了错误 shader 的事实。因此，让我们还是渲染它们，但采用分开渲染的方式。 如果有人从默认的 Unity 项目开始，随后切换到我们的 RP，那么他们的场景中可能会存在使用错误 shader 的对象。为了涵盖所有 Unity 的默认 shader，我们需要为 Always, ForwardBase, PrepassBase, Vertex, VertexLMRGBM, 和 VertexLM 使用 shader tag ID。请在一个静态数组中记录这些 ID。 static ShaderTagId[] legacyShaderTagIds = { new ShaderTagId(\"Always\"), new ShaderTagId(\"ForwardBase\"), new ShaderTagId(\"PrepassBase\"), new ShaderTagId(\"Vertex\"), new ShaderTagId(\"VertexLMRGBM\"), new ShaderTagId(\"VertexLM\") }; 在绘制可见几何体之后，通过一个独立的方法绘制所有不支持的 shader，从第一个 pass 开始。由于这些是无效的 pass，结果无论如何都会是错误的，所以我们不需要关心其他设置。我们可以通过 FilteringSettings.defaultValue 属性获取默认的过滤设置。 public void Render (ScriptableRenderContext context, Camera camera) { Setup(); DrawVisibleGeometry(); DrawUnsupportedShaders(); Submit(); } void DrawUnsupportedShaders () { var drawingSettings = new DrawingSettings( legacyShaderTagIds[0], new SortingSettings(camera) ); var filteringSettings = FilteringSettings.defaultValue; context.DrawRenderers( cullingResults, ref drawingSettings, ref filteringSettings ); } 我们可以通过在 drawing settings 上调用 SetShaderPassName 并传入绘制顺序索引和标签作为参数来绘制多个 pass。对数组中的所有 pass 执行此操作。因为我们在构建 drawing settings 已经设置了第一个 pass且同时设置了 SortingSettings，后面从第二个开始指定shader tag id。 var drawingSettings = new DrawingSettings( legacyShaderTagIds[0], new SortingSettings(camera) ); for (int i = 1; i &lt; legacyShaderTagIds.Length; i++) { drawingSettings.SetShaderPassName(i, legacyShaderTagIds[i]); } 标准着色器渲染为黑色 使用 standard shader 渲染的对象会显示出来，但它们现在是纯黑色的，因为我们的 RP 尚未为它们设置所需的 shader 属性。 3.2 错误材质 (Error Material) 为了清晰地标出哪些对象使用了不支持的 shader，我们将使用 Unity 的错误 shader 来绘制它们。通过调用 Shader.Find 并传入 Hidden/InternalErrorShader 字符串作为参数来查找该 shader，并以此构造一个新的材质。通过静态字段缓存该材质，以免每帧都创建新材质。然后将其赋值给绘制设置的 overrideMaterial 属性。 static Material errorMaterial; void DrawUnsupportedShaders () { if (errorMaterial == null) { errorMaterial = new Material(Shader.Find(\"Hidden/InternalErrorShader\")); } var drawingSettings = new DrawingSettings( legacyShaderTagIds[0], new SortingSettings(camera) ) { overrideMaterial = errorMaterial }; … } 使用洋红色错误着色器进行渲染 现在所有无效对象都已可见，且明显呈现错误状态。 3.3 Partial类 绘制无效对象对开发很有用，但不适用于发布的应用程序。因此让我们将 CameraRenderer 的所有仅限编辑器的代码放在一个单独的分部类文件中。首先复制原始的 CameraRenderer 脚本资产并将其重命名为 CameraRenderer.Editor。 一个类，两个脚本资产 然后将原始的 CameraRenderer 更改为 partial class，并从中移除标签数组、错误材质以及 DrawUnsupportedShaders 方法。 public partial class CameraRenderer { … } 清理另一个 partial class 文件，使其仅包含我们从原文件中移除的内容。 using UnityEngine; using UnityEngine.Rendering; partial class CameraRenderer { static ShaderTagId[] legacyShaderTagIds = { … }; static Material errorMaterial; void DrawUnsupportedShaders () { … } } 编辑器部分的内容仅需存在于编辑器中，因此请将其设置为以 UNITY_EDITOR 为条件。 partial class CameraRenderer { #if UNITY_EDITOR static ShaderTagId[] legacyShaderTagIds = { … }; static Material errorMaterial; void DrawUnsupportedShaders () { … } #endif } 然而，此时进行 build 将会失败，因为另一部分始终包含对 DrawUnsupportedShaders 的调用，而该方法现在仅在 editor 模式下存在。为了解决这个问题，我们也需要将该方法设为 partial。具体做法是在方法签名之前始终加上 partial ，类似于抽象方法的声明。我们可以在类定义的任何部分执行此操作，所以让我们将其放在 editor 部分。完整的方法声明也必须标记为 partial 。 partial void DrawUnsupportedShaders (); #if UNITY_EDITOR … partial void DrawUnsupportedShaders () { … } #endif 现在可以成功进行 build 编译了。编译器将自动移除所有未包含完整声明的 partial method 调用。 3.4 绘制辅助线 (Drawing Gizmos) 目前我们的 RP 不绘制辅助线 (Gizmos)，无论是在场景窗口还是在启用了它们的游视窗口中。 没有辅助线的场景 我们可以通过调用 UnityEditor.Handles.ShouldRenderGizmos 来检查是否应该绘制 gizmos。如果是，我们必须在 context 上调用 DrawGizmos ，并将 camera 作为第一个参数，第二个参数用于指示应绘制哪个 gizmos 子集。共有两个子集，分别用于图像效果（image effects）之前和之后。由于我们目前不支持图像效果，我们将同时调用两者。请在一个新的仅限编辑器使用的 DrawGizmos 方法中执行此操作。 using UnityEditor; using UnityEngine; using UnityEngine.Rendering; partial class CameraRenderer { partial void DrawGizmos (); partial void DrawUnsupportedShaders (); #if UNITY_EDITOR … partial void DrawGizmos () { if (Handles.ShouldRenderGizmos()) { context.DrawGizmos(camera, GizmoSubset.PreImageEffects); context.DrawGizmos(camera, GizmoSubset.PostImageEffects); } } partial void DrawUnsupportedShaders () { … } #endif } Gizmos 应该在所有其他内容之后绘制。 public void Render (ScriptableRenderContext context, Camera camera) { … Setup(); DrawVisibleGeometry(); DrawUnsupportedShaders(); DrawGizmos(); Submit(); } 带有辅助线的场景 3.5 绘制 Unity UI 另外一件需要我们注意的事情是 Unity 的游戏内用户界面。例如，通过 GameObject / UI / Button 添加一个按钮来创建一个简单的 UI。它会显示在游戏窗口中，但不会出现在场景窗口中。 游戏窗口中的UI按钮 frame debugger 显示 UI 是单独渲染的，而不是由我们的 RP 渲染。 帧调试器中的UI 至少，当 Canvas 组件的 Render Mode 设置为 Screen Space - Overlay （这是默认值）时是这样的。将其更改为 Screen Space - Camera 并使用主摄像机作为其 Render Camera ，将使其成为透明几何体的一部分。 帧调试器中的屏幕空间摄像机UI UI 在场景窗口中渲染时总是使用 World Space 模式，这就是为什么它通常看起来非常大。但虽然我们可以通过场景窗口编辑 UI，它却不会被绘制出来。 场景窗口中不可见的UI 在为 scene 窗口进行渲染时，我们必须通过调用以相机为参数的 ScriptableRenderContext.EmitWorldGeometryForSceneView ，显式地将 UI 添加到世界几何体中。在一个新的仅限编辑器使用的 PrepareForSceneWindow 方法中执行此操作。当场景相机的 cameraType 属性等于 CameraType.SceneView 时，我们正在使用该相机进行渲染。 partial void PrepareForSceneWindow (); #if UNITY_EDITOR … partial void PrepareForSceneWindow () { if (camera.cameraType == CameraType.SceneView) { ScriptableRenderContext.EmitWorldGeometryForSceneView(camera); } } #endif 由于这可能会向场景添加几何体，因此必须在剔除之前完成。 public void Render (ScriptableRenderContext context, Camera camera) { this.context = context; this.camera = camera; PrepareForSceneWindow(); if (!Cull()) { return; } … } 场景窗口中可见的UI 4 多个摄像机 (Multiple Cameras) 场景中可能会有多个处于激活状态的摄像机。如果是这样，我们必须确保它们能协同工作。 4.1 两个摄像机 每个摄像机都有一个 Depth 值，默认主摄像机为 -1。它们按深度增加的顺序渲染。为了观察到这一点，请复制 Main Camera ，将其重命名为 Secondary Camera ，并将其 Depth 设置为 0。同时建议给它设置另一个标签，因为 MainCamera 应该只由单个摄像机使用。 两个摄像机分组在单个样本范围内 场景现在被渲染了两次。结果图像仍然是相同的，因为渲染目标在两者之间被清除了。帧调试器显示了这一点，但由于具有相同名称的相邻样本范围会被合并，我们最终得到了一个单独的 Render Camera 范围。 如果每个摄像机都有自己的范围，那会更清晰。为了实现这一点，添加一个仅限编辑器的 PrepareBuffer 方法，使缓冲区的名称等于摄像机的名称。 partial void PrepareBuffer (); #if UNITY_EDITOR … partial void PrepareBuffer () { buffer.name = camera.name; } #endif 在准备场景窗口之前调用它。 public void Render (ScriptableRenderContext context, Camera camera) { this.context = context; this.camera = camera; PrepareBuffer(); PrepareForSceneWindow(); … } 每个摄像机分开的样本 4.2 处理变化的缓冲区名称 虽然帧调试器（frame debugger）现在为每个摄像机显示了独立的采样层级，但当我们进入播放模式时，Unity 的控制台会充斥着警告消息，提示 BeginSample 和 EndSample 的计数必须匹配。这是因为我们为采样及其 buffer 使用了不同的名称，导致它产生了混淆。除此之外，每次访问摄像机的 name 属性时都会分配内存，因此我们不希望在构建版本（builds）中这样做。 为了解决这两个问题，我们将添加一个 SampleName 字符串属性。如果在编辑器中，我们在 PrepareBuffer 中将其与 buffer 的名称一起设置，否则它只是 Render Camera 字符串的一个常量别名。 #if UNITY_EDITOR string SampleName { get; set; } partial void PrepareBuffer () { buffer.name = SampleName = camera.name; } #else const string SampleName = bufferName; #endif 在 Setup 和 Submit 中为样本使用 SampleName。 void Setup () { context.SetupCameraProperties(camera); buffer.ClearRenderTarget(true, true, Color.clear); buffer.BeginSample(SampleName); ExecuteBuffer(); } void Submit () { buffer.EndSample(SampleName); ExecuteBuffer(); context.Submit(); } 我们可以通过检查 profiler（通过 Window / Analysis / Profiler 打开）并先在编辑器中播放来查看差异。切换到 Hierarchy 模式并按 GC Alloc 列排序。你会看到两个 GC.Alloc 调用的条目，总共分配了 100 字节，这是由获取摄像机名称引起的。再往下看，你会看到这些名称作为采样显示： Main Camera 和 Secondary Camera 。 具有独立 sample 和 100B 分配的 Profiler 接下来，启用 Development Build 和 Autoconnect Profiler 进行构建。运行构建并确保分析器已连接并正在记录。在这种情况下，我们没有得到 100 字节的分配，我们得到了单个 Render Camera 样本。 构建的分析 我们可以通过将获取 camera 名称的代码包裹在名为 Editor Only 的 profiler 采样中，来明确我们仅在 editor 中分配内存，而不在 build 中分配。在这种情况下，我们需要调用 UnityEngine.Profiling 命名空间下的 Profiler.BeginSample 和 Profiler.EndSample。只有 BeginSample 需要传递名称参数。 partial void PrepareBuffer () { Profiler.BeginSample(\"Editor Only\"); buffer.name = SampleName = camera.name; Profiler.EndSample(); } 编辑器特有分配变得明显 4.3 Layers Camera 也可以配置为仅观察特定层级（layer）上的物体。这可以通过调整它们的 Culling Mask 来实现。为了查看实际效果，让我们将所有使用 standard shader 的对象移动到 Ignore Raycast 层。 图层切换为Ignore Raycast 从 Main Camera 的剔除遮罩（culling mask）中排除该图层。 剔除Ignore Raycast图层 并使其成为 Secondary Camera 唯一可见的图层。 剔除除了Ignore Raycast之外的所有图层 因为 Secondary Camera 最后渲染，我们最终只能看到无效对象。 游戏窗口中仅可见Ignore Raycast图层 4.4 清除标志 (Clear Flags) 我们可以通过调整第二个渲染摄像机的清除标志（clear flags）来合并两个摄像机的结果。这些标志由 CameraClearFlags 枚举定义，我们可以通过摄像机的 clearFlags 属性获取。请在 Setup 方法中的清除操作之前执行此步骤。 void Setup () { context.SetupCameraProperties(camera); CameraClearFlags flags = camera.clearFlags; buffer.ClearRenderTarget(true, true, Color.clear); … } CameraClearFlags 枚举定义了四个值。从 1 到 4 分别是 Skybox, Color, Depth, 和 Nothing。这些实际上不是独立的标志值，而是代表递减的清除量。深度缓冲区在除了最后一种情况外的所有情况下都必须被清除，因此当标志值最多为 Depth 时。 buffer.ClearRenderTarget(flags &lt;= CameraClearFlags.Depth, true, Color.clear); 我们实际上只需要在 flags 设置为 Color 时清除 color buffer，因为在 Skybox 的情况下，我们最终都会替换掉所有之前的颜色数据。然而， buffer.ClearRenderTarget( flags &lt;= CameraClearFlags.Depth, flags &lt;= CameraClearFlags.Color, Color.clear ); 如果我们清除为纯色，我们必须使用摄像机的背景颜色。但因为我们在线性颜色空间中渲染，我们必须将该颜色转换为线性空间，所以我们最终需要 camera.backgroundColor.linear。在所有其他情况下，颜色并不重要，所以我们可以用 Color.clear 满足。 buffer.ClearRenderTarget( flags &lt;= CameraClearFlags.Depth, flags &lt;= CameraClearFlags.Color, flags == CameraClearFlags.Color ? camera.backgroundColor.linear : Color.clear ); 因为 Main Camera 是第一个渲染的，它的清除标志应该设置为 Skybox 或 Color。启用帧调试器时，我们总是从清除缓冲区开始，但这在一般情况下并不能保证。 Secondary Camera 的 clear flags 决定了两个 camera 的渲染结果如何合并。在选择 skybox 或 color 的情况下，之前的渲染结果会被完全替换。当仅清除 depth 时， Secondary Camera 会正常渲染，但不绘制 skybox，因此之前的渲染结果会作为背景显示。当不清除任何内容（nothing）时，depth buffer 会被保留，因此 unlit 对象会像是由同一个 camera 绘制的一样遮挡无效对象。然而，由前一个 camera 绘制的 transparent 对象没有深度信息，因此会被覆盖，就像之前的 skybox 一样。 Clear color, depth-only, and nothing. 通过调整摄像机的 Viewport Rect ，还可以将渲染区域缩小到仅占整个渲染目标的一小部分。渲染目标的其余部分保持不变。在这种情况下，清除操作通过 Hidden/InternalClear shader 完成。stencil buffer 用于将渲染限制在 viewport 区域。 缩小次要摄像机的视口，清除颜色 请注意，每帧渲染多个摄像机意味着剔除、设置、排序等也必须执行多次。使用一个摄像机处理每个独特的视点通常是最有效的方法。 下一篇教程是绘制调用。" }, { "title": "三平面映射(翻译二十七)", "url": "/posts/triplanar-mapping/", "categories": "Unity3D, Rendering", "tags": "Shader", "date": "2018-02-01 09:44:33 +0800", "content": "移除对 UV 和切线的依赖 支持通用曲面方法 使用平面投影 混合三种映射方式 1. 无 UV 坐标的纹理化 通常进行纹理映射的方法是使用网格中每个顶点存储的 UV 坐标。但这并不是唯一的方法。有时，根本没有可用的 UV 坐标。例如，在处理任意形状的程序化几何体时。在运行时创建地形或洞穴系统时，通常无法为适当的纹理展开生成 UV 坐标。在这种情况下，我们必须使用另一种方法将纹理映射到表面上。其中一种方法就是三平面映射 (Triplanar Mapping)。 到目前为止，我们一直假设 UV 坐标是可用的。我们的 My Lighting Input 和 My Lighting 着色器包含文件都依赖于它们。虽然我们可以创建不依赖于顶点 UV 的替代方案，但如果能让当前的文件在有无 UV 的情况下都能工作会更方便。这需要一些改动。 我们将保持当前的方法作为默认方案，但在定义了 NO_DEFAULT_UV 时切换到无 UV 模式。 1.1 不使用默认 UV 当网格数据不包含 UV 时，我们就没有 UV 可以从顶点传递到片段程序。因此，让 My Lighting Input 中的 UV 插值器的存在取决于 NO_DEFAULT_UV。 struct InterpolatorsVertex { … #if !defined(NO_DEFAULT_UV) float4 uv : TEXCOORD0; #endif … }; struct Interpolators { … #if !defined(NO_DEFAULT_UV) float4 uv : TEXCOORD0; #endif … }; 有多个函数假设插值器始终包含 UV，因此我们必须确保它们能继续工作和编译。我们将通过在插值器声明下方引入一个新的 GetDefaultUV 函数来实现这一点。当没有 UV 可用时，它将简单地返回零，否则返回常规 UV。 我们还将允许通过定义 UV_FUNCTION 来提供替代方法，以防万一。这类似于 ALBEDO_FUNCTION，但覆盖必须在包含 My Lighting Input 之前定义。 float4 GetDefaultUV (Interpolators i) { #if defined(NO_DEFAULT_UV) return float4(0, 0, 0, 0); #else return i.uv; #endif } #if !defined(UV_FUNCTION) #define UV_FUNCTION GetDefaultUV #endif 现在我们可以将所有使用 i.uv 的地方更改为 UV_FUNCTION(i)。我只展示了 GetDetailMask 的更改，但它适用于所有 getter 函数。 float GetDetailMask (Interpolators i) { #if defined (_DETAIL_MASK) return tex2D(_DetailMask, UV_FUNCTION(i).xy).a; #else return 1; #endif } 接下来是 My Lighting，我们必须确保在没有 UV 可用时跳过顶点程序中所有与 UV 相关的操作。这适用于纹理坐标变换，也适用于默认的顶点位移方法。 InterpolatorsVertex MyVertexProgram (VertexData v) { … #if !defined(NO_DEFAULT_UV) i.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); #if VERTEX_DISPLACEMENT float displacement = tex2Dlod(_DisplacementMap, float4(i.uv.xy, 0, 0)).g; displacement = (displacement - 0.5) * _DisplacementStrength; v.normal = normalize(v.normal); v.vertex.xyz += v.normal * displacement; #endif #endif … } 视差效果（Parallax effect）也依赖于默认 UV，因此在 UV 不可用时跳过它。 void ApplyParallax (inout Interpolators i) { #if defined(_PARALLAX_MAP) &amp;&amp; !defined(NO_DEFAULT_UV) … #endif } 1.2 收集表面属性 没有 UV，必须有另一种方法来确定用于光照的表面属性。为了使其尽可能通用，我们的包含文件不应该关心这些属性是如何获得的。我们只需要一种通用的方法来提供表面属性。我们可以使用类似于 Unity 表面着色器（Surface Shaders）的方法，依靠一个函数来设置所有表面属性。 创建一个新的 MySurface.cginc 包含文件。在其中定义一个 SurfaceData 结构体，包含光照所需的所有表面属性：反照率（albedo）、发射（emission）、法线（normal）、Alpha、金属度（metallic）、遮蔽（occlusion）和光滑度（smoothness）。 #if !defined(MY_SURFACE_INCLUDED) #define MY_SURFACE_INCLUDED struct SurfaceData { float3 albedo, emission, normal; float alpha, metallic, occlusion, smoothness; }; #endif 我们把它放在一个单独的文件中，这样其他代码就可以在包含任何其他文件之前使用它。但我们的文件也会依赖它，所以把它包含在 My Lighting Input 中。 #include \"UnityPBSLighting.cginc\" #include \"AutoLight.cginc\" #include \"MySurface.cginc\" 在 My Lighting 中，在 MyFragmentProgram 的开头、ApplyParallax 之后和使用 Alpha 之前，使用默认函数设置一个新的 SurfaceData surface 变量。然后更改 Alpha 代码以依赖 surface.alpha 而不是调用 GetAlpha。同时移动 InitializeFragmentNormal，以便在配置表面之前处理法线向量。 FragmentOutput MyFragmentProgram (Interpolators i) { UNITY_SETUP_INSTANCE_ID(i); #if defined(LOD_FADE_CROSSFADE) UnityApplyDitherCrossFade(i.vpos); #endif ApplyParallax(i); InitializeFragmentNormal(i); SurfaceData surface; surface.normal = i.normal; surface.albedo = ALBEDO_FUNCTION(i); surface.alpha = GetAlpha(i); surface.emission = GetEmission(i); surface.metallic = GetMetallic(i); surface.occlusion = GetOcclusion(i); surface.smoothness = GetSmoothness(i); float alpha = surface.alpha; #if defined(_RENDERING_CUTOUT) clip(alpha - _Cutoff); #endif // InitializeFragmentNormal(i); … } 现在在确定片段颜色时，依赖 surface 而不是再次调用 getter 函数。 float3 albedo = DiffuseAndSpecularFromMetallic( surface.albedo, surface.metallic, specularTint, oneMinusReflectivity ); … float4 color = UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, surface.smoothness, i.normal, viewDir, CreateLight(i), CreateIndirectLight(i, viewDir) ); color.rgb += surface.emission; 以及在填充延迟渲染的 G-buffers 时： #if defined(DEFERRED_PASS) #if !defined(UNITY_HDR_ON) color.rgb = exp2(-color.rgb); #endif output.gBuffer0.rgb = albedo; output.gBuffer0.a = surface.occlusion; output.gBuffer1.rgb = specularTint; output.gBuffer1.a = surface.smoothness; output.gBuffer2 = float4(i.normal * 0.5 + 0.5, 1); output.gBuffer3 = color; … #endif CreateIndirectLight 函数也使用了 getter 函数，因此给它添加一个 SurfaceData 参数并改用该参数。 UnityIndirect CreateIndirectLight ( Interpolators i, float3 viewDir, SurfaceData surface ) { … #if defined(FORWARD_BASE_PASS) || defined(DEFERRED_PASS) … float3 reflectionDir = reflect(-viewDir, i.normal); Unity_GlossyEnvironmentData envData; envData.roughness = 1 - surface.smoothness; … float occlusion = surface.occlusion; … #endif return indirectLight; } 然后在 MyFragmentProgram 中为其调用添加 surface 作为参数。 CreateLight(i), CreateIndirectLight(i, viewDir, surface) 1.3 定制表面 为了能够更改表面数据的获取方式，我们将再次允许定义自定义函数。这个函数需要输入才能工作。默认情况下，那是 UV 坐标，主 UV 和细节 UV 都打包在一个 float4 中。替代输入可以是位置和法线向量。在我们的 Surface 文件中添加一个包含所有这些输入的 SurfaceParameters 结构体。 struct SurfaceData { float3 albedo, emission, normal; float alpha, metallic, occlusion, smoothness; }; struct SurfaceParameters { float3 normal, position; float4 uv; }; 回到 My Lighting，调整 MyFragmentProgram，使其在定义了 SURFACE_FUNCTION 时使用不同的方式设置表面数据。在这种情况下，用法线向量填充 surface 并将所有其他值设置为默认值。然后创建表面参数并调用自定义表面函数。它的参数是 surface（作为 inout 参数）和参数结构体。 SurfaceData surface; #if defined(SURFACE_FUNCTION) surface.normal = i.normal; surface.albedo = 1; surface.alpha = 1; surface.emission = 0; surface.metallic = 0; surface.occlusion = 1; surface.smoothness = 0.5; SurfaceParameters sp; sp.normal = i.normal; sp.position = i.worldPos.xyz; sp.uv = UV_FUNCTION(i); SURFACE_FUNCTION(surface, sp); #else surface.normal = i.normal; surface.albedo = ALBEDO_FUNCTION(i); surface.alpha = GetAlpha(i); surface.emission = GetEmission(i); surface.metallic = GetMetallic(i); surface.occlusion = GetOcclusion(i); surface.smoothness = GetSmoothness(i); #endif 由于 SURFACE_FUNCTION 可能会更改表面法线，之后将其赋回给 i.normal。这样我们就不需要更改所有使用 i.normal 的代码。 #if defined(SURFACE_FUNCTION) … #else … #endif i.normal = surface.normal; 1.4 无切线空间 请注意，与 Unity 的表面着色器方法不同，我们是在世界空间（而非切线空间）中使用法线向量。如果我们想在 SURFACE_FUNCTION 中使用切线空间法线映射，那么我们必须自己显式地执行此操作。我们还可以支持更多关于法线在调用 SURFACE_FUNCTION 前后应如何处理的配置选项，但我们在本教程中不会这样做。 我们要做的，是在不使用切线时能够关闭默认的切线空间法线映射方法。这样在不使用切线时可以节省工作。我们通过仅在默认法线映射或视差映射处于活动状态时打开切线空间来实现这一点。在 My Lighting Input 中用一个方便的 REQUIRES_TANGENT_SPACE 宏来表示。 #if defined(_NORMAL_MAP) || defined(_DETAIL_NORMAL_MAP) || defined(_PARALLAX_MAP) #define REQUIRES_TANGENT_SPACE 1 #define TESSELLATION_TANGENT 1 #endif #define TESSELLATION_UV1 1 #define TESSELLATION_UV2 1 现在我们只需在需要时包含切向和副法线向量插值器。 struct InterpolatorsVertex { … #if REQUIRES_TANGENT_SPACE #if defined(BINORMAL_PER_FRAGMENT) float4 tangent : TEXCOORD2; #else float3 tangent : TEXCOORD2; float3 binormal : TEXCOORD3; #endif #endif … }; struct Interpolators { … #if REQUIRES_TANGENT_SPACE #if defined(BINORMAL_PER_FRAGMENT) float4 tangent : TEXCOORD2; #else float3 tangent : TEXCOORD2; float3 binormal : TEXCOORD3; #endif #endif … }; 在 My Lighting 中，我们可以跳过在 MyVertexProgram 中设置这些向量。 InterpolatorsVertex MyVertexProgram (VertexData v) { … #if REQUIRES_TANGENT_SPACE #if defined(BINORMAL_PER_FRAGMENT) i.tangent = float4(UnityObjectToWorldDir(v.tangent.xyz), v.tangent.w); #else i.tangent = UnityObjectToWorldDir(v.tangent.xyz); i.binormal = CreateBinormal(i.normal, i.tangent, v.tangent.w); #endif #endif … } 在没有切线空间的情况下，InitializeFragmentNormal 简化为仅对插值法线进行归一化。 void InitializeFragmentNormal(inout Interpolators i) { #if REQUIRES_TANGENT_SPACE float3 tangentSpaceNormal = GetTangentSpaceNormal(i); #if defined(BINORMAL_PER_FRAGMENT) float3 binormal = CreateBinormal(i.normal, i.tangent.xyz, i.tangent.w); #else float3 binormal = i.binormal; #endif i.normal = normalize( tangentSpaceNormal.x * i.tangent + tangentSpaceNormal.y * binormal + tangentSpaceNormal.z * i.normal ); #else i.normal = normalize(i.normal); #endif } 1.5 三平面着色器 我们所有的着色器仍然有效，但现在可以使用我们的包含文件而不使用切线空间，并使用替代表面数据。让我们创建一个新的着色器来利用这一点。首先，创建一个新的 MyTriplanarMapping.cginc 包含文件。让它定义 NO_DEFAULT_UV，然后包含 Surface.cginc。实际上，由于我们将使用 My Lighting Input 中已经定义的 _MainTex 属性，请改为包含该文件。然后创建一个带有 inout SurfaceData surface 参数和常规 SurfaceParameters parameters 参数的 MyTriplanarSurfaceFunction。目前，只需让它使用法线来设置反照率。将此函数定义为 SURFACE_FUNCTION。 #if !defined(MY_TRIPLANAR_MAPPING_INCLUDED) #define MY_TRIPLANAR_MAPPING_INCLUDED #define NO_DEFAULT_UV #include \"My Lighting Input.cginc\" void MyTriPlanarSurfaceFunction ( inout SurfaceData surface, SurfaceParameters parameters ) { surface.albedo = parameters.normal * 0.5 + 0.5; } #define SURFACE_FUNCTION MyTriPlanarSurfaceFunction #endif 创建一个使用此包含文件而不是 My Lighting Input 的新着色器。我们将制作一个不带透明度的最小化着色器，支持通常的渲染管线，外加雾效和实例化。这是带有前向基础（forward base）和附加（additive）通道的着色器。 Shader \"Custom/Triplanar Mapping\" { Properties { _MainTex (\"Albedo\", 2D) = \"white\" {} } SubShader { Pass { Tags { \"LightMode\" = \"ForwardBase\" } CGPROGRAM #pragma target 3.0 #pragma multi_compile_fwdbase #pragma multi_compile_fog #pragma multi_compile_instancing #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #define FORWARD_BASE_PASS #include \"MyTriplanarMapping.cginc\" #include \"My Lighting.cginc\" ENDCG } Pass { Tags { \"LightMode\" = \"ForwardAdd\" } Blend One One ZWrite Off CGPROGRAM #pragma target 3.0 #pragma multi_compile_fwdadd_fullshadows #pragma multi_compile_fog #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"MyTriplanarMapping.cginc\" #include \"My Lighting.cginc\" ENDCG } } } 这里是延迟渲染（deferred）和阴影投射（shadow caster）通道。请注意，阴影通道不需要特殊处理，因为它不关心不透明几何体的表面属性。我们目前还没有添加光照贴图（lightmapping）支持，所以目前没有 meta 通道。 Shader \"Custom/Triplanar Mapping\" { … SubShader { … Pass { Tags { \"LightMode\" = \"Deferred\" } CGPROGRAM #pragma target 3.0 #pragma exclude_renderers nomrt #pragma multi_compile_prepassfinal #pragma multi_compile_instancing #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #define DEFERRED_PASS #include \"MyTriplanarMapping.cginc\" #include \"My Lighting.cginc\" ENDCG } Pass { Tags { \"LightMode\" = \"ShadowCaster\" } CGPROGRAM #pragma target 3.0 #pragma multi_compile_shadowcaster #pragma multi_compile_instancing #pragma vertex MyShadowVertexProgram #pragma fragment MyShadowFragmentProgram #include \"My Shadows.cginc\" ENDCG } } } 使用我们的新着色器创建一个材质并尝试一下。我使用了旧的测试纹理作为材质的主纹理，虽然此时它还没有被用到。 使用法线作为反照率的三平面映射材质 2. 三平面纹理化 当顶点 UV 坐标不可用时，我们如何执行纹理映射？我们必须使用替代方案。唯一可行的方法是使用世界位置——或者可能是对象空间位置——作为纹理映射的替代 UV 坐标来源。 2.1 基于位置的纹理映射 片段的世界位置是一个 3D 向量，但常规纹理映射是在 2D 中完成的。所以我们必须选择两个维度作为 UV 坐标，这意味着我们将纹理映射到 3D 空间的一个平面上。最明显的选择是使用 XY 坐标。 surface.albedo = tex2D(_MainTex, parameters.position.xy); 使用位置 XY 作为 UV 坐标 使用 3D 纹理呢？ 这也是可能的，但 3D 纹理需要更多的存储空间，并且很难做得好看。 结果是我们看到纹理沿 Z 轴投影。但这并不是唯一可能的方向。我们还可以通过改用 XZ 坐标来沿 Y 轴投影。这对应于纹理化地形时常用的平面纹理映射。 surface.albedo = tex2D(_MainTex, parameters.position.xz); 使用位置 XZ 作为 UV 坐标 第三个选项是通过使用 YZ 坐标沿 X 投影。 surface.albedo = tex2D(_MainTex, parameters.position.yz); 使用位置 YZ 作为 UV 坐标 但是当我们使用 YZ 时，纹理最终会旋转 90°。为了保持预期的方向，我们必须改用 ZY。 surface.albedo = tex2D(_MainTex, parameters.position.zy); 使用位置 ZY 作为 UV 坐标 2.2 合并所有三个映射 当表面主要与投影轴对齐时，单平面映射效果很好，但在不对齐时看起来很糟糕。当沿一个轴的效果不好时，沿另一个轴的效果可能会更好。因此，支持所有三个映射是有用的，这需要我们提供三对不同的 UV 坐标。 让我们保持确定这些 UV 坐标的逻辑独立。创建一个包含所有三个轴坐标对的 TriplanarUV 结构体。然后制作一个 GetTriplanarUV 函数，根据表面参数设置 UV。 struct TriplanarUV { float2 x, y, z; }; TriplanarUV GetTriplanarUV (SurfaceParameters parameters) { TriplanarUV triUV; float3 p = parameters.position; triUV.x = p.zy; triUV.y = p.xz; triUV.z = p.xy; return triUV; } 在 MyTriPlanarSurfaceFunction 中使用此函数，并对所有三个投影进行采样。最终的反照率变为它们的平均值。 void MyTriPlanarSurfaceFunction ( inout SurfaceData surface, SurfaceParameters parameters ) { TriplanarUV triUV = GetTriplanarUV(parameters); float3 albedoX = tex2D(_MainTex, triUV.x).rgb; float3 albedoY = tex2D(_MainTex, triUV.y).rgb; float3 albedoZ = tex2D(_MainTex, triUV.z).rgb; surface.albedo = (albedoX + albedoY + albedoZ) / 3; … } 对三个映射取平均值 2.3 基于法线的混合 我们现在总是得到最好的投影，但也得到了另外两个。我们不能只使用最好的那个，因为在“最好”突然发生变化的地方会出现接缝。但我们可以做的是在它们之间进行平滑混合。 首选的映射是与表面方向最一致的映射，这由表面法线指示。因此，我们可以用法线来定义所有三个投影的权重。我们必须使用法线向量的绝对值，因为表面可能面向负方向。此外，权重的总和必须为 1，因此我们必须通过除以它们的总和来对它们进行归一化。创建一个新函数来计算这些权重。 float3 GetTriplanarWeights (SurfaceParameters parameters) { float3 triW = abs(parameters.normal); return triW / (triW.x + triW.y + triW.z); } 现在我们可以通过权重来调制每个映射的贡献。 void MyTriPlanarSurfaceFunction ( inout SurfaceData surface, SurfaceParameters parameters ) { … float3 triW = GetTriplanarWeights(parameters); surface.albedo = albedoX * triW.x + albedoY * triW.y + albedoZ * triW.z; … } 混合三个映射 2.4 镜像映射 现在最可能的投影是最强的。在轴对齐的表面上，我们最终只能看到单个映射。轴对齐的立方体在所有面上看起来都不错，除了其中一半最终使用了镜像映射。 纹理在另一面被镜像了 当纹理被镜像时，并不总是一个问题，但当使用上面带有数字的测试纹理时就很明显了。因此，让我们确保纹理永远不会被镜像。我们通过在适当的时候取负 U 坐标来实现。在 X 映射的情况下，即 normal.x 为负时。同样，对于 Y 投影，当 normal.y 为负时。对于 Z 则相反。 TriplanarUV GetTriplanarUV (SurfaceParameters parameters) { TriplanarUV triUV; float3 p = parameters.position; triUV.x = p.zy; triUV.y = p.xz; triUV.z = p.xy; if (parameters.normal.x &lt; 0) { triUV.x.x = -triUV.x.x; } if (parameters.normal.y &lt; 0) { triUV.y.x = -triUV.y.x; } if (parameters.normal.z &gt;= 0) { triUV.z.x = -triUV.z.x; } return triUV; } 不再镜像 请注意，这会在每个映射维度为零的地方产生接缝，但这没关系，因为它们的权重在那里也是零。 2.5 偏移映射 因为我们是在表面上投影三次相同的纹理，最终可能会出现明显的重复。这在球体上可能非常明显。你可以移动它，直到出现如下图所示的纹理对齐。从左到右，你可以看到序列 44, 45, 40, 44, 45, 40，即使完整序列是 40–45。在它下面你可以看到 34, 35, 30, 34, 35, 30。垂直方向你可以看到 44 和 45 重复。 对齐的映射 我们可以通过偏移投影来消除此类重复。如果我们垂直移动 X 映射 1/2，那么我们就在 X 和 Z 之间消除了它们。同样，如果我们水平移动 X 映射 1/2，那么就在 Y 和 Z 之间消除了它们。X 和 Y 映射不对齐，所以我们不必担心它们。 TriplanarUV GetTriplanarUV (SurfaceParameters parameters) { … triUV.x.y += 0.5; triUV.z.x += 0.5; return triUV; } 偏移映射 我们使用 1/2 作为偏移量，因为那是最大值。在我们的测试纹理的情况下，它打破了数字序列，但保持了块对齐。如果我们使用的纹理具有三个而不是六个明显的条带，偏移 1/3 效果会更好。通常，三平面映射是针对地形纹理完成的，对于这些纹理，你不需要担心精确对齐。 3. 其他表面属性 除了反照率，还有更多的表面属性可以存储在映射中。例如，对于我们的电路材料，我们还有金属度、遮蔽、光滑度和法线贴图。我们也来支持这些。 仅使用电路反照率贴图 3.1 MOS 贴图 使用三平面映射时，我们使用三个不同的投影对贴图进行采样。这使着色器中的纹理采样量增加了三倍。为了保持可控性，我们应该旨在最小化每个投影的采样量。我们可以通过在一个贴图中存储多个表面属性来实现。我们的电路材料已经有这样一个贴图，在 R 通道存储金属度，在 G 通道存储遮蔽，在 A 通道存储光滑度。所以这就是一个金属度-遮蔽-光滑度贴图，简称 MOS 贴图。我们将在三平面着色器中依赖这样的 MOS 贴图，因此添加一个属性。 Properties { _MainTex (\"Albedo\", 2D) = \"white\" {} [NoTilingOffset] _MOSMap (\"MOS\", 2D) = \"white\" {} } 带有电路 MOS 贴图的材质 为此贴图添加一个变量——因为它在 My Lighting Input 中没有定义——然后像反照率贴图一样对其采样三次。 sampler2D _MOSMap; … void MyTriPlanarSurfaceFunction ( inout SurfaceData surface, SurfaceParameters parameters ) { TriplanarUV triUV = GetTriplanarUV(parameters); float3 albedoX = tex2D(_MainTex, triUV.x).rgb; float3 albedoY = tex2D(_MainTex, triUV.y).rgb; float3 albedoZ = tex2D(_MainTex, triUV.z).rgb; float4 mosX = tex2D(_MOSMap, triUV.x); float4 mosY = tex2D(_MOSMap, triUV.y); float4 mosZ = tex2D(_MOSMap, triUV.z); … } 使用三平面权重混合 MOS 数据，然后使用结果设置表面。 surface.albedo = albedoX * triW.x + albedoY * triW.y + albedoZ * triW.z; float4 mos = mosX * triW.x + mosY * triW.y + mosZ * triW.z; surface.metallic = mos.x; surface.occlusion = mos.y; surface.smoothness = mos.a; 使用电路 MOS 贴图 3.2 法线贴图 也添加对法线贴图的支持。我们不能将其打包在另一个贴图中，因此它需要自己的属性。 Properties { _MainTex (\"Albedo\", 2D) = \"white\" {} [NoTilingOffset] _MOSMap (\"MOS\", 2D) = \"white\" {} [NoTilingOffset] _NormalMap (\"Normals\", 2D) = \"white\" {} } 带有电路法线贴图的材质 采样该贴图三次，并为每个轴解包法线。 void MyTriPlanarSurfaceFunction ( inout SurfaceData surface, SurfaceParameters parameters ) { … float4 mosX = tex2D(_MOSMap, triUV.x); float4 mosY = tex2D(_MOSMap, triUV.y); float4 mosZ = tex2D(_MOSMap, triUV.z); float3 tangentNormalX = UnpackNormal(tex2D(_NormalMap, triUV.x)); float3 tangentNormalY = UnpackNormal(tex2D(_NormalMap, triUV.y)); float3 tangentNormalZ = UnpackNormal(tex2D(_NormalMap, triUV.z)); … } 我们可以像混合其他数据一样混合法线，只需将其归一化即可。但是，这仅对世界空间法线有效，而我们采样的是切线空间法线。让我们首先假设可以直接将它们用作世界空间法线，看看会发生什么。为了更明显，再次将法线用于反照率。 float3 tangentNormalX = UnpackNormal(tex2D(_NormalMap, triUV.x)); float3 tangentNormalY = UnpackNormal(tex2D(_NormalMap, triUV.y)); float3 tangentNormalZ = UnpackNormal(tex2D(_NormalMap, triUV.z)); float3 worldNormalX = tangentNormalX; float3 worldNormalY = tangentNormalY; float3 worldNormalZ = tangentNormalZ; float3 triW = GetTriplanarWeights(parameters); … surface.normal = normalize( worldNormalX * triW.x + worldNormalY * triW.y + worldNormalZ * triW.z ); surface.albedo = surface.normal * 0.5 + 0.5; 切线空间中的投影法线 最终的法线向量是不正确的。切线空间法线在 Z 通道中存储了它们的局部上方向——远离表面——因此结果主要是蓝色的。这与 Z 投影的 XYZ 方向匹配，但不匹配其他两个。 在 Y 投影的情况下，上方向对应于 Y，而不是 Z。所以我们必须交换 Y 和 Z 以将切线空间转换为世界空间。同样，我们必须为 X 投影交换 X 和 Z。 float3 tangentNormalX = UnpackNormal(tex2D(_NormalMap, triUV.x)); float3 tangentNormalY = UnpackNormal(tex2D(_NormalMap, triUV.y)); float3 tangentNormalZ = UnpackNormal(tex2D(_NormalMap, triUV.z)); float3 worldNormalX = tangentNormalX.zyx; float3 worldNormalY = tangentNormalY.xzy; float3 worldNormalZ = tangentNormalZ; 世界空间中的投影法线 因为我们取负了 X 坐标以防止镜像，所以我们也必须对切线空间法线向量执行此操作。否则它们仍然会被镜像。 float3 tangentNormalX = UnpackNormal(tex2D(_NormalMap, triUV.x)); float3 tangentNormalY = UnpackNormal(tex2D(_NormalMap, triUV.y)); float3 tangentNormalZ = UnpackNormal(tex2D(_NormalMap, triUV.z)); if (parameters.normal.x &lt; 0) { tangentNormalX.x = -tangentNormalX.x; } if (parameters.normal.y &lt; 0) { tangentNormalY.x = -tangentNormalY.x; } if (parameters.normal.z &gt;= 0) { tangentNormalZ.x = -tangentNormalZ.x; } float3 worldNormalX = tangentNormalX.zyx; float3 worldNormalY = tangentNormalY.xzy; float3 worldNormalZ = tangentNormalZ; 在这些情况下，我们还必须翻转法线的上方向，因为它们指向内部。 if (parameters.normal.x &lt; 0) { tangentNormalX.x = -tangentNormalX.x; tangentNormalX.z = -tangentNormalX.z; } if (parameters.normal.y &lt; 0) { tangentNormalY.x = -tangentNormalY.x; tangentNormalY.z = -tangentNormalY.z; } if (parameters.normal.z &gt;= 0) { tangentNormalZ.x = -tangentNormalZ.x; } else { tangentNormalZ.z = -tangentNormalZ.z; } 取消镜像并翻转的法线 3.3 与表面法线混合 虽然法线向量现在已与它们的投影正确对齐，但它们与实际表面法线没有关系。例如，球体最终得到的法线就像立方体一样。这并不直接明显，因为我们根据实际表面法线平滑地混合这些法线，但当我们调整混合时，情况会变得更糟。 通常，我们会依赖切线到世界变换矩阵来使法线适合几何体表面。但对于我们的三个投影，我们没有这样的矩阵。我们可以做的替代方案是在每个投影法线和表面法线之间进行混合，使用 whiteout blending。我们可以使用 BlendNormals 函数，但它也会对结果进行归一化。这有点过头了，考虑到我们混合了三个结果，然后再次对该结果进行归一化。所以让我们制作我们自己的变体，不进行每次投影的归一化。 float3 BlendTriplanarNormal (float3 mappedNormal, float3 surfaceNormal) { float3 n; n.xy = mappedNormal.xy + surfaceNormal.xy; n.z = mappedNormal.z * surfaceNormal.z; return n; } whiteout blending 是如何工作的？ 在《Rendering 6, Bumpiness》中有描述。 Whiteout blending 假设 Z 指向上方。因此将表面法线转换为投影空间，在该切线空间中执行混合，然后将结果转换回世界空间。 float3 worldNormalX = BlendTriplanarNormal(tangentNormalX, parameters.normal.zyx).zyx; float3 worldNormalY = BlendTriplanarNormal(tangentNormalY, parameters.normal.xzy).xzy; float3 worldNormalZ = BlendTriplanarNormal(tangentNormalZ, parameters.normal); 不正确的法线混合 对于面向负方向的表面，这会出错，因为那样我们最终会乘以两个负 Z 值，从而翻转最终 Z 的符号。我们可以通过使用其中一个 Z 值的绝对值来解决此问题。但这相当于一开始就不取负采样的 Z 组件，因此我们可以直接移除那段代码。 if (parameters.normal.x &lt; 0) { tangentNormalX.x = -tangentNormalX.x; // tangentNormalX.z = -tangentNormalX.z; } if (parameters.normal.y &lt; 0) { tangentNormalY.x = -tangentNormalY.x; // tangentNormalY.z = -tangentNormalY.z; } if (parameters.normal.z &gt;= 0) { tangentNormalZ.x = -tangentNormalZ.x; } // else { // tangentNormalZ.z = -tangentNormalZ.z; // } 正确的法线混合 生成的法线向量现在偏向于原始表面法线。虽然这并不完美，但通常足够了。你可以更进一步，完全放弃采样的 Z 组件，仅使用原始 Z 组件。这被称为 UDN 混合，在使用 DXT5nm 压缩时更便宜，因为不需要重建 Z 组件，但它会降低非对齐表面的法线强度。 法线贴图功能正常后，恢复原始反照率，这样我们就可以看到完整的电路材料。 // surface.albedo = surface.normal * 0.5 + 0.5; 使用所有电路贴图 3.4 缩放贴图 最后，让我们能够缩放贴图。通常，这是通过单个纹理的平铺和偏移值完成的，但这对于三平面映射没有太大意义。偏移量不太有用，非均匀缩放也是。因此，改用单个缩放属性。 Properties { [NoScaleOffset] _MainTex (\"Albedo\", 2D) = \"white\" {} [NoScaleOffset] _MOSMap (\"MOS\", 2D) = \"white\" {} [NoScaleOffset] _NormalMap (\"Normals\", 2D) = \"white\" {} _MapScale (\"Map Scale\", Float) = 1 } 带有贴图缩放的材质 添加所需的贴图缩放变量，并在确定 UV 坐标时使用它来缩放位置。 float _MapScale; struct TriplanarUV { float2 x, y, z; }; TriplanarUV GetTriplanarUV (SurfaceParameters parameters) { TriplanarUV triUV; float3 p = parameters.position * _MapScale; … } 使用双倍贴图缩放 4. 调整混合权重 最终的表面数据是通过使用原始表面法线在三个映射之间进行混合找到的。到目前为止，我们一直直接用法线，仅取其绝对值并对结果进行归一化，使权重的总和为 1。这是最直接的方法，但也可以通过各种方式微调权重。 4.1 混合偏移 更改权重计算的第一种方法是引入偏移（offset）。如果我们从所有权重中减去相同的量，那么较小的权重比大的权重受到的影响更大，这会改变它们的相对重要性。它们甚至可能变成负值。添加一个混合偏移属性来实现这一点。 我们必须确保并非所有权重都变成负值，因此最大偏移量应小于最大可能的最小权重，即当法线向量的所有三个分量相等时。那是 $\\sqrt{1/3}$，约为 0.577，但我们就用 0.5 作为最大值，0.25 作为默认值。 _MapScale (\"Map Scale\", Float) = 1 _BlendOffset (\"Blend Offset\", Range(0, 0.5)) = 0.25 带有混合偏移的材质 在对权重进行归一化之前减去偏移量，看看效果如何。 float _BlendOffset; … float3 GetTriplanarWeights (SurfaceParameters parameters) { float3 triW = abs(parameters.normal); triW = triW - _BlendOffset; return triW / (triW.x + triW.y + triW.z); } 不正确的偏移用法 当混合权重保持正值时看起来不错，但负权重最终会从最终数据中减去。为了防止这种情况，在归一化之前对权重进行饱和处理（clamp）。 triW = saturate(triW - _BlendOffset); 结果是偏移量越高，混合区域越小。为了更清楚地看到混合如何变化，将权重用于反照率。 void MyTriPlanarSurfaceFunction ( inout SurfaceData surface, SurfaceParameters parameters ) { … surface.albedo = triW; } 调整偏移量 4.2 混合指数 另一种缩小混合区域的方法是通过幂运算（exponentiation），在归一化之前将权重提高到大于 1 的某个幂次。这类似于偏移，但是非线性的。为此添加一个着色器属性，使用任意最大值 8 和默认值 2。 _BlendOffset (\"Blend Offset\", Range(0, 0.5)) = 0.25 _BlendExponent (\"Blend Exponent\", Range(1, 8)) = 2 带有混合指数的材质 使用 pow 函数在偏移后应用指数。 float _BlendOffset, _BlendExponent; … float3 GetTriplanarWeights (SurfaceParameters parameters) { float3 triW = abs(parameters.normal); triW = saturate(triW - _BlendOffset); triW = pow(triW, _BlendExponent); return triW / (triW.x + triW.y + triW.z); } 调整指数 你最终可能会同时使用这两种方法来调整混合权重。如果你定下最终指数为 2、4 或 8，则可以硬编码几次乘法而不是依赖 pow。 4.3 基于高度的混合 除了依靠原始表面法线，我们还可以让表面数据影响混合。如果表面数据包含高度，那么可以将其计入权重。我们的 MOS 贴图仍然有一个未使用的通道，因此可以将它们转换为 MOHS 贴图，包含金属度、遮蔽、高度和光滑度数据。这里是电路材料的这样一张图。它与 MOS 图相同，但在蓝色通道中包含高度数据。 电路 MOHS 贴图 将我们的 MOS 属性重命名为 MOHS 并分配新纹理。确保其 sRGB 导入复选框已禁用。 [NoScaleOffset] _MainTex (\"Albedo\", 2D) = \"white\" {} [NoScaleOffset] _MOHSMap (\"MOHS\", 2D) = \"white\" {} [NoScaleOffset] _NormalMap (\"Normals\", 2D) = \"white\" {} 现在带有 MOHS 贴图的材质 同时也重命名变量。 sampler2D _MOHSMap; … void MyTriPlanarSurfaceFunction ( inout SurfaceData surface, SurfaceParameters parameters ) { … float4 mohsX = tex2D(_MOHSMap, triUV.x); float4 mohsY = tex2D(_MOHSMap, triUV.y); float4 mohsZ = tex2D(_MOHSMap, triUV.z); … float4 mohs = mohsX * triW.x + mohsY * triW.y + mohsZ * triW.z; surface.metallic = mohs.x; surface.occlusion = mohs.y; surface.smoothness = mohs.a; … } 给 GetTriplanarWeights 添加三个高度值的参数。让我们首先尝试直接使用高度，在幂运算之前替换法线向量。 float3 GetTriplanarWeights ( SurfaceParameters parameters, float heightX, float heightY, float heightZ ) { float3 triW = abs(parameters.normal); triW = saturate(triW - _BlendOffset); triW = float3(heightX, heightY, heightZ); triW = pow(triW, _BlendExponent); return triW / (triW.x + triW.y + triW.z); } 然后在调用函数时添加高度作为参数。 float3 triW = GetTriplanarWeights(parameters, mohsX.z, mohsY.z, mohsZ.z); 仅基于高度的混合 仅使用高度并不能给我们有用的结果，但可以清楚地看到金色电路条最高，因此主导了混合。现在将高度与它们各自的权重相乘。 triW *= float3(heightX, heightY, heightZ); 与高度相乘 这看起来好多了，但高度的影响仍然非常强。调节这一点很有用，因此在着色器中添加“混合高度强度（Blend Height Strength）”属性。在最大强度下，它可能会完全消除某些权重，这不应该发生。因此将强度的范围限制在 0–0.99，默认值为 0.5。 _BlendOffset (\"Blend Offset\", Range(0, 0.5)) = 0.25 _BlendExponent (\"Blend Exponent\", Range(1, 8)) = 2 _BlendHeightStrength (\"Blend Height Strength\", Range(0, 0.99)) = 0.5 带有混合高度强度的材质 通过在 1 和高度之间进行插值来应用强度，使用强度作为插值因子。然后将权重与该结果相乘。 float _BlendOffset, _BlendExponent, _BlendHeightStrength; … float3 GetTriplanarWeights ( SurfaceParameters parameters, float heightX, float heightY, float heightZ ) { float3 triW = abs(parameters.normal); triW = saturate(triW - _BlendOffset); triW *= lerp(1, float3(heightX, heightY, heightZ), _BlendHeightStrength); triW = pow(triW, _BlendExponent); return triW / (triW.x + triW.y + triW.z); } 使用高度与偏移量结合效果最好，可以限制它们的影响范围。除此之外，更高的指数会使效果更明显。 调整高度强度 最后，恢复反照率以查看完整材质上的混合设置效果。 // surface.albedo = triW; 所有混合设置在最小 vs 最大时的对比 5. 自定义着色器 GUI 我们没有使用为其他着色器创建的着色器 GUI 类，因为它不适用于我们的三平面着色器。它依赖于我们的三平面着色器所没有的属性。虽然我们可以让 MyLightingShaderGUI 也支持这个着色器，但最好还是保持简单并创建一个新类。 5.1 基类 与其复制我们可以重用的 MyLightingShaderGUI 的基本功能，不如创建一个共同的基类供两个 GUI 扩展。让我们将其命名为 MyBaseShaderGUI。将 MyLightingShaderGUI 中的所有通用代码放入其中，省略其余部分。让所有应该直接提供给其子类的内容都设为 protected。这允许类本身及其子类访问，但其他地方不能访问。 using UnityEngine; //using UnityEngine.Rendering; using UnityEditor; public class MyBaseShaderGUI : ShaderGUI { static GUIContent staticLabel = new GUIContent(); protected Material target; protected MaterialEditor editor; MaterialProperty[] properties; public override void OnGUI ( MaterialEditor editor, MaterialProperty[] properties ) { this.target = editor.target as Material; this.editor = editor; this.properties = properties; // DoRenderingMode(); // … // DoAdvanced(); } protected MaterialProperty FindProperty (string name) { return FindProperty(name, properties); } protected static GUIContent MakeLabel (string text, string tooltip = null) { … } protected static GUIContent MakeLabel ( MaterialProperty property, string tooltip = null ) { … } protected void SetKeyword (string keyword, bool state) { … } protected bool IsKeywordEnabled (string keyword) { return target.IsKeywordEnabled(keyword); } protected void RecordAction (string label) { editor.RegisterPropertyChangeUndo(label); } } 让 MyLightingShaderGUI 继承 MyBaseShaderGUI 而不是直接继承 ShaderGUI。然后从中删除现在属于其基类一部分的所有代码。不再在 OnGUI 中自己设置变量，而是通过调用 base.OnGUI 将其委托给基类的 OnGUI 方法。 public class MyLightingShaderGUI : MyBaseShaderGUI { … public override void OnGUI ( MaterialEditor editor, MaterialProperty[] properties ) { // this.target = editor.target as Material; // this.editor = editor; // this.properties = properties; base.OnGUI(editor, properties); DoRenderingMode(); … DoAdvanced(); } … } 5.2 三平面着色器 GUI 添加一个新的 MyTriplanarShaderGUI 类来创建我们三平面着色器的 GUI。让它继承 MyBaseShaderGUI。给它一个 OnGUI 方法，在其中调用 base.OnGUI，然后显示贴图缩放属性。使用独立的方法来处理贴图、混合和其他设置。 using UnityEngine; using UnityEditor; public class MyTriplanarShaderGUI : MyBaseShaderGUI { public override void OnGUI ( MaterialEditor editor, MaterialProperty[] properties ) { base.OnGUI(editor, properties); editor.ShaderProperty(FindProperty(\"_MapScale\"), MakeLabel(\"Map Scale\")); DoMaps(); DoBlending(); DoOtherSettings(); } void DoMaps () {} void DoBlending () {} void DoOtherSettings () {} } 声明此类为我们三平面着色器的自定义编辑器。 Shader \"Custom/Triplanar Mapping\" { … CustomEditor \"MyTriplanarShaderGUI\" } 仅显示贴图缩放 5.3 贴图 为贴图部分创建一个标签，然后在单行上显示三个纹理属性。为 MOHS 贴图提供提示工具，以解释每个通道应包含的内容。 void DoMaps () { GUILayout.Label(\"Maps\", EditorStyles.boldLabel); editor.TexturePropertySingleLine( MakeLabel(\"Albedo\"), FindProperty(\"_MainTex\") ); editor.TexturePropertySingleLine( MakeLabel( \"MOHS\", \"Metallic (R) Occlusion (G) Height (B) Smoothness (A)\" ), FindProperty(\"_MOHSMap\") ); editor.TexturePropertySingleLine( MakeLabel(\"Normals\"), FindProperty(\"_NormalMap\") ); } 贴图 GUI 5.4 混合 混合部分很简单，只需一个标签和三个属性。 void DoBlending () { GUILayout.Label(\"Blending\", EditorStyles.boldLabel); editor.ShaderProperty(FindProperty(\"_BlendOffset\"), MakeLabel(\"Offset\")); editor.ShaderProperty( FindProperty(\"_BlendExponent\"), MakeLabel(\"Exponent\") ); editor.ShaderProperty( FindProperty(\"_BlendHeightStrength\"), MakeLabel(\"Height Strength\") ); } 混合 GUI 5.5 其他设置 对于其他设置，通过调用 MaterialEditor.RenderQueueField 允许自定义渲染队列。同时也能够切换 GPU 实例化。 void DoOtherSettings () { GUILayout.Label(\"Other Settings\", EditorStyles.boldLabel); editor.RenderQueueField(); editor.EnableInstancingField(); } 其他设置 GUI 6. 独立的顶部贴图 通常，你不希望外观完全统一。最明显的例子是地形，其中水平表面——那些指向上的，而不是向下的——可以是草地，而所有其他表面可以是岩石。你甚至可能想将三平面映射与纹理溅射（texture splatting）相结合，但那很昂贵，因为需要更多的纹理采样。替代方法是依靠贴花（decals）、其他细节对象或顶点颜色来增加多样性。 6.1 更多贴图 为了支持独立的顶部贴图，我们需要添加三个替代贴图属性。 [NoScaleOffset] _MainTex (\"Albedo\", 2D) = \"white\" {} [NoScaleOffset] _MOHSMap (\"MOHS\", 2D) = \"white\" {} [NoScaleOffset] _NormalMap (\"Normals\", 2D) = \"white\" {} [NoScaleOffset] _TopMainTex (\"Top Albedo\", 2D) = \"white\" {} [NoScaleOffset] _TopMOHSMap (\"Top MOHS\", 2D) = \"white\" {} [NoScaleOffset] _TopNormalMap (\"Top Normals\", 2D) = \"white\" {} 并非总是需要独立的顶部贴图，因此我们将其设为一个着色器特性，使用 _SEPARATE_TOP_MAPS 关键字。除了阴影通道外，为所有通道添加对它的支持。 #pragma target 3.0 #pragma shader_feature _SEPARATE_TOP_MAPS 将这些额外的贴图添加到我们的着色器 GUI 中。使用顶部反照率贴图来确定是否应设置该关键字。 void DoMaps () { GUILayout.Label(\"Top Maps\", EditorStyles.boldLabel); MaterialProperty topAlbedo = FindProperty(\"_TopMainTex\"); Texture topTexture = topAlbedo.textureValue; EditorGUI.BeginChangeCheck(); editor.TexturePropertySingleLine(MakeLabel(\"Albedo\"), topAlbedo); if (EditorGUI.EndChangeCheck() &amp;&amp; topTexture != topAlbedo.textureValue) { SetKeyword(\"_SEPARATE_TOP_MAPS\", topAlbedo.textureValue); } editor.TexturePropertySingleLine( MakeLabel( \"MOHS\", \"Metallic (R) Occlusion (G) Height (B) Smoothness (A)\" ), FindProperty(\"_TopMOHSMap\") ); editor.TexturePropertySingleLine( MakeLabel(\"Normals\"), FindProperty(\"_TopNormalMap\") ); GUILayout.Label(\"Maps\", EditorStyles.boldLabel); … } 6.2 使用大理石 要查看实际运行中的独立顶部贴图，我们需要另一组纹理。我们可以使用大理石反照率和法线贴图。这里是配套的 MOHS 贴图。 大理石 MOHS 贴图 顶部使用电路——因为它是绿色的，所以有点像草——其余部分使用大理石。 顶部为电路，其余为大理石 由于着色器还不了解顶部贴图，我们目前只能看到大理石。 仅显示大理石 6.3 启用顶部贴图 向 MyTriplanarMapping 添加所需的采样器变量。在所有纹理采样后，检查关键字是否在 MyTriPlanarSurfaceFunction 中定义。如果是，添加代码以用顶部贴图的采样覆盖 Y 投影的数据。但仅对向上的表面执行此操作，即表面法线具有正 Y 分量时。 sampler2D _TopMainTex, _TopMOHSMap, _TopNormalMap; … void MyTriPlanarSurfaceFunction ( inout SurfaceData surface, SurfaceParameters parameters ) { … float3 tangentNormalZ = UnpackNormal(tex2D(_NormalMap, triUV.z)); #if defined(_SEPARATE_TOP_MAPS) if (parameters.normal.y &gt; 0) { albedoY = tex2D(_TopMainTex, triUV.y).rgb; mohsY = tex2D(_TopMOHSMap, triUV.y); tangentNormalY = UnpackNormal(tex2D(_TopNormalMap, triUV.y)); } #endif } 如果所有表面都指向上的话？ 在典型的基于高度图的地形网格的情况下，所有表面法线都保证指向向上。因此，不需要检查法线的 Y 分量是否为正，可以省略。 这导致着色器会根据 Y 投影对常规贴图或顶部贴图进行采样。在我们的案例中，我们在大理石顶部得到了一个电路层。通常它是草、沙或雪。 顶部电路 默认混合设置在投影之间产生相当平滑的混合，在电路和大理石交汇的地方看起来不太好。指数设为 8 会导致更突然的过渡，这更适合这些材料。也可以为顶部贴图支持不同的混合设置，但高度混合已经允许通过 MOHS 贴图进行大量控制。 指数设为 8 6.4 稍后解包 虽然着色器编译器使用 if-else 方法聪明地采样顶部或常规贴图，但它在解包法线方面不够聪明。它不能假设 UnpackNormal 的两次使用可以合并。为了帮助编译器，我们可以推迟解包原始法线，直到选择贴图之后。 float3 tangentNormalX = UnpackNormal(tex2D(_NormalMap, triUV.x)); // float3 tangentNormalY = UnpackNormal(tex2D(_NormalMap, triUV.y)); float4 rawNormalY = tex2D(_NormalMap, triUV.y); float3 tangentNormalZ = UnpackNormal(tex2D(_NormalMap, triUV.z)); #if defined(_SEPARATE_TOP_MAPS) if (parameters.normal.y &gt; 0) { albedoY = tex2D(_TopMainTex, triUV.y).rgb; mohsY = tex2D(_TopMOHSMap, triUV.y); // tangentNormalY = UnpackNormal(tex2D(_TopNormalMap, triUV.y)); rawNormalY = tex2D(_TopNormalMap, triUV.y); } #endif float3 tangentNormalY = UnpackNormal(rawNormalY); 7. 光照贴图 (Lightmapping) 我们的三平面着色器还没有完成，因为它还不支持光照贴图。它可以接收烘培光照，但它并不对其做出贡献。通过将所有对象设为静态并将方向光切换到烘培模式最容易看到这一点。等待烘培完成，然后通过将场景视图模式从“着色（Shaded）”切换到“烘培全局光照 / 反照率（Baked Global Illumination / Albedo）”来检查烘培的反照率。所有使用三平面映射的对象都变成了黑色。 光照贴图使用黑色反照率 要支持光照贴图，我们必须向着色器添加一个 meta 通道，它必须依赖 My Lightmapping 而不是 My Lighting。 Pass { Tags { \"LightMode\" = \"Meta\" } Cull Off CGPROGRAM #pragma vertex MyLightmappingVertexProgram #pragma fragment MyLightmappingFragmentProgram #pragma shader_feature _SEPARATE_TOP_MAPS #include \"MyTriplanarMapping.cginc\" #include \"My Lightmapping.cginc\" ENDCG } 7.1 使用表面数据 为了让 My Lightmapping 配合我们的三平面方法工作，它也必须支持新的表面方法。为了方便，让它包含 My Lighting Input 并删除所有现在重复的变量、插值器和 getter 函数。 //#include \"UnityPBSLighting.cginc\" #include \"My Lighting Input.cginc\" #include \"UnityMetaPass.cginc\" //float4 _Color; //… // //float3 GetEmission (Interpolators i) { // … //} Interpolators MyLightmappingVertexProgram (VertexData v) { … } float4 MyLightmappingFragmentProgram (Interpolators i) : SV_TARGET { … } 像 My Lighting 一样，它必须定义默认的反照率函数。并且它应该在 MyLightmappingFragmentProgram 中使用相同的表面方法，除了它只关心反照率、发射、金属度和光滑度。 #if !defined(ALBEDO_FUNCTION) #define ALBEDO_FUNCTION GetAlbedo #endif float4 MyLightmappingFragmentProgram (Interpolators i) : SV_TARGET { SurfaceData surface; surface.normal = normalize(i.normal); surface.albedo = 1; surface.alpha = 1; surface.emission = 0; surface.metallic = 0; surface.occlusion = 1; surface.smoothness = 0.5; #if defined(SURFACE_FUNCTION) SurfaceParameters sp; sp.normal = i.normal; sp.position = i.worldPos.xyz; sp.uv = UV_FUNCTION(i); SURFACE_FUNCTION(surface, sp); #else surface.albedo = ALBEDO_FUNCTION(i); surface.emission = GetEmission(i); surface.metallic = GetMetallic(i); surface.smoothness = GetSmoothness(i); #endif … } 用新的表面数据替换 getter 函数的旧用法。 float4 MyLightmappingFragmentProgram (Interpolators i) : SV_TARGET { … UnityMetaInput surfaceData; surfaceData.Emission = surface.emission; float oneMinusReflectivity; surfaceData.Albedo = DiffuseAndSpecularFromMetallic( surface.albedo, surface.metallic, surfaceData.SpecularColor, oneMinusReflectivity ); float roughness = SmoothnessToRoughness(surface.smoothness) * 0.5; surfaceData.Albedo += surfaceData.SpecularColor * roughness; return UnityMetaFragment(surfaceData); } 7.2 包含相关输入 插值器现在还包括法线和世界位置向量，因此应该在 MyLightMappingVertexProgram 中设置它们。 Interpolators MyLightmappingVertexProgram (VertexData v) { Interpolators i; i.pos = UnityMetaVertexPosition( v.vertex, v.uv1, v.uv2, unity_LightmapST, unity_DynamicLightmapST ); i.normal = UnityObjectToWorldNormal(v.normal); i.worldPos.xyz = mul(unity_ObjectToWorld, v.vertex); i.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); return i; } 这些向量通常不需要，所以我们可以在不需要时跳过计算它们，只需改用虚拟常量。我们可以定义两个宏 META_PASS_NEEDS_NORMALS 和 META_PASS_NEEDS_POSITION 来指示是否需要它们。 #if defined(META_PASS_NEEDS_NORMALS) i.normal = UnityObjectToWorldNormal(v.normal); #else i.normal = float3(0, 1, 0); #endif #if defined(META_PASS_NEEDS_POSITION) i.worldPos.xyz = mul(unity_ObjectToWorld, v.vertex); #else i.worldPos.xyz = 0; #endif 此外，仅在需要时包含 UV 坐标。 #if !defined(NO_DEFAULT_UV) i.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); #endif 7.3 三平面光照贴图 剩下要做的就是声明我们的三平面着色器在其 meta 通道中同时需要法线和位置数据。一旦完成并且光照再次烘培，反照率将正确显示在场景视图中。 #pragma shader_feature _SEPARATE_TOP_MAPS #define META_PASS_NEEDS_NORMALS #define META_PASS_NEEDS_POSITION #include \"MyTriplanarMapping.cginc\" #include \"My Lightmapping.cginc\" 正确光照贴图的反照率 现在我们的三平面着色器已完全正常工作。你可以将其作为你自己工作的基础，根据需要进行扩展、微调和调整。 光照贴图数据似乎不依赖于世界空间？ 确实，当进行光照贴图时，我们最终使用的是对象空间而不是世界空间。发生这种情况是因为 Unity 没有为 meta 通道设置对象到世界的变换矩阵。其结果是 meta 通道仅对位于原点且没有旋转或缩放调整的对象正确工作。因此它适用于典型的地形，但不适用于其他事物。对于其他对象，它仍然是可行的，只要材料大多是统一的并且顶部正确对齐（如果使用了独立贴图）。" }, { "title": "FXAA像素平滑(翻译二十六)", "url": "/posts/unity-fxaa/", "categories": "Unity3D, Shader", "tags": "Shader, Anti-aliasing, FXAA, Post Processing", "date": "2018-01-31 00:00:00 +0800", "content": "计算图像亮度（Luminance）。 寻找高对比度像素。 识别对比度边缘。 选择性混合。 搜索边缘端点。 1 搭建场景 显示器的分辨率是有限的。因此，未与像素网格对齐的图像特征会产生锯齿（Aliasing）。对角线和曲线看起来像阶梯，通常被称为“锯齿（jaggies）”。细线可能会断开。比像素还小的高对比度特征有时出现，有时不出现，导致物体移动时产生闪烁，通常被称为“火花（ﬁreﬂies）”。为了减轻这些问题，开发了一系列抗锯齿技术。本教程涵盖了经典的 FXAA 解决方案。 Thin lines and their aliased rasterization 1.1 测试场景 本教程我创建了一个与景深教程类似的测试场景。它包含高对比度和低对比度区域、明亮和黑暗区域、多个直线和曲线边缘以及微小特征。我们一如既往地使用 HDR 和线性色彩空间。所有场景截图都放大了 4 倍，以便更容易区分单个像素。 Test scene, zoomed in 4×, without any anti-aliasing 1.2 超采样 (Supersampling) 消除锯齿最直接的方法是以高于显示器的分辨率进行渲染，然后对其进行降采样。这是一种空间抗锯齿方法。 Sampling at double resolution and averaging 2×2 blocks 超采样抗锯齿（SSAA）就是这样做的。场景渲染到两倍分辨率的缓冲，然后对四个像素的块求平均值以产生最终图像。这种方法可以消除锯齿，但也会稍微模糊整个图像，而且非常昂贵。 SSAA 2× 多采样抗锯齿（MSAA）是对 SSAA 的改进，它减少了填充率。但它不适用于延迟渲染等技术。 MSAA 2× and 8× MSAA 8× 1.3 后期处理 (Post E!ect) 第三种抗锯齿方法是通过后期处理 pass 完成。这些技术在最终分辨率下工作，因此无法访问实际的子像素数据，而是必须分析图像并根据这种解释选择性地模糊。 我们将创建自己的快速近似抗锯齿（fast approximate anti-aliasing，简称 FXAA）版本。它由 NVIDIA 的 Timothy Lottes 开发。我们将创建最新版本——FXAA 3.11——特别是 PC 高质量变体。 我们将为新的 FXAA shader 使用与 DepthOfField 类似的设置。 Shader \"Hidden/FXAA\" { Properties { _MainTex (\"Texture\", 2D) = \"white\" {} } CGINCLUDE #include \"UnityCG.cginc\" sampler2D _MainTex; float4 _MainTex_TexelSize; struct VertexData { float4 vertex : POSITION; float2 uv : TEXCOORD0; }; struct Interpolators { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; }; Interpolators VertexProgram (VertexData v) { Interpolators i; i.pos = UnityObjectToClipPos(v.vertex); i.uv = v.uv; return i; } ENDCG SubShader { Cull Off ZTest Always ZWrite Off Pass { // 0 blitPass CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram float4 FragmentProgram (Interpolators i) : SV_Target { return tex2D(_MainTex, i.uv); } ENDCG } } } 创建一个极简的 FXAAEffect 组件。 using UnityEngine; using System; [ExecuteInEditMode, ImageEffectAllowedInSceneView] public class FXAAEffect : MonoBehaviour { [HideInInspector] public Shader fxaaShader; [NonSerialized] Material fxaaMaterial; void OnRenderImage (RenderTexture source, RenderTexture destination) { if (fxaaMaterial == null) { fxaaMaterial = new Material(fxaaShader); fxaaMaterial.hideFlags = HideFlags.HideAndDontSave; } Graphics.Blit(source, destination, fxaaMaterial); } } 确保禁用摄像机的 MSAA，并应用效果。 HDR camera without MSAA and with FXAA 2 亮度 (Luminance) FXAA 通过选择性地降低图像对比度来工作。对比度是由像素的光强度决定的，即它们的亮度（luminance）。FXAA 实际上是在包含像素亮度的灰阶图像上工作的。 2.1 计算亮度 我们可以使用 LinearRgbToLuminance 函数计算亮度。由于我们不使用色调映射，请使用 clamp 后的颜色。 sample.rgb = LinearRgbToLuminance(saturate(sample.rgb)); Luminance 2.2 提供亮度数据 FXAA 并不自己计算亮度，数据通常必须由之前的 pass 放入 alpha 通道。我们支持三种模式：Alpha, Green, Calculate。 Luminance source, set to calculate Pass { // 0 luminancePass CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { half4 sample = tex2D(_MainTex, i.uv); sample.a = LinearRgbToLuminance(saturate(sample.rgb)); return sample; } ENDCG } 3 混合高对比度像素 FXAA 通过混合高对比度像素来工作。步骤包括：计算局部对比度、决定是否超过阈值、计算混合因子、确定边缘方向，最后进行混合。 3.1 确定相邻像素的对比度 FXAA 使用直接的水平和垂直相邻像素（NEWS 交叉）以及中间像素来确定对比度。 NESW cross plus middle pixel struct LuminanceData { float m, n, e, s, w; float highest, lowest, contrast; }; 局部对比度只是最高和最低亮度值之间的差。 Local contrast 3.2 跳过低对比度像素 我们可以设置一个对比度阈值来跳过非锯齿区域。 Contrast threshold 如果对比度低于阈值，则跳过像素（图中红色部分）。此外还有一个相对阈值（图中绿色部分）。 Red pixels are skipped Green pixels are skipped 3.3 计算混合因子 对于需要处理的像素，我们需要确定混合因子。这取决于中心像素与整个 3×3 邻域（包括对角像素）之间的对比度。 Entire neighborhood 我们使用一种低通滤波器，它类似于帐篷滤波器。计算中心像素与该平均值之间的差，得到高通滤波器。 High-pass ﬁlter 最后，通过对比度进行归一化，并使用 smoothstep 平滑结果。 Blend factor 3.4 混合方向 FXAA 会根据对比度梯度决定将中心像素与其 NEWS 邻域中的哪一个混合。 Blend directions 我们比较水平和垂直对比度。如果水平对比度大于等于垂直对比度，则为水平边缘。 Red pixels are on horizontal edges 3.5 混合 (Blending) 最终结果是通过混合因子在中心像素与其邻居之间进行线性插值。 With and without blending 你可以通过一个子像素混合因子（Subpixel Blending）来调节 FXAA 的强度。 Adjusting the amount of blending 4 沿边缘混合 由于 3×3 块只能平滑局部特征，我们需要沿长边缘搜索端点，以更好地平滑阶梯。 Edge blend comparison 4.1 边缘亮度 我们跟踪边缘梯度，并在边缘两侧寻找端点。 Edge gradients 4.2 沿边缘行走 我们沿边缘两个方向行走，直到找到对比度梯度不再匹配的端点。 Searching for the ends of an edge 在搜索过程中，我们取每步之间纹理采样的平均亮度。 Texture samples while searching 4.3 迭代搜索 由于在 Shader 中无限搜索是不可能的，我们限制搜索步数（例如 10 步）。 Edge search iterations 4.4 计算边缘混合因子 通过端点距离计算一个边缘混合因子，并将其与之前的像素混合因子（子像素混合）结合，取其中的最大值。 Final FXAA 你现在拥有了一个基本的 FXAA 实现，可以根据质量设置调整搜索步数。" }, { "title": "景深光线的弯曲(翻译二十五)", "url": "/posts/unity-depth-of-field/", "categories": "Unity3D, Shader", "tags": "Shader, Depth of Field, Post Processing", "date": "2018-01-30 10:12:34 +0800", "content": "确定弥散圆（Circle of Confusion，CoC）。 创建 Bokeh（散景）。 对图像进行聚焦和去焦。 分离和合并前景与背景。 1 搭建场景 我们感知光线是因为我们感觉到光子撞击我们的视网膜。同样，摄像机可以记录光线，是因为光子撞击其胶片或图像传感器。在所有情况下，光线都被聚焦以产生清晰的图像，但并非所有东西都能同时处于焦点上。只有特定距离的东西才是清晰的，而所有更近或更远的东西看起来都是模糊的。这种视觉效果被称为景深（Depth-of-Field）。关于失焦投影如何表现的细节被称为 Bokeh（散景），这是日语中模糊的意思。 通常，我们用自己的眼睛并不会注意到景深，因为我们关注的是焦点所在，而不是焦点之外的东西。它在照片和视频中可能更加明显，因为我们可以观察图像中不在摄像机焦点的部分。尽管这是一种物理限制，但 Bokeh 可以产生巨大的效果来引导观众的注意力。因此，它是一种艺术工具。 GPU 不需要聚焦光线，它们表现得像完美的摄像机，拥有无限的焦点。如果你想创建锐利的图像，这很棒，但如果你想将景深用于艺术目的，这就不太走运了。但是有很多方法可以伪造它。在本教程中，我们将创建一个类似于 Unity Post-processing Stack v2 中的景深效果，尽管会尽可能简化。 1.1 搭建场景 为了测试我们自己的景深效果，创建一个包含不同距离物体的小场景。我使用了一个 10×10 的平面，其电路材料平铺了五次作为地面。它为我们提供了一个具有大跨度、锐利、高频颜色变化的表面。这对于测试来说非常棒。我在上面放了一堆物体，还让四个物体漂浮在摄像机附近。 Test scene 我们将为新的 DepthOfField shader 使用与 Bloom shader 相同的设置。你可以复制它，并将其缩减为目前仅执行 blit 的单个 pass。不过，这一次我们将把 shader 放在 Hidden 菜单类别中，这会将其从 shader 下拉列表中排除。这是唯一值得注意的新东西。 Shader \"Hidden/DepthOfField\" { Properties { _MainTex (\"Texture\", 2D) = \"white\" {} } CGINCLUDE #include \"UnityCG.cginc\" sampler2D _MainTex; float4 _MainTex_TexelSize; struct VertexData { float4 vertex : POSITION; float2 uv : TEXCOORD0; }; struct Interpolators { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; }; Interpolators VertexProgram (VertexData v) { Interpolators i; i.pos = UnityObjectToClipPos(v.vertex); i.uv = v.uv; return i; } ENDCG SubShader { Cull Off ZTest Always ZWrite Off Pass { CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { return tex2D(_MainTex, i.uv); } ENDCG } } } 创建一个极简的 DepthOfFieldEffect 组件，再次使用与 bloom 效果相同的方法，但隐藏 shader 属性。 using UnityEngine; using System; [ExecuteInEditMode, ImageEffectAllowedInSceneView] public class DepthOfFieldEffect : MonoBehaviour { [HideInInspector] public Shader dofShader; [NonSerialized] Material dofMaterial; void OnRenderImage (RenderTexture source, RenderTexture destination) { if (dofMaterial == null) { dofMaterial = new Material(dofShader); dofMaterial.hideFlags = HideFlags.HideAndDontSave; } Graphics.Blit(source, destination, dofMaterial); } } 为了方便，我们不在 editor 中手动分配 shader，而是将其定义为组件的默认值。为此，在 editor 中选中脚本，并将 shader 字段挂在 inspector 顶部。 Default shader reference 将我们的新效果作为唯一的后期处理添加到摄像机。再次强调，我们假设在线性 HDR 空间中渲染，因此请相应配置项目和摄像机。此外，因为我们需要读取深度缓冲，此效果在开启 MSAA 时无法正确工作。因此请禁用摄像机的 MSAA。同时请注意，由于我们将依赖深度缓冲，该效果不会考虑透明几何体。 HDR camera without MSAA and with depth-of-ﬁeld 那我们就不能将它与透明物体一起使用了吗？ 透明物体看起来也会受到影响，但使用的是它们背后任何东西的深度信息。这是所有使用深度缓冲技术的共同局限。你仍然可以使用透明，但只有当这些物体背后有足够近的固体表面时，看起来才勉强可以接受。 2 弥散圆 (Circle of Confusion) 最简单的摄像机形式是完美的针孔摄像机。像所有摄像机一样，它有一个记录光线的图像平面。在图像平面前面有一个极小的孔——被称为光圈（Aperture）——刚好大到允许单根光线通过。摄像机前面的物体会向多个方向发射或反射光线。对于每一个点，只有单根光线能够通过孔并被记录。 Recording three points 投影图像是翻转的吗？ 的确如此。所有用摄像机记录的图像，包括你的眼睛，都是翻转的。图像在进一步处理过程中会再次翻转，所以你不需要担心。 因为每个点只捕获单根光线，所以图像总是清晰的。不幸的是，单根光线并不亮，所以产生的图像几乎看不见。你必须等待很长时间才能积累足够的光线以获得清晰的图像，这意味着这种摄像机需要很长的曝光时间。对于移动的物体会产生大量运动模糊。因此，它不是一个实用的摄像机。 为了能够缩短曝光时间，必须更快地积累光线。唯一的方法是同时记录多根光线。这可以通过增大光圈半径来实现。假设孔是圆形的，这意味着每个点都将以一锥光线而不是一根线投射到图像平面上。所以我们接收到了更多的光，但它不再落在一个点上，而是投射成一个圆盘（Disc）。覆盖多大面积取决于点、孔和图像平面之间的距离。结果是得到了一个更亮但模糊的图像。 Using a larger aperture 为了再次聚焦光线，我们必须以某种方式获取进入的光锥并将其带回一个点。这是通过在摄像机孔中放置一个透镜（Lens）来完成的。透镜以这样一种方式弯曲光线，使发散的光再次聚焦。这可以产生明亮且锐利的投影，但仅限于距离摄像机固定距离的点。距离更远的点发出的光线不会被足够聚焦，而距离太近的点发出的光线会被过度聚焦。在两种情况下，我们再次将点投射为圆盘，其大小取决于失焦程度。这种失焦投影被称为弥散圆（circle of confusion，简称 CoC）。 Only one point is in focus 2.1 可视化 CoC 弥散圆的半径是衡量点失焦程度的一个指标。让我们从可视化这个值开始。在 DepthOfFieldEffect 中添加一个常量来指示我们的第一个 pass，即 CoC pass。 const int circleOfConfusionPass = 0; … void OnRenderImage (RenderTexture source, RenderTexture destination) { … Graphics.Blit(source, destination, dofMaterial, circleOfConfusionPass); } 因为 CoC 取决于到摄像机的距离，我们需要读取深度缓冲。采样深度纹理，转换为线性深度并渲染。 CGINCLUDE #include \"UnityCG.cginc\" sampler2D _MainTex, _CameraDepthTexture; … ENDCG SubShader { … Pass { // 0 circleOfConfusionPass CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { half depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv); depth = LinearEyeDepth(depth); return depth; } ENDCG } } 2.2 选择简单的 CoC 我们对原始深度值不感兴趣，而是对 CoC 值感兴趣。为此，我们需要确定一个焦距（focus distance）。这是摄像机与焦平面之间的距离，在这个平面上一切都是完美的。添加一个公共字段。 [Range(0.1f, 100f)] public float focusDistance = 10f; CoC 的大小随着点到焦平面的距离而增大。确切的关系取决于摄像机及其配置，这可能变得相当复杂。模拟真实摄像机是可能的，但我们将使用一个简单的焦距范围（focus range），以便更容易理解和控制。我们的 CoC 将在这个范围内从零增加到最大值。 [Range(0.1f, 10f)] public float focusRange = 3f; Sliders for focus distance and range 在 blit 之前设置这些配置。 dofMaterial.SetFloat(\"_FocusDistance\", focusDistance); dofMaterial.SetFloat(\"_FocusRange\", focusRange); Graphics.Blit(source, destination, dofMaterial, circleOfConfusionPass); 添加变量到 shader。我们使用深度 $d$，焦距 $f$ 和焦距范围 $r$，可以通过 $CoC = \\frac{d - f}{r}$ 找到 CoC。 float _FocusDistance, _FocusRange; half4 FragmentProgram (Interpolators i) : SV_Target { float depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv); depth = LinearEyeDepth(depth); float coc = (depth - _FocusDistance) / _FocusRange; return coc; } Raw CoC 这会导致焦距之外的点出现正的 CoC 值，而焦距之前的点出现负的 CoC 值。值 -1 和 1 代表最大 CoC，因此我们应该通过 clamp 确保 CoC 值不超过这个范围。 float coc = (depth - _FocusDistance) / _FocusRange; coc = clamp(coc, -1, 1); return coc; 我们保留负的 CoC 值，以便区分前景和背景点。为了看到负的 CoC 值，你可以用红色着色。 Negative CoC is red 2.3 缓冲 CoC 我们需要 CoC 来缩放点的投影，但我们将在另一个 pass 中执行。所以我们将 CoC 值存储在一个临时缓冲中。因为我们只需要存储单个值，我们可以使用单通道纹理 RenderTextureFormat.RHalf。此外，此缓冲包含 CoC 数据而不是颜色值，因此它应该始终被视为线性数据。 RenderTexture coc = RenderTexture.GetTemporary( source.width, source.height, 0, RenderTextureFormat.RHalf, RenderTextureReadWrite.Linear ); Graphics.Blit(source, coc, dofMaterial, circleOfConfusionPass); Graphics.Blit(coc, destination); RenderTexture.ReleaseTemporary(coc); 3 Bokeh (散景) 虽然 CoC 决定了每个点的散景效果强度，但光圈决定了它的外观。基本上，图像是由许多光圈形状在图像平面上的投影构成的。因此，创建散景的一种方法是根据其 CoC 的大小和不透明度，使用其颜色为每个纹素（texel）渲染一个 sprite。由于大量的过度绘制，这种方法成本非常高。 另一种方法是反向工作。与其将单个片元投射到许多片元上，不如让每个片元从所有可能影响它的纹素中累积颜色。这种技术不需要生成额外的几何体，但需要进行大量的纹理采样。我们将使用这种方法。 3.1 累积散景 创建一个用于生成散景效果的新 pass。假设整个图像完全失焦，我们需要平均当前片元周围 9×9 纹素块的颜色。这总共需要 81 次采样。 Pass { // 1 bokehPass CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { half3 color = 0; for (int u = -4; u &lt;= 4; u++) { for (int v = -4; v &lt;= 4; v++) { float2 o = float2(u, v) * _MainTex_TexelSize.xy; color += tex2D(_MainTex, i.uv + o).rgb; } } color *= 1.0 / 81; return half4(color, 1); } ENDCG } Square bokeh 结果是一个更块状的图像。实际上，我们使用的是方形光圈。 3.2 圆形散景 理想的光圈是圆形的，产生由许多重叠圆盘组成的散景。我们可以通过简单地丢弃那些偏移量太大的样本，将散景形状变成直径为 4 步的圆盘。 Round bokeh 为了不限于规则网格，我们使用旋转或同心圆模式。我们将使用 Unity Post-processing Stack v2 中的采样核（Kernel）。 #define BOKEH_KERNEL_MEDIUM #if defined(BOKEH_KERNEL_SMALL) static const int kernelSampleCount = 16; … #elif defined (BOKEH_KERNEL_MEDIUM) static const int kernelSampleCount = 22; static const float2 kernel[kernelSampleCount] = { float2(0, 0), float2(0.53333336, 0), … }; #endif half4 FragmentProgram (Interpolators i) : SV_Target { half3 color = 0; for (int k = 0; k &lt; kernelSampleCount; k++) { float2 o = kernel[k]; o *= _MainTex_TexelSize.xy * 8; color += tex2D(_MainTex, i.uv + o).rgb; } color *= 1.0 / kernelSampleCount; return half4(color, 1); } Using the medium kernel 3.3 模糊散景 为了用相同数量的样本覆盖更多区域，我们可以像 bloom 效果一样，在半分辨率下创建该效果。 int width = source.width / 2; int height = source.height / 2; RenderTextureFormat format = source.format; RenderTexture dof0 = RenderTexture.GetTemporary(width, height, 0, format); RenderTexture dof1 = RenderTexture.GetTemporary(width, height, 0, format); Graphics.Blit(source, coc, dofMaterial, circleOfConfusionPass); Graphics.Blit(source, dof0); Graphics.Blit(dof0, dof1, dofMaterial, bokehPass); Graphics.Blit(dof1, destination); 为了保持散景大小一致，我们必须将采样偏移减半：o *= _MainTex_TexelSize.xy * 4;。同时，我们会在生成散景后添加一个额外的模糊 pass（postfilter pass），使用 3×3 帐篷滤波器（tent filter）。 With a tent filter 3.4 散景大小 让散景半径通过一个字段可配置，范围为 1–10，默认值为 4（以半分辨率纹素表示）。 [Range(1f, 10f)] public float bokehRadius = 4f; Conﬁgurable bokeh radius 4 聚焦 现在我们可以确定 CoC 的大小并创建最大尺寸的散景。下一步是结合这些来渲染可变的散景，模拟摄像机聚焦。 4.1 降采样 CoC 因为我们在半分辨率下创建散景，所以我们也需要半分辨率的 CoC 数据。我们必须自己进行降采样，在一个自定义的 prefilter pass 中进行。 Graphics.Blit(source, coc, dofMaterial, circleOfConfusionPass); Graphics.Blit(source, dof0, dofMaterial, preFilterPass); 在 prefilter pass 中，我们采用四个高分辨率纹素中极端的 CoC 值（绝对值最大）。 half coc0 = tex2D(_CoCTex, i.uv + o.xy).r; … half cocMin = min(min(min(coc0, coc1), coc2), coc3); half cocMax = max(max(max(coc0, coc1), coc2), coc3); half coc = cocMax &gt;= -cocMin ? cocMax : cocMin; return half4(color, coc); 4.2 使用正确的 CoC 在第一个 pass 中计算 CoC 时，将其乘以散景半径：coc = clamp(coc, -1, 1) * _BokehRadius;。 在散景 pass 中，如果样本的 CoC 至少与用于其偏移的核半径（kernel radius）一样大，那么该点的投影就会重叠该片元。 for (int k = 0; k &lt; kernelSampleCount; k++) { float2 o = kernel[k] * _BokehRadius; half radius = length(o); … half4 s = tex2D(_MainTex, i.uv + o); if (abs(s.a) &gt;= radius) { color += s.rgb; weight += 1; } } Bokeh based on CoC 4.3 平滑采样 与其完全丢弃样本，我们根据 CoC 和偏移半径分配 0–1 范围内的权重。 half Weigh (half coc, half radius) { return saturate((coc - radius + 2) / 2); } Smoothed sampling threshold 4.4 保持对焦 半分辨率渲染强制执行了最小程度的模糊。为了让焦点区域保持锐利，我们将半分辨率效果与全分辨率源图像混合。 half dofStrength = smoothstep(0.1, 1, abs(coc)); half3 color = lerp(source.rgb, dof.rgb, dofStrength); Sharp in-focus region 4.5 分离前景和背景 当失焦的前景位于对焦的背景前面时，简单的混合会产生错误。为了解决这个问题，我们需要分离前景和背景。背景样本的权重基于 max(0, min(s.a, coc))，而前景样本基于 -s.a。 Cutting out the foreground 4.6 重新合并前景和背景 我们将前景和背景数据保存在单个缓冲中。在 combine pass 中，根据 CoC 和前景权重进行插值。 half dofStrength = smoothstep(0.1, 1, abs(coc)); half3 color = lerp( source.rgb, dof.rgb, dofStrength + dof.a - dofStrength * dof.a ); Foreground edge without artifacts 4.7 降低散景强度 最后，通过在 prefilter pass 中使用加权平均值来降低散景强度，以防止整体亮度发生太大变化。加权公式为 $w = \\frac{1}{1 + \\max(r, g, b)}$。 Weighed bokeh e!ect 你现在拥有了一个简单的景深效果，大致相当于 Unity Post-processing Stack v2 中的效果。" }, { "title": "Bloom模糊光照(翻译二十四)", "url": "/posts/unity-bloom/", "categories": "Unity3D, Shader", "tags": "Shader, Bloom, Post Porcess", "date": "2018-01-29 12:03:00 +0800", "content": "渲染到临时纹理。 通过降采样和升采样进行模糊。 执行渐进式采样。 应用盒式滤波器（Box Filter）。 为图像添加 Bloom。 1 搭建场景 显示器能产生的光量是有限的。它可以从黑色变为全亮，这在 Shader 中对应于 RGB 值 0 和 1。这就是所谓的光的低动态范围（Low Dynamic Range，LDR）。全白像素的亮度因显示器而异，并且可以由用户调整，但它永远不会令人致盲。 现实生活并不局限于 LDR 光。没有最大亮度。光子同时到达得越多，物体看起来就越亮，直到看起来很痛苦甚至致盲。直视太阳会损害你的眼睛。 为了表示非常明亮的颜色，我们可以超越 LDR 进入高动态范围（High Dynamic Range，HDR）。这仅仅意味着我们不强制执行最大值 1。只要输入和输出格式可以存储大于 1 的值，Shader 就可以毫无问题地处理 HDR 颜色。但是，显示器无法超越其最大亮度，因此最终颜色仍会被限制在 LDR 内。 为了使 HDR 颜色可见，必须将它们映射到 LDR，这称为色调映射（Tonemapping）。这归结为非线性地使图像变暗，以便能够区分原始 HDR 颜色。这有点类似于我们的眼睛如何适应明亮的场景，尽管色调映射是恒定的。还有自动曝光（Auto-exposure）技术，它可以动态调整图像亮度。两者可以一起使用。但我们的眼睛并不总是能够充分适应。有些场景实在太亮了，这让我们很难看清。 我们如何在受限于 LDR 显示器的同时展示这种效果？ Bloom 是一种通过使像素颜色渗入相邻像素来弄乱图像的效果。这就像模糊图像，但是基于亮度。这样，我们可以通过模糊来传达过亮的颜色。这有点类似于光线如何在我们的眼睛内部扩散，这在高亮度的情况下会变得明显，但这主要是一种非现实的效果。 许多人不喜欢 Bloom，因为它弄乱了原本清晰的图像，并且让东西看起来不切实际地发光。这不是 Bloom 固有的错误，只是它经常被滥用。如果你追求真实感，请适度使用 Bloom，在有意义的时候使用。Bloom 也可以用于非现实效果的艺术创作。例如梦境序列，表示头晕，或用于创意场景转换。 1.1 Bloom 场景 我们将通过摄像机后期特效组件创建自己的 Bloom 效果，类似于我们在雾教程中创建延迟雾效果的方式。虽然你可以从新项目开始或继续该教程，但我使用了之前的高级渲染教程：表面置换作为该项目的基础。 创建一个具有默认照明的新场景。在里面放一堆明亮的物体，背景是黑色的。我使用了一个黑色平面和一堆大小不一的纯白色、黄色、绿色和红色立方体和球体。确保摄像机启用了 HDR。还将项目设置为使用线性色彩空间，以便我们可以最好地看到效果。 Bright objects on a black surface 通常，你会将色调映射应用于具有线性和 HDR 渲染的场景。你可以先做自动曝光，然后应用 Bloom，最后执行最终的色调映射。但在本教程中，我们将专注于 Bloom，并且不会应用任何其他效果。这意味着所有最终超过 LDR 的颜色都将在最终图像中被截断。 1.2 Bloom 效果 创建一个新的 BloomEffect 组件。就像 DeferredFogEffect 一样，让它在编辑模式下执行并给它一个 OnRenderImage 方法。最初它不做任何额外的事情，只是从源纹理 Blit 到目标纹理。 using UnityEngine; using System; [ExecuteInEditMode] public class BloomEffect : MonoBehaviour { void OnRenderImage (RenderTexture source, RenderTexture destination) { Graphics.Blit(source, destination); } } 让我们也将此效果应用于场景视图，以便更容易从不同的角度查看效果。这是通过向类添加 ImageEffectAllowedInSceneView 属性来完成的。 [ExecuteInEditMode, ImageEffectAllowedInSceneView] public class BloomEffect : MonoBehaviour { … } 将此组件作为唯一的效果添加到摄像机对象。这就完成了我们的测试场景。 Camera with bloom effect component 2 模糊 Bloom 效果是通过获取原始图像，以某种方式对其进行模糊处理，然后将结果与原始图像相结合来创建的。因此，要创建 Bloom，我们必须首先能够模糊图像。 2.1 渲染到另一个纹理 应用效果是通过从一个渲染纹理渲染到另一个渲染纹理来完成的。如果我们可以在一次 Pass 中完成所有工作，那么我们可以简单地使用适当的 Shader 从源 Blit 到目标。但是模糊需要做很多工作，所以让我们引入一个中间步骤。我们首先从源 Blit 到临时纹理，然后从该纹理 Blit 到最终目标。 获取临时渲染纹理最好通过调用 RenderTexture.GetTemporary 来完成。此方法负责为我们管理临时纹理，根据 Unity 的判断创建、缓存和销毁它们。至少，我们要指定纹理的尺寸。我们将从与源纹理相同的大小开始。 void OnRenderImage (RenderTexture source, RenderTexture destination) { RenderTexture r = RenderTexture.GetTemporary( source.width, source.height ); Graphics.Blit(source, destination); } 由于我们要模糊图像，我们不会对深度缓冲区做任何事情。为了表明这一点，使用 0 作为第三个参数。 RenderTexture r = RenderTexture.GetTemporary( source.width, source.height, 0 ); 因为我们使用的是 HDR，所以我们必须使用适当的纹理格式。由于摄像机应该启用 HDR，源纹理的格式将是正确的，所以我们可以使用它。它很可能是 ARGBHalf，但也可能使用其他格式。 RenderTexture r = RenderTexture.GetTemporary( source.width, source.height, 0, source.format ); 现在不直接从源 Blit 到目标，而是先从源 Blit 到临时纹理，然后再从那里 Blit 到目标。 // Graphics.Blit(source, destination); Graphics.Blit(source, r); Graphics.Blit(r, destination); 之后，我们不再需要临时纹理。为了使其可供重用，请通过调用 RenderTexture.ReleaseTemporary 释放它。 Graphics.Blit(source, r); Graphics.Blit(r, destination); RenderTexture.ReleaseTemporary(r); 虽然结果看起来还是一样，但我们现在通过临时纹理移动它。 2.2 降采样 模糊图像是通过平均像素来完成的。对于每个像素，我们必须决定将一堆附近的像素组合起来。包括哪些像素定义了用于效果的滤波核（Filter Kernel）。少量的模糊可以通过仅平均几个像素来完成，这意味着一个小核。大量的模糊需要一个大核，组合许多像素。 核中的像素越多，我们必须采样输入纹理的次数就越多。由于这是逐像素进行的，大核可能需要大量的采样工作。所以让我们尽可能保持简单。 平均像素的最简单、最快的方法是利用 GPU 内置的双线性过滤（Bilinear Filtering）。如果我们将临时纹理的分辨率减半，那么每四个源像素我们就得到一个像素。低分辨率像素将在原始四个像素之间精确采样，因此我们最终得到它们的平均值。我们甚至不需要为此使用自定义 Shader。 Bilinear downsampling int width = source.width / 2; int height = source.height / 2; RenderTextureFormat format = source.format; RenderTexture r = RenderTexture.GetTemporary(width, height, 0, format); 使用半尺寸的中间纹理意味着我们将源纹理降采样（Downsampling）到一半分辨率。在那一步之后，我们从临时纹理转到目标纹理，从而再次升采样（Upsampling）到原始分辨率。 Bilinear upsampling, showing interpolation for one pixel 这是一个两步模糊过程，每个像素与其周围的 4×4 像素块混合，有四种可能的配置。 Relative weights for indicated pixel, total 64 结果是一个比原始图像更块状且稍微模糊的图像。 Using a half-size intermediate texture 我们可以通过进一步减小中间步骤的大小来增加效果。 Dividing dimensions by 4, 8, 16, and 32 2.3 渐进式降采样 不幸的是，直接降采样到低分辨率会导致糟糕的结果。我们最终主要丢弃了像素，只保留了孤立的四个像素组的平均值。 Directly going to quarter size eliminates 12 out of 16 pixels 更好的方法是多次降采样，每一步将分辨率减半，直到达到所需的级别。这样所有像素最终都会对最终结果做出贡献。 Downsampling to half resolution twice keeps information of all pixels 为了控制我们这样做的次数，添加一个公共 iterations 字段。使其成为一个范围为 1–16 的滑动条。这将允许我们将 65536×65536 的纹理一直降采样到单个像素，这应该足够了。 [Range(1, 16)] public int iterations = 1; Iterations slider 为了使其工作，首先将 r 重命名为 currentDestination。在第一次 Blit 之后，添加一个显式的 currentSource 变量并将 currentDestination 分配给它，然后将其用于最终的 Blit 并释放它。 RenderTexture currentDestination = RenderTexture.GetTemporary(width, height, 0, format); Graphics.Blit(source, currentDestination); RenderTexture currentSource = currentDestination; Graphics.Blit(currentSource, destination); RenderTexture.ReleaseTemporary(currentSource); 现在我们可以在当前源的声明和最终 Blit 之间放置一个循环。因为它在第一次降采样之后，其迭代器应该从 1 开始。每一步，首先再次将纹理大小减半。然后抓取一个新的临时纹理并将当前源 Blit 到它。然后释放当前源并使当前目标成为新源。 RenderTexture currentSource = currentDestination; for (int i = 1; i &lt; iterations; i++) { width /= 2; height /= 2; currentDestination = RenderTexture.GetTemporary(width, height, 0, format); Graphics.Blit(currentSource, currentDestination); RenderTexture.ReleaseTemporary(currentSource); currentSource = currentDestination; } Graphics.Blit(currentSource, destination); 这工作正常，除非我们最终迭代次数过多，将大小减小到零。为了防止这种情况，在发生之前跳出循环。典型显示器的高度通常小于其宽度，所以你可以仅根据高度来判断。因为单像素线并没有真正增加多少，我在纹理高度降至 2 以下时就中止。 width /= 2; height /= 2; if (height &lt; 2) { break; } 那纵向模式的手机和其他例外情况呢？ 如果你想支持所有宽高比，只需同时检查宽度和高度。 Progressive downsampling 2 to 5 iterations 2.4 渐进式升采样 虽然渐进式降采样是一种改进，但结果仍然太快变得太块状。让我们看看如果我们也渐进式升采样是否有帮助。 双向迭代意味着我们最终会将每个尺寸渲染两次，除了最小的那个。与其每次渲染都释放然后声明相同的纹理两次，不如将它们保存在数组中。我们可以简单地为此使用一个固定大小为 16 的数组字段，这应该绰绰有余。 RenderTexture[] textures = new RenderTexture[16]; 每次我们抓取临时纹理时，也将其添加到数组中。 RenderTexture currentDestination = textures[0] = RenderTexture.GetTemporary(width, height, 0, format); … for (int i = 1; i &lt; iterations; i++) { … currentDestination = textures[i] = RenderTexture.GetTemporary(width, height, 0, format); … } 然后在初始循环之后添加第二个循环。这个循环从最低级别的一步开始。我们可以将迭代器从第一个循环中提升出来，从中减去 2，并将其用作另一个循环的起点。第二个循环向后进行，将迭代器一直减小到 0。这是我们应该释放旧源纹理的地方，而不是在第一个循环中。此外，我们也在这里清理数组。 int i = 1; for (; i &lt; iterations; i++) { … Graphics.Blit(currentSource, currentDestination); // RenderTexture.ReleaseTemporary(currentSource); currentSource = currentDestination; } for (i -= 2; i &gt;= 0; i--) { currentDestination = textures[i]; textures[i] = null; Graphics.Blit(currentSource, currentDestination); RenderTexture.ReleaseTemporary(currentSource); currentSource = currentDestination; } 5 iterations, with and without progressive upsampling 结果好多了，但还不够好。 2.5 自定义着色 为了改进我们的模糊，我们必须切换到比简单双线性过滤更高级的滤波核。这需要我们使用自定义 Shader，所以创建一个新的 Bloom Shader。就像 DeferredFog Shader 一样，从一个简单的 Shader 开始，它有一个 _MainTex 属性，没有剔除，并且不使用深度缓冲区。给它一个带有顶点和片元程序的 Pass。 Shader \"Custom/Bloom\" { Properties { _MainTex (\"Texture\", 2D) = \"white\" {} } SubShader { Cull Off ZTest Always ZWrite Off Pass { CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram ENDCG } } } 顶点程序甚至比雾效果的还要简单。它只需要将顶点位置转换到裁剪空间并通过全屏 Quad 的纹理坐标。因为我们最终会有多个 Pass，除了片元程序之外的所有东西都可以共享并在 CGINCLUDE 块中定义。 Properties { _MainTex (\"Texture\", 2D) = \"white\" {} } CGINCLUDE #include \"UnityCG.cginc\" sampler2D _MainTex; struct VertexData { float4 vertex : POSITION; float2 uv : TEXCOORD0; }; struct Interpolators { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; }; Interpolators VertexProgram (VertexData v) { Interpolators i; i.pos = UnityObjectToClipPos(v.vertex); i.uv = v.uv; return i; } ENDCG SubShader { … } 我们将在 Pass 本身中定义 FragmentProgram 函数。最初，只需采样源纹理并将其用作结果，使其变为红色以验证我们正在使用自定义 Shader。通常 HDR 颜色以半精度格式存储，所以让我们明确使用 half 而不是 float，即使这对于非移动平台没有区别。 Pass { CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { return tex2D(_MainTex, i.uv) * half4(1, 0, 0, 0); } ENDCG } 向我们的效果添加一个公共字段来保存对此 Shader 的引用，并在 Inspector 中挂接它。 public Shader bloomShader; Bloom effect with shader 添加一个字段来保存将使用此 Shader 的材质，这不需要序列化。在渲染之前，检查我们是否有此材质，如果没有则创建它。我们不需要在层次结构中看到它，也不需要保存它，所以相应地设置它的 hideFlags。 [NonSerialized] Material bloom; void OnRenderImage (RenderTexture source, RenderTexture destination) { if (bloom == null) { bloom = new Material(bloomShader); bloom.hideFlags = HideFlags.HideAndDontSave; } … } 每次我们 Blit 时，都应该使用此材质而不是默认材质。 void OnRenderImage (RenderTexture source, RenderTexture destination) { … Graphics.Blit(source, currentDestination, bloom); … Graphics.Blit(currentSource, currentDestination, bloom); … Graphics.Blit(currentSource, currentDestination, bloom); … Graphics.Blit(currentSource, destination, bloom); … } Using our custom shader 2.6 盒式采样（Box Sampling） 我们将调整 Shader，使其使用不同于双线性过滤的采样方法。因为采样取决于像素大小，所以在 CGINCLUDE 块中添加神奇的 float4 _MainTex_TexelSize 变量。请记住，这对应于源纹理的纹素大小，而不是目标纹理。 sampler2D _MainTex; float4 _MainTex_TexelSize; 由于我们总是采样主纹理并且只关心 RGB 通道，让我们创建一个方便的最小 Sample 函数。 half3 Sample (float2 uv) { return tex2D(_MainTex, uv).rgb; } 我们将使用简单的盒式滤波器核（Box Filter Kernel），而不是仅依赖双线性滤波器。它取四个样本而不是一个，对角定位，以便我们获得四个相邻 2×2 像素块的平均值。对这些样本求和并除以四，这样我们最终得到一个 4×4 像素块的平均值，使我们的核大小加倍。 Downsampling with a 4×4 box, showing the sample points half3 SampleBox (float2 uv) { float4 o = _MainTex_TexelSize.xyxy * float2(-1, 1).xxyy; half3 s = Sample(uv + o.xy) + Sample(uv + o.zy) + Sample(uv + o.xw) + Sample(uv + o.zw); return s * 0.25f; } 在我们的片元程序中使用此采样函数。 half4 FragmentProgram (Interpolators i) : SV_Target { // return tex2D(_MainTex, i.uv) * half4(1, 0, 0, 0); return half4(SampleBox(i.uv), 1); } 5 iterations, with 4×4 box sampling 2.7 不同的 Pass 结果更加平滑且质量更高，但也更加模糊。这主要是由于使用新的 4×4 盒式滤波器进行升采样。由于我们使用源纹素大小来定位采样点，我们最终覆盖了一个很大的区域，具有未聚焦的规则权重分布。 Upsampling with a 4×4 box 我们可以通过调整用于选择采样点的 UV 增量（Delta）来调整盒式滤波器。为了使这成为可能，将增量变成参数，而不是总是使用 1。 half3 SampleBox (float2 uv, float delta) { float4 o = _MainTex_TexelSize.xyxy * float2(-delta, delta).xxyy; half3 s = Sample(uv + o.xy) + Sample(uv + o.zy) + Sample(uv + o.xw) + Sample(uv + o.zw); return s * 0.25f; } 复制我们的 Shader Pass，这样我们最终得到两个。第一个—— Pass 0 ——将用于降采样，所以它应该使用原始增量 1。第二个 Pass 将用于升采样，我们将使用增量 0.5。 Pass { // 0 CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { return half4(SampleBox(i.uv, 1), 1); } ENDCG } Pass { // 1 CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { return half4(SampleBox(i.uv, 0.5), 1); } ENDCG } 使用 0.5 的 UV 增量，我们最终用重叠的样本覆盖了一个 3×3 的盒子。所以一些像素对结果的贡献超过一次，增加了它们的权重。中间像素参与所有样本，对角像素仅使用一次，而其他像素出现两次。结果是一个更聚焦的升采样核。 Focused upsampling 接下来，我们必须指示 Blit 时应使用哪个 Pass。为了方便起见，向 BloomEffect 添加常量，以便我们可以使用名称而不是索引。 const int BoxDownPass = 0; const int BoxUpPass = 1; 前两个 Blit 是 Down Pass，另外两个是 Up Pass。 void OnRenderImage (RenderTexture source, RenderTexture destination) { … Graphics.Blit(source, currentDestination, bloom, BoxDownPass); … Graphics.Blit(currentSource, currentDestination, bloom, BoxDownPass); … Graphics.Blit(currentSource, currentDestination, bloom, BoxUpPass); … Graphics.Blit(currentSource, destination, bloom, BoxUpPass); … } Blurring with different passes 此时，我们有一个相当简单但不错的模糊过程。我们可以使用许多不同的核来代替这些简单的滤波器核，每一个都有自己的优势——比如更好的时间稳定性——和成本。但是，对于本教程，我们将坚持使用这些。 3 创建 Bloom 模糊原始图像是创建 Bloom 效果的第一步。第二步是将模糊图像与原始图像相结合，使其变亮。但是，我们不会只使用最终的模糊结果，因为那会产生相当均匀的涂抹。相反，较低量的模糊应该比较高量的模糊对结果的贡献更大。我们可以通过累积中间结果，在升采样时添加到旧数据来做到这一点。 Additive blurring 3.1 叠加混合 使用叠加混合（Additive Blending）可以添加到我们在某个中间级别已有的内容，而不是替换纹理的内容。我们要做的就是将升采样 Pass 的混合模式设置为 One One。 Pass { // 1 Blend One One CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { return half4(SampleBox(i.uv, 0.5), 1); } ENDCG } 这种简单的方法对于所有中间 Pass 都可以正常工作，但在渲染到最终目标时会出错，因为我们还没有渲染到它。我们可能会最终每帧累积光照，使图像过曝，或者其他情况，具体取决于 Unity 如何重用纹理。为了解决这个问题，我们必须为最后一次升采样创建一个单独的 Pass，在那里我们将原始源纹理与最后一个中间纹理结合起来。所以我们需要一个源的 Shader 变量。 sampler2D _MainTex, _SourceTex; 添加第三个 Pass，它是第二个 Pass 的副本，只是它使用默认的混合模式，并将盒式样本添加到源纹理的样本中。 Pass { // 2 CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { half4 c = tex2D(_SourceTex, i.uv); c.rgb += SampleBox(i.uv, 0.5); return c; } ENDCG } 为此 Pass 定义一个常量，它将 Bloom 应用于原始图像。 const int BoxDownPass = 0; const int BoxUpPass = 1; const int ApplyBloomPass = 2; 最后一个 Blit 必须使用此 Pass，并使用正确的源纹理。 // Graphics.Blit(currentSource, destination, bloom, BoxUpPass); bloom.SetTexture(\"_SourceTex\", source); Graphics.Blit(currentSource, destination, bloom, ApplyBloomPass); RenderTexture.ReleaseTemporary(currentSource); Bloom 3.2 Bloom 阈值 现在我们仍在模糊整个图像。只是对于明亮的像素最明显。但 Bloom 的用途之一是仅将其应用于非常明亮的像素。为了实现这一点，我们必须引入亮度阈值（Threshold）。为此添加一个公共字段，滑动条范围从 0 到某个非常亮的值，如 10。让我们使用默认阈值 1，排除 LDR 像素。 [Range(0, 10)] public float threshold = 1; Threshold slider 阈值决定了哪些像素对 Bloom 效果有贡献。如果它们不够亮，就不应该在降采样和升采样过程中包含它们。简单地将它们转换为黑色就可以做到这一点，这必须由 Shader 完成。所以在我们 Blit 之前设置材质的 _Threshold 变量。 if (bloom == null) { … } bloom.SetFloat(\"_Threshold\", threshold); 也将此变量添加到 Shader 的 CGINCLUDE 块中，再次使用 half 类型。 half _Threshold; 我们将使用阈值过滤掉我们不希望包含的像素。由于我们在模糊过程的开始执行此操作，因此称为预过滤步骤（Prefilter Step）。为此创建一个函数，它接受一种颜色并输出经过过滤的颜色。 half3 Prefilter (half3 c) { return c; } 我们将使用颜色的最大分量来确定其亮度 $b = c_r \\vee c_g \\vee c_b$，其中 $\\vee$ 符号是我用来表示最大函数的运算符。 我们可以通过从亮度中减去阈值，然后除以亮度来确定颜色的贡献因子 $w = \\frac{b-t}{b}$，其中 $t$ 是阈值。当 $t=0$ 时，结果始终为 1，这使颜色保持不变。随着 $t$ 增加，亮度曲线将向下弯曲，以便在 $b=t$ 处降至零。 由于曲线的形状，它被称为拐点（Knee）。由于我们不希望出现负因子，我们必须确保 $b-t$ 不会降至零以下，从而导致 $w = \\frac{0 \\vee (b-t)}{b}$。 Knee curves with thresholds 0.25, 0.5, 0.75, and 1 为了避免 Shader 中除以零，确保除数至少有一个小值，如 0.00001。然后使用结果调制颜色。 half3 Prefilter (half3 c) { half brightness = max(c.r, max(c.g, c.b)); half contribution = max(0, brightness - _Threshold); contribution /= max(brightness, 0.00001); return c * contribution; } 过滤器仅应用于第一个 Pass。所以复制第一个 Pass，将其作为 Pass 0 放在顶部。将过滤器应用于盒式样本的结果。 Pass { // 0 CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { return half4(Prefilter(SampleBox(i.uv, 1)), 1); } ENDCG } 为这个新 Pass 添加一个常量，并将所有后续 Pass 的索引增加一。 const int BoxDownPrefilterPass = 0; const int BoxDownPass = 1; const int BoxUpPass = 2; const int ApplyBloomPass = 3; 将新 Pass 用于第一次 Blit。 RenderTexture currentDestination = textures[0] = RenderTexture.GetTemporary(width, height, 0, format); Graphics.Blit(source, currentDestination, bloom, BoxDownPrefilterPass); RenderTexture currentSource = currentDestination; 此时，将阈值设置为 1，你可能会看到没有或几乎没有 Bloom，假设使用的灯光和材质没有 HDR 值。为了让 Bloom 出现，你可以增加一些材质的光贡献。例如，我使黄色材质发光，这与反射光一起将黄色像素推入 HDR。 Emissive yellow 3.3 隔离 Bloom 为了更好地查看图像的哪些部分对 Bloom 有贡献，如果我们可以单独查看模糊效果，那就太方便了。所以让我们向我们的效果添加一个调试选项，通过公共布尔字段控制。 public bool debug; Debug toggle 我们将为调试目的创建一个单独的 Pass，所以在底部为其添加一个常量。 const int BoxDownPrefilterPass = 0; const int BoxDownPass = 1; const int BoxUpPass = 2; const int ApplyBloomPass = 3; const int DebugBloomPass = 4; 在调试模式下，直接将最后一个中间结果 Blit 到最终目标——使用调试 Pass ——而不是将其添加到源中。 if (debug) { Graphics.Blit(currentSource, destination, bloom, DebugBloomPass); } else { bloom.SetTexture(\"_SourceTex\", currentSource); Graphics.Blit(source, destination, bloom, ApplyBloomPass); } RenderTexture.ReleaseTemporary(currentSource); 新的调试 Pass 只是执行最后一次升采样，不与任何东西结合。 Pass { // 4 CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { return half4(SampleBox(i.uv, 0.5), 1); } ENDCG } Bloom effect only 在调试模式下，我们可以清楚地看到黄色像素最终产生了 Bloom。除此之外，一些白色像素也被包括在内，但只有当它们最终反射大量定向光时才会如此。 为什么一些白色表面最终会变成 HDR？ 默认的 Unity 场景非常明亮。除了定向光，还有环境光照和反射，它们都有助于最终的像素颜色。所有这些加在一起可能会导致亮度值大于 1。 3.4 软阈值（Soft Threshold） 我们用来调制颜色的拐点曲线以一定角度切过零，导致突然的截止点。这就是为什么它也被称为硬拐点（Hard Knee）。这意味着我们可能会在产生 Bloom 的区域和不产生 Bloom 的区域之间出现急剧过渡。这可以在上面截图中的大白色球体中看到。该球体有一个明确定义的部分被包括在内。这在某种程度上被模糊混淆了，但它仍然是一个严酷的过渡。 可以使这种过渡更平滑，从零混合到完全贡献。我们将通过滑动条控制它。在 0 处，我们得到当前的严酷过渡。在 1 处，我们得到一个软阈值，它从亮度 0 一直平滑地淡入 Bloom，直到它与硬拐点匹配。我们将使用 0.5 作为默认值。 [Range(0, 1)] public float softThreshold = 0.5f; Soft threshold slider 这种淡入淡出也是由 Shader 完成的，所以将软阈值因子传递给材质。 bloom.SetFloat(\"_Threshold\", threshold); bloom.SetFloat(\"_SoftThreshold\", softThreshold); 并向 Shader 添加一个变量。 half _Threshold, _SoftThreshold; 通过软化我们的硬拐点曲线，我们将其变成软拐点（Soft Knee）。我们要做的不是取 $b-t$ 和 0 的最大值，而是取 $b-t$ 和单独的软化曲线 $s$ 的最大值。所以我们得到 $w = \\frac{s \\vee (b-t)}{b}$。 软曲线定义为 $s = \\frac{(b-t+k)^2}{4k}$，其中 $k = t \\cdot ts$，$ts$ 是软阈值。 Soft curves divided by b with threshold 1, and soft 0, 0.25, 0.5, and 0.75 我们必须截断这条曲线，在它接触 0 的地方以及它遇到硬拐点的地方，这是通过使用 $s = \\frac{(0 \\vee (b-t+k) \\wedge 2k)^2}{4k}$ 来完成的，其中 $\\wedge$ 代表最小函数。调整预过滤函数以执行此计算，再次防止除以零。 half3 Prefilter (half3 c) { half brightness = max(c.r, max(c.g, c.b)); half knee = _Threshold * _SoftThreshold; half soft = brightness - _Threshold + knee; soft = clamp(soft, 0, 2 * knee); soft = soft * soft / (4 * knee + 0.00001); half contribution = max(soft, brightness - _Threshold); contribution /= max(brightness, 0.00001); return c * contribution; } Soft threshold 请注意，软拐点函数的某些部分可以隔离，以便它们仅依赖于配置值，这些值对于每个 Pass 都是常数。我们可以预先计算这些部分并将它们传递给向量中的 Shader，从而减少它必须做的工作量。我们可以将这些与阈值结合在一个单独的过滤器向量中。 // bloom.SetFloat(\"_Threshold\", threshold); // bloom.SetFloat(\"_SoftThreshold\", softThreshold); float knee = threshold * softThreshold; Vector4 filter; filter.x = threshold; filter.y = filter.x - knee; filter.z = 2f * knee; filter.w = 0.25f / (knee + 0.00001f); bloom.SetVector(\"_Filter\", filter); 相应地调整 Shader。 // half _Threshold, _SoftTheshold; half4 _Filter; … half3 Prefilter (half3 c) { half brightness = max(c.r, max(c.g, c.b)); // half knee = _Threshold * _SoftThreshold; half soft = brightness - _Filter.y; soft = clamp(soft, 0, _Filter.z); soft = soft * soft * _Filter.w; half contribution = max(soft, brightness - _Filter.x); contribution /= max(brightness, 0.00001); return c * contribution; } 3.5 Bloom 强度 最后，让我们能够调制 Bloom 效果的强度。这使得将其淡入淡出成为可能，也可以创造极其强烈的效果。为此添加一个滑动条，范围如 0–10。默认值应为 1。 [Range(0, 10)] public float intensity = 1; Intensity slider 将此强度值作为材质属性传递给 Shader。由于通常使用 Gamma 空间中的因子设置 Bloom 的强度，因此将其从 Gamma 转换为线性空间。 bloom.SetVector(\"_Filter\", filter); bloom.SetFloat(\"_Intensity\", Mathf.GammaToLinearSpace(intensity)); 向 Shader 添加适当的变量。 half _Intensity; 将强度计入最后两个 Pass 的最终盒式样本中。 Pass { // 3 CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { half4 c = tex2D(_SourceTex, i.uv); c.rgb += _Intensity * SampleBox(i.uv, 0.5); return c; } ENDCG } Pass { // 4 CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram half4 FragmentProgram (Interpolators i) : SV_Target { return half4(_Intensity * SampleBox(i.uv, 0.5), 1); } ENDCG } Bloom intensity 你现在有了一个基本的 Bloom 效果。它与 Unity 的 Post Processing Stack v2 的 Bloom 效果非常相似。可以通过添加色调、使采样增量可配置、使用不同的滤波器等来进一步扩展它。或者你可以继续景深Depth of Field" }, { "title": "曲面细分表面置换(翻译二十三)", "url": "/posts/unity-surface-displacement/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-28 09:12:01 +0800", "content": "在 GPU 上调整顶点位置。 细分阴影几何体。 跳过细分不可见的三角形。 1 重定位顶点 网格通常由三角形组成，而三角形总是平坦的。曲率的错觉是通过顶点法线添加的。法线贴图可用于添加更多表面不规则的错觉，这些不规则比单个网格三角形还要小。除此之外，视差贴图（Parallax Mapping）使得伪造表面置换成为可能。但所有这些方法都是错觉。使表面更复杂的所谓“最稳健”的方法就是简单地使用更多更小的三角形。更小的三角形意味着我们有更多的顶点，足以描述我们想要的所有表面细节。不幸的是，这将导致更大的网格，需要更多的存储空间、CPU 和 GPU 内存以及内存带宽。曲面细分是解决这个问题的一种方法，因为它允许我们在需要时在 GPU 上生成更多三角形。这意味着 GPU 必须做更多的工作，但我们可以将其限制在真正需要的时候。 仅仅切割现有的三角形并插值顶点数据不足以添加更多细节。这只是给了我们更多描述相同平坦表面的三角形。我们必须引入新数据，以某种方式调整三角形的顶点。 添加更多细节的一个直接方法是通过置换贴图（Displacement Map）调整网格的顶点。该贴图用于向上或向下移动顶点，就像高度场（Height Field）可用于将平坦的地形网格变成实际的景观一样。本教程将介绍如何做到这一点。 1.1 劫持视差贴图 要置换顶点，我们需要一个置换贴图。虽然我们的 Tessellation Shader 没有这样一个贴图的属性，但它有一个我们在视差贴图教程中使用的视差贴图。视差贴图实际上就是一个置换贴图，只是我们用它来伪造置换。我们也可以将同一个贴图用于实际的置换。 假设一个 Shader 可以决定使用真正的顶点置换而不是视差贴图，只需定义 VERTEX_DISPLACEMENT_INSTEAD_OF_PARALLAX 即可。如果定义了该宏并且我们有视差贴图，那么我们必须确保不包含视差代码，而是将其替换为适当的顶点置换代码。为此，当我们有视差贴图且应该使用顶点置换时，在 My Lighting Input 中取消定义 _PARALLAX_MAP 并定义一个方便的 VERTEX_DISPLACEMENT 宏。 #include \"UnityPBSLighting.cginc\" #include \"AutoLight.cginc\" #if defined(_PARALLAX_MAP) &amp;&amp; defined(VERTEX_DISPLACEMENT_INSTEAD_OF_PARALLAX) #undef _PARALLAX_MAP #define VERTEX_DISPLACEMENT 1 #endif 让我们也为 _ParallaxMap 和 _ParallaxStrength 变量创建宏别名，这样我们可以使用 _DisplacementMap 和 _DisplacementStrength 代替。这使得以后如果你想摆脱视差代码并切换到适当的置换属性时更容易。 #if defined(_PARALLAX_MAP) &amp;&amp; defined(VERTEX_DISPLACEMENT_INSTEAD_OF_PARALLAX) #undef _PARALLAX_MAP #define VERTEX_DISPLACEMENT 1 #define _DisplacementMap _ParallaxMap #define _DisplacementStrength _ParallaxStrength #endif 由于我们不会同时使用视差贴图和曲面细分，我们可以在 Tessellation Shader 的 CGINCLUDE 块中删除所有与视差相关的定义。相反，我们只需要定义 VERTEX_DISPLACEMENT_INSTEAD_OF_PARALLAX。 CGINCLUDE #define BINORMAL_PER_FRAGMENT #define FOG_DISTANCE // #define PARALLAX_BIAS 0 // #define PARALLAX_OFFSET_LIMITING // #define PARALLAX_RAYMARCHING_STEPS 10 // #define PARALLAX_RAYMARCHING_INTERPOLATE // #define PARALLAX_RAYMARCHING_SEARCH_STEPS 3 // #define PARALLAX_FUNCTION ParallaxRaymarching // #define PARALLAX_SUPPORT_SCALED_DYNAMIC_BATCHING #define VERTEX_DISPLACEMENT_INSTEAD_OF_PARALLAX ENDCG 我们将在对象空间（Object Space）中执行顶点置换。为了允许相当数量的置换，将最大强度从 0.1 增加到 1。 _ParallaxStrength (\"Parallax Strength\", Range(0, 1)) = 0 Parallax map with strength set to 1 1.2 改变顶点位置 置换顶点必须在 My Lighting 的顶点程序中完成，在我们使用顶点位置做任何其他事情之前。这意味着如果我们想支持像所有其他贴图一样缩放和偏移置换贴图，我们必须在此点之前转换纹理坐标。所以让我们将 TRANSFORM_TEX 行移动到第一次使用顶点位置之前。 InterpolatorsVertex MyVertexProgram (VertexData v) { InterpolatorsVertex i; UNITY_INITIALIZE_OUTPUT(InterpolatorsVertex, i); UNITY_SETUP_INSTANCE_ID(v); UNITY_TRANSFER_INSTANCE_ID(v, i); i.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); i.pos = UnityObjectToClipPos(v.vertex); i.worldPos.xyz = mul(unity_ObjectToWorld, v.vertex); #if FOG_DEPTH i.worldPos.w = i.pos.z; #endif i.normal = UnityObjectToWorldNormal(v.normal); #if defined(BINORMAL_PER_FRAGMENT) i.tangent = float4(UnityObjectToWorldDir(v.tangent.xyz), v.tangent.w); #else i.tangent = UnityObjectToWorldDir(v.tangent.xyz); i.binormal = CreateBinormal(i.normal, i.tangent, v.tangent.w); #endif // i.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); // i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); … } 一旦我们有了最终的纹理坐标，我们就可以采样置换贴图。这与采样视差贴图相同，所以我们将使用它的绿色纹理通道。但是，因为我们不是在片元程序中这样做，所以没有屏幕空间导数（Screen-space Derivatives）可用，因此 GPU 无法确定使用哪个 Mipmap 级别。我们不能使用 tex2D，而必须使用 tex2Dlod 来指定显式 Mip 级别。这是通过提供两个额外的纹理坐标来完成的，第三个是未使用的 3D 坐标，第四个指定 Mip 级别。我们将对两者都使用 0，实际上不使用 Mipmaps。 i.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); #if VERTEX_DISPLACEMENT float displacement = tex2Dlod(_DisplacementMap, float4(i.uv.xy, 0, 0)).g; #endif 就像我们对视差贴图所做的那样，让我们解释贴图，使 0.5 的值意味着没有变化，从而可以向上和向下移动顶点。之后，计入置换强度，以便我们可以控制顶点在对象空间中移动的程度。 #if VERTEX_DISPLACEMENT float displacement = tex2Dlod(_DisplacementMap, float4(i.uv.xy, 0, 0)).g; displacement = (displacement - 0.5) * _DisplacementStrength; #endif 如果我们使用默认的高度场，那么我们只需在此点将置换添加到顶点 Y 位置。 float displacement = tex2Dlod(_DisplacementMap, float4(i.uv.xy, 0, 0)).g; displacement = (displacement - 0.5) * _DisplacementStrength; v.vertex.y += displacement; Quad vertices displaced along Y 当将此方法应用于 Quad 时，结果看起来像一团乱七八糟的三角形，但仍然是平坦的。这是因为 Quad 在对象空间中与 XY 平面对齐。如果我们想扰动其原本平坦的表面，我们必须调整其 Z 坐标。一般来说，从网格的角度来看，正置换应该向上移动顶点。但并非所有网格都是平面。在球体的情况下，置换向外移动顶点是有意义的。所以通常最合理的做法是沿顶点法线进行置换。 // v.vertex.y += displacement; v.vertex.xyz += v.normal * displacement; 因为我们使用的是曲面细分，新顶点的法线向量是通过插值创建的。所以只有当所有顶点法线具有相同的方向时，它们才保证是单位长度的。为了保证我们通常获得单位长度的法线向量，我们在使用它们进行置换之前应该将它们归一化。 v.normal = normalize(v.normal); v.vertex.xyz += v.normal * displacement; Displacement along normal vector 1.3 使用足够的三角形 支持所需的细节级别需要多少三角形？这取决于情况。我们在全强度下的置换贴图产生了相当大的变化，所以我们需要相当多的三角形才能使其看起来不错。但我们不想使用比我们需要的更多的三角形，所以我们应该使用 Edge 曲面细分模式而不是 Uniform 模式。 Variable tessellation of a quad 处于 Edge 模式时，曲面细分由 Edge Length 属性和视图距离控制。所以使用的三角形数量可能会有很大差异。一个 Quad 本身只包含两个三角形。我们需要大量的曲面细分才能获得比低多边形锯齿平面更好的东西。我们可以通过使用具有更多三角形的基础网格来大大帮助曲面细分。例如，Unity 的默认平面网格由 10×10 个 Quad 组成。使用它而不是 Quad 可以防止完全退化，如果需要，也可以产生比 Quad 高得多的顶点分辨率。 Variable tessellation of a plane 使用平面代替 Quad 允许更微调的曲面细分，这使得更容易从所有视角实现视觉上均匀的三角形密度。 Shallow view angle, uniform triangle density 然而，这仍然不能保证所有三角形最终具有相同的视觉大小。细分的三角形仅在其顶点被置换之前大约是相同的大小。如果三角形的顶点最终被不同程度地置换，它将沿法线向量被拉伸。通常，顶点置换并不像我们在本教程中使用的示例那样极端。如果你将其用于地形网格，常规网格应该有足够的分辨率来表示地形的大特征。如果你使用平坦网格作为地形的基础，请考虑在确定曲面细分因子之前置换原始顶点，以便在细分时将粗略的高程考虑在内。 1.4 法线着色 到目前为止，我们一直在使用平坦线框着色效果，以使其在视觉上明显地显示三角形是如何被细分的。但大多数时候，目标是在没有明显曲面细分的情况下增强网格。所以让我们恢复到默认的着色方法。我们通过从 Tessellation Shader 的 forward base、additive 和 deferred 通道中删除几何着色器指令来做到这一点。我们还必须在相同的通道中用 My Lighting 替换 MyFlatWireframe 包含文件的使用。 // #pragma geometry MyGeometryProgram … // #include \"MyFlatWireframe.cginc\" #include \"My Lighting.cginc\" #include \"MyTessellation.cginc\" 随着平坦线框效果的移除，我们也不再需要线框属性了。 // _WireframeColor (\"Wireframe Color\", Color) = (0, 0, 0) // _WireframeSmoothing (\"Wireframe Smoothing\", Range(0, 10)) = 1 // _WireframeThickness (\"Wireframe Thickness\", Range(0, 10)) = 1 我们现在回到了正常的着色，结果看起来很平坦。这是因为我们置换了顶点位置，但没有调整顶点法线以匹配。确切的最终结果取决于你是使用 forward 还是 deferred 渲染路径。 Forward and deferred rendering 渲染路径之间的视觉差异是由于阴影造成的。我们还没有为阴影做任何特殊处理，所以我们得到了平面的默认阴影。在 forward 渲染的情况下，用于屏幕空间阴影的深度通道和阴影投射都是用这个平面完成的。所以我们的平面最终没有投射阴影给自己。在 deferred 渲染的情况下，细分的几何体用于填充 G-buffer，包括深度缓冲。所以向下置换的细分几何体最终被平坦平面的阴影所遮挡。我们将在下一节处理阴影，所以现在我将使用 forward 渲染路径。 为了让我们的置换表面获得正确的着色，我们必须使用正确的法线向量。幸运的是，我们有一个与视差贴图匹配的法线贴图，所以我们可以直接使用它。 With normal map 这种方法适用于任何网格，但重要的是没有纹理接缝。任何接缝都会导致不连续性。如果我们只使用法线贴图，这将导致着色中的伪影，暗示在不应该有硬边的地方出现了硬边。在置换的情况下，它会导致网格中的间隙，这要糟糕得多。要看到这一点的一个好例子是，将我们的曲面细分材质应用于默认球体并检查其极点。 Displacing sphere vertices creates gaps 我们怎样才能让它适用于球体？ 你必须使用一种没有不连续性的置换贴图方法。例如，你可以使用立方体贴图（Cubemap）而不是经纬度贴图。 此时，我们已经通过曲面细分完成了一个置换效果，取代了之前教程中的视差效果。曲面细分相对于视差贴图的一大优势是它与其他所有东西都能很好地配合，因为它只是三角形。所有适用于常规三角形的技术都适用，并且它与其他几何体正确相交。 Tessellated geometry intersects correctly 2 阴影 目前，阴影的表现就像我们的平面仍然是平坦的一样。只有在使用 deferred 渲染时，置换的几何体才被用于接收阴影，但投射的阴影仍然是平坦的。我们现在要确保障阴影与置换表面匹配。 Incorrect shadows, forward and deferred 2.1 带曲面细分的 Shadow Caster Pass 要使阴影工作，我们要做的第一件事是为 Shadow Caster Pass 启用曲面细分。这意味着此通道的 Shader Target 级别必须增加到 4.6。 #pragma target 4.6 由于我们的置换方法使用视差贴图，我们必须为其添加适当的 Shader Feature，还要为 Edge 曲面细分模式添加一个 Feature。 #pragma shader_feature _SMOOTHNESS_ALBEDO #pragma shader_feature _PARALLAX_MAP #pragma shader_feature _TESSELLATION_EDGE 然后我们必须将阴影的顶点程序替换为曲面细分顶点程序，并添加所需的 Hull 和 Domain 程序。 // #pragma vertex MyShadowVertexProgram #pragma vertex MyTessellationVertexProgram #pragma fragment MyShadowFragmentProgram #pragma hull MyHullProgram #pragma domain MyDomainProgram 这些程序定义在 MyTessellation 中，所以在 My Shadows 之后包含它。 #include \"My Shadows.cginc\" #include \"MyTessellation.cginc\" 2.2 使细分阴影工作 此时我们的 Shadow Caster Pass 无法在没有错误的情况下编译。这是因为 My Shadows 没有遵循与 My Lighting 完全相同的方法。第一个问题是 MyTessellation 期望 VertexData 的顶点位置字段名为 vertex，而在 My Shadows 中它被称为 position。让我们通过在 My Shadows 中将其重命名为 vertex 来解决这个问题。 struct VertexData { UNITY_VERTEX_INPUT_INSTANCE_ID // float4 position : POSITION; float4 vertex : POSITION; float3 normal : NORMAL; float2 uv : TEXCOORD0; }; 顶点位置在 MyShadowVertexProgram 中使用了两次，所以也要更改这些引用。 InterpolatorsVertex MyShadowVertexProgram (VertexData v) { … #if defined(SHADOWS_CUBE) i.position = UnityObjectToClipPos(v.vertex); i.lightVec = mul(unity_ObjectToWorld, v.vertex).xyz - _LightPositionRange.xyz; #else i.position = UnityClipSpaceShadowCasterPos(v.vertex.xyz, v.normal); i.position = UnityApplyLinearShadowBias(i.position); #endif … } 下一个问题是阴影使用的顶点数据比其他三个通道少。具体来说，它们不需要 tangent、uv1 和 uv2 数据。我们可以无论如何都添加这些数据，但那会不必要地使阴影变慢。相反，让我们调整 MyTessellation 以便它可以支持更少的顶点数据。我们可以通过仅在定义了适当的宏时才在 TessellationControlPoint 结构中包含 tangent、uv1 和 uv2 来做到这一点。 struct TessellationControlPoint { float4 vertex : INTERNALTESSPOS; float3 normal : NORMAL; #if TESSELLATION_TANGENT float4 tangent : TANGENT; #endif float2 uv : TEXCOORD0; #if TESSELLATION_UV1 float2 uv1 : TEXCOORD1; #endif #if TESSELLATION_UV2 float2 uv2 : TEXCOORD2; #endif }; 使用相同的技巧，我们可以控制 MyTessellationVertexProgram 是否将相关字段从顶点数据复制到控制点。 TessellationControlPoint MyTessellationVertexProgram (VertexData v) { TessellationControlPoint p; p.vertex = v.vertex; p.normal = v.normal; #if TESSELLATION_TANGENT p.tangent = v.tangent; #endif p.uv = v.uv; #if TESSELLATION_UV1 p.uv1 = v.uv1; #endif #if TESSELLATION_UV2 p.uv2 = v.uv2; #endif return p; } 以及 MyDomainProgram 是否插值数据。 [UNITY_domain(\"tri\")] InterpolatorsVertex MyDomainProgram ( … ) { … MY_DOMAIN_PROGRAM_INTERPOLATE(vertex) MY_DOMAIN_PROGRAM_INTERPOLATE(normal) #if TESSELLATION_TANGENT MY_DOMAIN_PROGRAM_INTERPOLATE(tangent) #endif MY_DOMAIN_PROGRAM_INTERPOLATE(uv) #if TESSELLATION_UV1 MY_DOMAIN_PROGRAM_INTERPOLATE(uv1) #endif #if TESSELLATION_UV2 MY_DOMAIN_PROGRAM_INTERPOLATE(uv2) #endif return MyVertexProgram(data); } 这种方法允许我们微调在细分时包含哪些网格数据。我们不需要阴影的 tangent、uv1 和 uv2，但其他三个通道可能都需要它们。所以让我们在 My Lighting Input 的顶部定义相关的宏。 #define TESSELLATION_TANGENT 1 #define TESSELLATION_UV1 1 #define TESSELLATION_UV2 1 #if defined(_PARALLAX_MAP) &amp;&amp; defined(VERTEX_DISPLACEMENT_INSTEAD_OF_PARALLAX) … #endif 我们在其他通道中总是需要所有这些数据吗？ 不，但我们只是假设我们需要。你可以进一步微调，仅在真正需要时才包含 UV1 和 UV2。 最后一个问题是 My Lighting 依赖于 MyVertexProgram 的存在，但我们已将 Shadow Caster Pass 的顶点程序命名为 MyShadowVertexProgram。快速解决方案是在 My Shadows 中定义一个宏别名。这样 MyVertexProgram 也适用于阴影，而不会破坏现有的 Shader。 #define MyVertexProgram MyShadowVertexProgram InterpolatorsVertex MyShadowVertexProgram (VertexData v) { … } 2.3 置换阴影几何体 阴影现在被细分了。下一步是置换它们的顶点，为此我们可以使用应用于 My Lighting 的相同方法。首先，将适当的宏定义复制到 My Shadows 的顶部。唯一的区别是我们还必须定义 SHADOWS_NEED_UV，如果它尚未定义的话。 #if SHADOWS_SEMITRANSPARENT || defined(_RENDERING_CUTOUT) #if !defined(_SMOOTHNESS_ALBEDO) #define SHADOWS_NEED_UV 1 #endif #endif #if defined(_PARALLAX_MAP) &amp;&amp; defined(VERTEX_DISPLACEMENT_INSTEAD_OF_PARALLAX) #undef _PARALLAX_MAP #define VERTEX_DISPLACEMENT 1 #define _DisplacementMap _ParallaxMap #define _DisplacementStrength _ParallaxStrength #if !defined(SHADOWS_NEED_UV) #define SHADOWS_NEED_UV 1 #endif #endif 阴影没有使用视差贴图，所以我们现在必须添加所需的变量。 sampler2D _ParallaxMap; float _ParallaxStrength; 在阴影顶点程序中，将纹理坐标的变换移动到顶点位置的使用之上。 InterpolatorsVertex MyShadowVertexProgram (VertexData v) { InterpolatorsVertex i; UNITY_SETUP_INSTANCE_ID(v); UNITY_TRANSFER_INSTANCE_ID(v, i); #if SHADOWS_NEED_UV i.uv = TRANSFORM_TEX(v.uv, _MainTex); #endif #if defined(SHADOWS_CUBE) i.position = UnityObjectToClipPos(v.vertex); i.lightVec = mul(unity_ObjectToWorld, v.vertex).xyz - _LightPositionRange.xyz; #else i.position = UnityClipSpaceShadowCasterPos(v.vertex.xyz, v.normal); i.position = UnityApplyLinearShadowBias(i.position); #endif // #if SHADOWS_NEED_UV // i.uv = TRANSFORM_TEX(v.uv, _MainTex); // #endif return i; } 然后置换顶点位置。 #if SHADOWS_NEED_UV i.uv = TRANSFORM_TEX(v.uv, _MainTex); #endif #if VERTEX_DISPLACEMENT float displacement = tex2Dlod(_DisplacementMap, float4(i.uv.xy, 0, 0)).g; displacement = (displacement - 0.5) * _DisplacementStrength; v.normal = normalize(v.normal); v.vertex.xyz += v.normal * displacement; #endif Displaced shadows 我们现在得到了正确置换的阴影。接收和投射阴影现在都是正确的，对于两个渲染路径都是如此。 阴影工作正常，曲面细分相对于视差贴图的另一个优势是我们自动获得了自阴影（Self-shadowing）。它不需要任何额外的工作。 With and without self-shadowing 当然，你必须牢记阴影贴图的局限性。此外，当使用 Edge 曲面细分模式时，阴影贴图的视图距离与常规摄像机不同。这意味着细分的阴影几何体并不完全匹配常规细分几何体，这可能会产生阴影伪影。曲面细分越精细，这个问题就越小。 Varying tessellation with shadows 3 剔除三角形 虽然曲面细分很好，但它并不便宜，特别是当需要高水平的曲面细分时。需要意识到的一件重要事情是，网格的每个三角形都会被细分，无论它最终是否可见。然而，可以对此做些什么。 场景中并非所有东西都会被渲染。只有位于摄像机视锥体（View Frustum）内的对象才能被看到。这些对象由 Unity 发送到 GPU，其他所有东西都被剔除。但是，如果对象的包围盒哪怕只有一小部分位于视锥体内，它的整个网格都将由 GPU 处理，从而被细分。幸运的是，有一种方法可以在细分时跳过三角形，有效地在曲面细分阶段之前剔除它们。 3.1 跳过一些三角形 曲面细分的数量由 Edge 和 Inside 曲面细分因子控制。因子 1 对应于不添加三角形。更高的因子导致更多的三角形。但也可以使用因子 0。当曲面细分因子之一为零时，原始三角形被丢弃，根本不会被渲染。 如果我们能弄清楚三角形是否位于视锥体之外，我们就可以将其曲面细分因子设置为 0，有效地在 GPU 上逐三角形执行视锥体剔除。让我们在 MyTessellation 中添加一个函数来解决这个问题。将其放在 MyPatchConstantFunction 之上，因为该函数将调用它。我们将从一个非常简单的测试开始。如果三角形的所有三个顶点都有负的 X 坐标，我们将认为它被剔除了。我们可以使用布尔值来传达这一点。 bool TriangleIsCulled (float3 p0, float3 p1, float3 p2) { return p0.x &lt; 0 &amp;&amp; p1.x &lt; 0 &amp;&amp; p2.x &lt; 0; } 在 MyPatchConstantFunction 中使用此函数来检查我们是否可以跳过三角形。如果是，将所有边因子设置为零。否则，像往常一样确定因子。 TessellationFactors MyPatchConstantFunction ( InputPatch&lt;TessellationControlPoint, 3&gt; patch ) { float3 p0 = mul(unity_ObjectToWorld, patch[0].vertex).xyz; float3 p1 = mul(unity_ObjectToWorld, patch[1].vertex).xyz; float3 p2 = mul(unity_ObjectToWorld, patch[2].vertex).xyz; TessellationFactors f; if (TriangleIsCulled(p0, p1, p2)) { f.edge[0] = f.edge[1] = f.edge[2] = f.inside = 0; } else { f.edge[0] = TessellationEdgeFactor(p1, p2); f.edge[1] = TessellationEdgeFactor(p2, p0); f.edge[2] = TessellationEdgeFactor(p0, p1); f.inside = (TessellationEdgeFactor(p1, p2) + TessellationEdgeFactor(p2, p0) + TessellationEdgeFactor(p0, p1)) * (1 / 3.0); } return f; } Culling triangles with only negative X coordinates 3.2 视锥体剔除 要执行实际的视锥体剔除，我们必须验证三角形是否位于摄像机的视锥体内。视锥体是一个金字塔，其顶部被平行于其底部的平面切断。金字塔的底部和侧面也可以由平面定义。这些平面形成一个系统，其中视锥体内的空间被认为位于所有六个裁剪平面之上。所以我们必须检查每个平面，点是否位于其上方或下方。让我们创建一个布尔函数来检查单个平面，默认返回 true。在 TriangleIsCulled 内部调用该函数，替换我们的测试代码。 bool TriangleIsBelowClipPlane (float3 p0, float3 p1, float3 p2) { return true; } bool TriangleIsCulled (float3 p0, float3 p1, float3 p2) { // return p0.x &lt; 0 &amp;&amp; p1.x &lt; 0 &amp;&amp; p2.x &lt; 0; return TriangleIsBelowClipPlane(p0, p1, p2); } 因为摄像机可能有任何位置和方向，我们无法提前对其裁剪平面做出任何假设。所以我们必须能够处理任意方向和位置的平面。一般来说，平面可以由定义其局部向上方向的法线向量以及相对于世界原点的偏移来定义。此数据可以存储在一个四分量向量中，其中 W 分量包含偏移。对应于我们之前丢弃具有负 X 坐标的三角形的测试用例的平面向量将是 (1, 0, 0, 0)。如果我们改为丢弃 X 坐标高达 2 的三角形，则向量将是 (1, 0, 0, 2)。 要弄清楚点是否位于平面上方或下方，我们可以通过点积将该点的向量投影到平面的法线向量上。如果结果为负，则它们的角度大于 90°，因此点位于平面下方。平面的偏移也必须考虑在内，方法是将其添加到计算中，例如将其设为 (px, py, pz, 1) 和平面向量之间的点积，其中 px, py 和 pz 是点的坐标。相应地调整 TriangleIsBelowClipPlane。 bool TriangleIsBelowClipPlane (float3 p0, float3 p1, float3 p2) { float4 plane = float4(1, 0, 0, 0); return dot(float4(p0, 1), plane) &lt; 0 &amp;&amp; dot(float4(p1, 1), plane) &lt; 0 &amp;&amp; dot(float4(p2, 1), plane) &lt; 0; } 摄像机的实际裁剪平面通过 UnityShaderVariables 中定义的 unity_CameraWorldClipPlanes 数组提供。它包含六个平面定义，分别用于左、右、底、顶、近和远平面。所以要使用摄像机的左平面，我们必须使用 unity_CameraWorldClipPlanes[0]。 // float4 plane = float4(1, 0, 0, 0); float4 plane = unity_CameraWorldClipPlanes[0]; 为了使 TriangleIsBelowClipPlane 适用于任何摄像机裁剪平面，添加平面索引作为附加参数，并使用它来选择适当的摄像机平面。 bool TriangleIsBelowClipPlane ( float3 p0, float3 p1, float3 p2, int planeIndex ) { float4 plane = unity_CameraWorldClipPlanes[planeIndex]; return dot(float4(p0, 1), plane) &lt; 0 &amp;&amp; dot(float4(p1, 1), plane) &lt; 0 &amp;&amp; dot(float4(p2, 1), plane) &lt; 0; } 现在我们可以在 TriangleIsCulled 内部检查所有裁剪平面。如果三角形最终位于其中任何一个之下，那么它就不可能是可见的，应该被裁剪。我们必须检查左、右、底和顶平面。近平面实际上并不需要，因为视锥体通常在摄像机后方很短的距离处汇聚成一点。所以检查近平面不值得额外努力。远平面也不必要，因为在这个距离上，曲面细分通常无论如何都不会发生。 bool TriangleIsCulled (float3 p0, float3 p1, float3 p2) { return TriangleIsBelowClipPlane(p0, p1, p2, 0) || TriangleIsBelowClipPlane(p0, p1, p2, 1) || TriangleIsBelowClipPlane(p0, p1, p2, 2) || TriangleIsBelowClipPlane(p0, p1, p2, 3); } 3.3 偏移剔除 因为我们只裁剪那些我们看不到的三角形，所以除了帧率可能存在差异外，我们不应该能够区分裁剪和不裁剪。然而，这只有在我们不置换任何顶点时才是真的。当顶点确实被置换时，即使原始三角形位于视锥体之外，置换的顶点也有可能最终位于视锥体内。在我们置换平面的情况下，你可以通过以浅角度观察平面来验证这一点，这样它的一些网格三角形最终刚好位于视图底部下方。你会很快遇到孔洞，因为三角形突然消失，而它们不应该消失。 Hole created by incorrect culling 这个问题的解决方案是在确定三角形是否位于裁剪平面下方时，将最大置换量考虑在内。这可以通过向 TriangleIsBelowClipPlane 添加偏移（Bias）来完成。与其检查点积是否小于零，不如检查它是否小于此偏移。 bool TriangleIsBelowClipPlane ( float3 p0, float3 p1, float3 p2, int planeIndex, float bias ) { float4 plane = unity_CameraWorldClipPlanes[planeIndex]; return dot(float4(p0, 1), plane) &lt; bias &amp;&amp; dot(float4(p1, 1), plane) &lt; bias &amp;&amp; dot(float4(p2, 1), plane) &lt; bias; } 我们应该对所有平面检查使用相同的偏移，所以将其作为参数添加到 TriangleIsCulled 中。 bool TriangleIsCulled (float3 p0, float3 p1, float3 p2, float bias) { return TriangleIsBelowClipPlane(p0, p1, p2, 0, bias) || TriangleIsBelowClipPlane(p0, p1, p2, 1, bias) || TriangleIsBelowClipPlane(p0, p1, p2, 2, bias) || TriangleIsBelowClipPlane(p0, p1, p2, 3, bias); } 让我们在 MyPatchConstantFunction 中使用偏移 1，看看会发生什么。 float bias = 1; if (TriangleIsCulled(p0, p1, p2, bias)) { f.edge[0] = f.edge[1] = f.edge[2] = f.inside = 0; } Using a positive bias 正偏移有效地将裁剪平面向上推，减小了视锥体的大小。结果，当三角形靠近视图边缘时，它们会被过快地裁剪。负偏移具有相反的效果，因此位于视锥体之外但仍在其附近的三角形不会被裁剪。所以当使用顶点置换时，我们必须使用负偏移。由于我们在任何维度上的最大置换等于置换强度的一半，这就是我们需要的负偏移。 float bias = 0; #if VERTEX_DISPLACEMENT bias = -0.5 * _DisplacementStrength; #endif if (TriangleIsCulled(p0, p1, p2, bias)) { f.edge[0] = f.edge[1] = f.edge[2] = f.inside = 0; } 我们现在正在尽可能多地裁剪三角形，同时保证永远不会出现孔洞。当然，确定我们是否应该裁剪三角形也需要工作，所以它不会提高完全在视图中的网格的性能，实际上会让它变得更糟。但是当你渲染具有大量曲面细分的大型网格，并且它们通常只是部分可见时，你最终可以显着提高帧率。 曲面细分的介绍到此结束。现在你知道如何细分三角形以及如何通过置换贴图添加几何细节。这并不是你可以用曲面细分做的唯一事情。例如，还有 PN 三角形、Phong 曲面细分、程序化置换等等。" }, { "title": "曲面细分(翻译二十二)", "url": "/posts/unity-tessellation/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-27 10:00:02 +0800", "content": "创建 Hull Shader 和 Domain Shader。 细分三角形。 控制细分的方式。 1 Hull 和 Domain 曲面细分是将物体切割成更小部分的艺术。在我们的案例中，我们将细分三角形，从而得到覆盖相同空间的小三角形。这使得为几何体添加更多细节成为可能，尽管在本教程中我们将重点关注曲面细分过程本身。 GPU 能够细分喂给它进行渲染的三角形。它这样做有多种原因，例如当三角形的一部分被剪裁时。我们无法控制这一点，但也有一个我们可以配置的曲面细分阶段。该阶段位于顶点（Vertex）和片元（Fragment）着色器阶段之间。但它不仅仅是向我们的 Shader 添加另一个程序那么简单。我们需要一个 Hull 程序和一个 Domain 程序。 Inside a hull shader 1.1 创建曲面细分 Shader 第一步是创建一个启用了曲面细分的 Shader。让我们将需要的代码放在它自己的文件 MyTessellation.cginc 中，并带上包含保护。 #if !defined(TESSELLATION_INCLUDED) #define TESSELLATION_INCLUDED #endif 为了清楚地看到三角形被细分，我们将使用 Flat Wireframe Shader。复制该 Shader，重命名为 Tessellation Shader 并调整其菜单名称。 Shader \"Custom/Tessellation\" { … } 使用曲面细分时的最低 Shader Target 级别是 4.6。如果我们不手动设置，Unity 将发出警告并自动使用该级别。我们将为 forward base、additive 通道以及 deferred 通道添加曲面细分阶段。还要在这些通道中包含 MyTessellation，放在 MyFlatWireframe 之后。 #pragma target 4.6 … #include \"MyFlatWireframe.cginc\" #include \"MyTessellation.cginc\" 那 Shadow 通道呢？ 在渲染阴影时也可以使用曲面细分，但在本教程中我们不会这样做。 创建一个依赖于此 Shader 的材质，并在场景中添加一个使用该材质的 Quad。我将材质设为灰色，这样它就不会太亮，就像 Flat Wireframe 材质一样。 A quad 请注意，它由两个等腰直角三角形组成。短边长度为 1，而长对角线长度为 $\\sqrt{2}$。 1.2 Hull Shader 与几何着色器（Geometry Shader）类似，曲面细分阶段非常灵活，可以处理三角形、四边形或等值线。我们必须告诉它它要处理什么表面，并提供必要的数据。这是 Hull 程序的工作。在 MyTessellation 中添加一个 Hull 程序，从一个不执行任何操作的 void 函数开始。 void MyHullProgram () {} Hull 程序操作一个 Surface Patch，它作为参数传递给程序。我们必须添加一个 InputPatch 参数来实现这一点。 void MyHullProgram (InputPatch patch) {} Patch 是网格顶点的集合。就像我们为几何函数的流参数所做的那样，我们必须指定顶点的数据格式。我们现在将使用 VertexData 结构。 void MyHullProgram (InputPatch&lt;VertexData&gt; patch) {} **不应该是 InputPatch 吗？** 由于 Hull 阶段在 Vertex 阶段之后，逻辑上 Hull 函数的输入类型必须与 Vertex 函数的输出类型匹配。这是事实，但我们现在将忽略这一点。 由于我们处理的是三角形，每个 Patch 将包含三个顶点。这个数量必须指定为 InputPatch 的第二个模板参数。 void MyHullProgram (InputPatch&lt;VertexData, 3&gt; patch) {} Hull 程序的工作是将所需的顶点数据传递给曲面细分阶段。虽然它被喂给了一个完整的 Patch，但函数一次只能输出一个顶点。它将针对 Patch 中的每个顶点调用一次，并带有一个额外的参数，指定它应该处理哪个控制点（顶点）。该参数是一个带有 SV_OutputControlPointID 语义的无符号整数。 void MyHullProgram ( InputPatch&lt;VertexData, 3&gt; patch, uint id : SV_OutputControlPointID ) {} 只需将 Patch 像数组一样建立索引并返回所需的元素。 VertexData MyHullProgram ( InputPatch&lt;VertexData, 3&gt; patch, uint id : SV_OutputControlPointID ) { return patch[id]; } 这看起来像是一个功能性的程序，所以让我们添加一个编译器指令来将其用作 Hull Shader。为涉及的三个 Shader 通道都这样做。 #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #pragma hull MyHullProgram #pragma geometry MyGeometryProgram 这将产生一些编译器错误，抱怨我们没有正确配置 Hull Shader。与几何函数一样，它需要属性来配置。首先，我们必须明确告诉它它正在处理三角形。这是通过 UNITY_domain 属性完成的，使用 tri 作为参数。 [UNITY_domain(\"tri\")] VertexData MyHullProgram … 这还不够。我们还必须明确指定我们每个 Patch 输出三个控制点，三角形的每个角对应一个。 [UNITY_domain(\"tri\")] [UNITY_outputcontrolpoints(3)] VertexData MyHullProgram … 当 GPU 创建新的三角形时，它需要知道我们要将它们定义为顺时针还是逆时针。与 Unity 中的所有其他三角形一样，它们应该是顺时针的。这通过 UNITY_outputtopology 属性控制。其参数应为 triangle_cw。 [UNITY_domain(\"tri\")] [UNITY_outputcontrolpoints(3)] [UNITY_outputtopology(\"triangle_cw\")] VertexData MyHullProgram … 还需要告诉 GPU 它应该如何切割 Patch，这通过 UNITY_partitioning 属性完成。有几种不同的划分方法，我们稍后将研究它们。目前，只需使用 integer 模式。 [UNITY_domain(\"tri\")] [UNITY_outputcontrolpoints(3)] [UNITY_outputtopology(\"triangle_cw\")] [UNITY_partitioning(\"integer\")] VertexData MyHullProgram … 除了划分方法之外，GPU 还必须知道 Patch 应该被切割成多少部分。这不是一个常数值，它可以针对每个 Patch 而变化。我们必须提供一个函数来评估这一点，称为 Patch Constant 函数。让我们假设我们有这样一个函数，名为 MyPatchConstantFunction。 [UNITY_domain(\"tri\")] [UNITY_outputcontrolpoints(3)] [UNITY_outputtopology(\"triangle_cw\")] [UNITY_partitioning(\"integer\")] [UNITY_patchconstantfunc(\"MyPatchConstantFunction\")] VertexData MyHullProgram … 1.3 Patch Constant 函数 Patch 如何被细分是 Patch 的属性。这意味着 Patch Constant 函数对每个 Patch 仅调用一次，而不是对每个控制点调用一次。这就是为什么它被称为 Constant 函数，在整个 Patch 上保持不变。实际上，这个函数是与 MyHullProgram 并行运行的一个子阶段。 Inside a hull shader 为了确定如何细分三角形，GPU 使用四个曲面细分因子（Tessellation Factors）。三角形 Patch 的每条边都有一个因子。三角形内部也有一个因子。三个边向量必须作为具有 SV_TessFactor 语义的 float 数组传递。内部因子使用 SV_InsideTessFactor 语义。让我们为此创建一个结构。 struct TessellationFactors { float edge[3] : SV_TessFactor; float inside : SV_InsideTessFactor; }; Patch Constant 函数将 Patch 作为输入参数并输出曲面细分因子。现在让我们创建这个缺失的函数。只需将其所有因子设置为 1。这将指示曲面细分阶段不细分 Patch。 TessellationFactors MyPatchConstantFunction (InputPatch&lt;VertexData, 3&gt; patch) { TessellationFactors f; f.edge[0] = 1; f.edge[1] = 1; f.edge[2] = 1; f.inside = 1; return f; } 1.4 Domain Shader 此时，Shader 编译器会抱怨 Shader 不能在没有曲面细分评估着色器（Tessellation Evaluation Shader）的情况下拥有曲面细分控制着色器（Tessellation Control Shader）。Hull Shader 只是我们需要让曲面细分工作的一部分。一旦曲面细分阶段确定了 Patch 应该如何细分，就轮到 Domain Shader（在 HLSL 中也称为几何评估着色器）来评估结果并生成最终三角形的顶点。所以让我们为我们的 Domain Shader 创建一个函数，再次从一个存根开始。 void MyDomainProgram () {} Hull 和 Domain Shader 都作用于同一个域（Domain），即三角形。我们再次通过 UNITY_domain 属性发出信号。 [UNITY_domain(\"tri\")] void MyDomainProgram () {} Domain 程序被喂给了所使用的曲面细分因子，以及原始 Patch，在这种情况下其类型为 OutputPatch。 [UNITY_domain(\"tri\")] void MyDomainProgram ( TessellationFactors factors, OutputPatch&lt;VertexData, 3&gt; patch ) {} 虽然曲面细分阶段决定了 Patch 应该如何细分，但它并不生成任何新的顶点。相反，它为这些顶点提供了重心坐标（Barycentric Coordinates）。由 Domain Shader 使用这些坐标来导出最终顶点。为了使这成为可能，Domain 函数对每个顶点调用一次，并为其提供重心坐标。它们具有 SV_DomainLocation 语义。 [UNITY_domain(\"tri\")] void MyDomainProgram ( TessellationFactors factors, OutputPatch&lt;VertexData, 3&gt; patch, float3 barycentricCoordinates : SV_DomainLocation ) {} 在函数内部，我们必须生成最终的顶点数据。 [UNITY_domain(\"tri\")] void MyDomainProgram ( TessellationFactors factors, OutputPatch&lt;VertexData, 3&gt; patch, float3 barycentricCoordinates : SV_DomainLocation ) { VertexData data; } 要找到此顶点的中心位置，我们必须使用重心坐标在原始三角形域上进行插值。X、Y 和 Z 坐标决定了第一、第二和第三个控制点的权重。 VertexData data; data.vertex = patch[0].vertex * barycentricCoordinates.x + patch[1].vertex * barycentricCoordinates.y + patch[2].vertex * barycentricCoordinates.z; 我们必须以同样的方式插值所有顶点数据。让我们为此定义一个方便的宏，它可以用于所有向量大小。 #define MY_DOMAIN_PROGRAM_INTERPOLATE(fieldName) data.fieldName = patch[0].fieldName * barycentricCoordinates.x + patch[1].fieldName * barycentricCoordinates.y + patch[2].fieldName * barycentricCoordinates.z; MY_DOMAIN_PROGRAM_INTERPOLATE(vertex) 除了位置，还要插值法线、切线和所有 UV 坐标。 MY_DOMAIN_PROGRAM_INTERPOLATE(vertex) MY_DOMAIN_PROGRAM_INTERPOLATE(normal) MY_DOMAIN_PROGRAM_INTERPOLATE(tangent) MY_DOMAIN_PROGRAM_INTERPOLATE(uv) MY_DOMAIN_PROGRAM_INTERPOLATE(uv1) MY_DOMAIN_PROGRAM_INTERPOLATE(uv2) 唯一不插值的是实例 ID。由于 Unity 不支持同时使用 GPU Instancing 和曲面细分，因此没有必要复制此 ID。要防止编译器错误，请从三个 Shader 通道中删除 multi-compile 指令。这也将从 Shader 的 GUI 中删除 Instancing 选项。 // #pragma multi_compile_instancing // #pragma instancing_options lodfade force_same_maxcount_for_gl 是否可以同时使用 Instancing 和曲面细分？ 目前不行。请记住，GPU Instancing 在多次渲染相同对象时非常有用。由于曲面细分成本很高且是关于添加细节的，它们通常不是一个好的组合。如果你想让某些物体在近处使用曲面细分，你可以使用 LOD Group。让 LOD 0 使用非实例化的曲面细分材质，而所有其他 LOD 级别使用实例化的非曲面细分材质。 我们现在有了一个新顶点，它将在这一阶段之后被发送到几何程序或插值器。但这些程序期望的是 InterpolatorsVertex 数据，而不是 VertexData。为了解决这个问题，我们让 Domain Shader 接管原始顶点程序的职责。这是通过在其中调用 MyVertexProgram——就像任何其他函数一样——并返回其结果来完成的。 [UNITY_domain(\"tri\")] InterpolatorsVertex MyDomainProgram ( TessellationFactors factors, OutputPatch&lt;VertexData, 3&gt; patch, float3 barycentricCoordinates : SV_DomainLocation ) { … return MyVertexProgram(data); } 现在我们可以将 Domain Shader 添加到我们的三个 Shader 通道中，但我们仍然会得到错误。 #pragma hull MyHullProgram #pragma domain MyDomainProgram 1.5 控制点（Control Points） MyVertexProgram 只需调用一次，只是我们改变了发生调用的位置。但我们仍然必须指定一个顶点程序在顶点着色器阶段调用，该阶段位于 Hull Shader 之前。此时我们不需要做任何事情，所以我们可以使用一个简单地原样传递顶点数据的函数。 VertexData MyTessellationVertexProgram (VertexData v) { return v; } 让我们的三个 Shader 通道从现在起使用此函数作为其顶点程序。 #pragma vertex MyTessellationVertexProgram 这将产生另一个编译器错误，抱怨位置语义的重复使用。为了使其工作，我们必须为顶点程序使用一个替代的输出结构，该结构使用 INTERNALTESSPOS 语义作为顶点位置。结构的其余部分与 VertexData 相同，只是它永远没有实例 ID。由于此顶点数据用作曲面细分过程的控制点，让我们将其命名为 TessellationControlPoint。 struct TessellationControlPoint { float4 vertex : INTERNALTESSPOS; float3 normal : NORMAL; float4 tangent : TANGENT; float2 uv : TEXCOORD0; float2 uv1 : TEXCOORD1; float2 uv2 : TEXCOORD2; }; 更改 MyTessellationVertexProgram，使其将顶点数据放入控制点结构并返回它。 TessellationControlPoint MyTessellationVertexProgram (VertexData v) { TessellationControlPoint p; p.vertex = v.vertex; p.normal = v.normal; p.tangent = v.tangent; p.uv = v.uv; p.uv1 = v.uv1; p.uv2 = v.uv2; return p; } 接下来，MyHullProgram 也必须更改，以便它使用 TessellationControlPoint 而不是 VertexData。只有其参数类型需要更改。 TessellationControlPoint MyHullProgram ( InputPatch&lt;TessellationControlPoint, 3&gt; patch, uint id : SV_OutputControlPointID ) { return patch[id]; } Patch Constant 函数也是如此。 TessellationFactors MyPatchConstantFunction ( InputPatch&lt;TessellationControlPoint, 3&gt; patch ) { … } Domain 程序的参数类型也必须更改。 InterpolatorsVertex MyDomainProgram ( TessellationFactors factors, OutputPatch&lt;TessellationControlPoint, 3&gt; patch, float3 barycentricCoordinates : SV_DomainLocation ) { … } 此时我们终于有了一个正确的曲面细分 Shader。它应该可以编译并像以前一样渲染 Quad。它还没有被细分，因为曲面细分因子始终为 1。 2 细分三角形 曲面细分设置的重点是我们可以细分 Patch。这允许我们将单个三角形替换为小三角形的集合。我们现在就来做这件事。 2.1 曲面细分因子 三角形 Patch 如何被细分由其曲面细分因子控制。我们在 MyPatchConstantFunction 中确定这些因子。目前，我们将它们都设置为 1，这不会产生视觉变化。Hull、曲面细分和 Domain Shader 阶段正在工作，但它们正在传递原始顶点数据并且不生成任何新内容。要更改此设置，请将所有因子设置为 2。 TessellationFactors MyPatchConstantFunction ( InputPatch&lt;TessellationControlPoint, 3&gt; patch ) { TessellationFactors f; f.edge[0] = 2; f.edge[1] = 2; f.edge[2] = 2; f.inside = 2; return f; } Tessellation factors 2 三角形现在确实被细分了。它们的所有边都被分成了两个子边，导致每个三角形增加了三个新顶点。此外，在每个三角形的中心还添加了另一个顶点。这使得为每条原始边生成两个三角形成为可能，因此原始三角形已被六个较小的三角形替换。由于 Quad 由两个三角形组成，我们现在总共得到 12 个三角形。 如果你改为将所有因子设置为 3，每条边将被分成三个子边。在这种情况下，不会有中心顶点。相反，在原始三角形内部添加了三个顶点，形成一个较小的内部三角形。外边缘将通过三角形带（Triangle Strips）连接到这个内部三角形。 Tessellation factors 3 当曲面细分因子为偶数时，将有一个单一的中心顶点。当它们为奇数时，将有一个中心三角形。如果我们使用更大的曲面细分因子，我们最终会得到多个嵌套的三角形。向中心每迈进一阶，三角形被细分的数量就会减少二，直到我们最终得到一或零个子边。 Tessellation factors 4–7 2.2 不同的边和内部因子 三角形如何被细分由内部曲面细分因子控制。边因子可用于覆盖其各自边被细分的数量。这仅影响原始 Patch 边缘，不影响生成的内部三角形。为了清楚地看到这一点，将内部因子设置为 7，同时保持边因子为 1。 f.edge[0] = 1; f.edge[1] = 1; f.edge[2] = 1; f.inside = 7; Factor 7 inside, but 1 outside 实际上，三角形使用因子 7 进行细分，之后最外圈的三角形被丢弃。然后每条边使用其自己的因子进行细分，之后生成一个三角形带以将边和内部三角形缝合在一起。 边因子也有可能大于内部因子。例如，将边因子设置为 7，同时将内部因子保持为 1。 f.edge[0] = 7; f.edge[1] = 7; f.edge[2] = 7; f.inside = 1; Factor 1 inside, but 7 outside 在这种情况下，内部因子被迫表现得像 2，因为否则无法生成新的三角形。 2.3 可变因子 硬编码的曲面细分因子不是很有用。所以让我们把它变成可配置的，从一个单一的统一（Uniform）因子开始。 float _TessellationUniform; … TessellationFactors MyPatchConstantFunction ( InputPatch&lt;TessellationControlPoint, 3&gt; patch ) { TessellationFactors f; f.edge[0] = _TessellationUniform; f.edge[1] = _TessellationUniform; f.edge[2] = _TessellationUniform; f.inside = _TessellationUniform; return f; } 为我们的 Shader 添加一个属性。将其范围设置为 1–64。无论我们想使用多高的因子，硬件在每个 Patch 上的细分限制都是 64。 _TessellationUniform (\"Tessellation Uniform\", Range(1, 64)) = 1 为了能够编辑此因子，请在 MyLightingShaderGUI 中添加一个 DoTessellation 方法，将其显示在自己的部分中。 void DoTessellation () { GUILayout.Label(\"Tessellation\", EditorStyles.boldLabel); EditorGUI.indentLevel += 2; editor.ShaderProperty( FindProperty(\"_TessellationUniform\"), MakeLabel(\"Uniform\") ); EditorGUI.indentLevel -= 2; } 在 OnGUI 内部，在渲染模式和线框（Wireframe）部分之间调用此方法。仅在所需属性存在时才执行此操作。 public override void OnGUI ( MaterialEditor editor, MaterialProperty[] properties ) { … DoRenderingMode(); if (target.HasProperty(\"_TessellationUniform\")) { DoTessellation(); } if (target.HasProperty(\"_WireframeColor\")) { DoWireframe(); } … } Configurable uniform tessellation 2.4 分数因子（Fractional Factors） 即使我们使用 float 来设置曲面细分因子，我们最终总是在每条边上得到整数等效的细分。这是因为我们正在使用 integer 划分模式。虽然它是查看曲面细分如何工作的好模式，但它阻止了我们在细分级别之间平滑过渡。幸运的是，还有分数划分模式。让我们将模式更改为 fractional_odd。 [UNITY_domain(\"tri\")] [UNITY_outputcontrolpoints(3)] [UNITY_outputtopology(\"triangle_cw\")] [UNITY_partitioning(\"fractional_odd\")] [UNITY_patchconstantfunc(\"MyPatchConstantFunction\")] TessellationControlPoint MyHullProgram … Fractional odd partitioning 当使用完整的奇数因子时，fractional_odd 划分模式产生与 integer 模式相同的结果。但在奇数因子之间转换时，额外的边细分将分离开来并增长，或者缩小并合并。这意味着边不再总是被分成等长的段。这种方法的优点是细分级别之间的过渡现在是平滑的。 也可以使用 fractional_even 模式。它的工作方式相同，只是它基于偶数因子。 Fractional even partitioning 通常使用 fractional_odd 模式，因为它处理因子 1，而 fractional_even 模式被迫使用最低级别 2。 3 曲面细分启发式（Tessellation Heuristics） 什么是最好的曲面细分因子？这是在使用曲面细分时你必须问自己的主要问题。这个问题没有唯一的客观答案。通常，你能做的最好的事情就是提出一些度量标准，作为一个能够产生良好结果的启发式方法。在本教程中，我们将支持两种简单的方法。 3.1 边因子 虽然必须按边提供曲面细分因子，但你不必直接根据边来确定因子。例如，你可以针对每个顶点确定因子，然后对每条边求平均值。也许因子存储在纹理中。无论如何，有一个单独的函数来确定给定一条边的两个控制点的因子是很方便的。创建这样一个函数，目前只是返回统一值。 float TessellationEdgeFactor ( TessellationControlPoint cp0, TessellationControlPoint cp1 ) { return _TessellationUniform; } 在 MyPatchConstantFunction 内部使用此函数处理边因子。 TessellationFactors MyPatchConstantFunction ( InputPatch&lt;TessellationControlPoint, 3&gt; patch ) { TessellationFactors f; f.edge[0] = TessellationEdgeFactor(patch[1], patch[2]); f.edge[1] = TessellationEdgeFactor(patch[2], patch[0]); f.edge[2] = TessellationEdgeFactor(patch[0], patch[1]); f.inside = _TessellationUniform; return f; } 对于内部因子，我们将简单地使用边因子的平均值。 f.inside = (f.edge[0] + f.edge[1] + f.edge[2]) * (1 / 3.0); 3.2 边长（Edge Length） 由于边曲面细分因子控制我们对原始三角形边的细分程度，因此根据这些边的长度来确定此因子是有意义的。例如，我们可以指定一个理想的三角形边长。如果我们最终得到的三角形边长超过该长度，我们就应该按理想长度对其进行细分。为此添加一个变量。 float _TessellationUniform; float _TessellationEdgeLength; 还要添加一个属性。让我们使用 0.1 到 1 的范围，默认值为 0.5。这是世界空间单位。 _TessellationUniform (\"Tessellation Uniform\", Range(1, 64)) = 1 _TessellationEdgeLength (\"Tessellation Edge Length\", Range(0.1, 1)) = 0.5 我们需要一个 Shader Feature 来实现在统一和基于边的曲面细分之间切换。在我们的三个通道中都添加所需的指令，使用 _TESSELLATION_EDGE 关键字。 #pragma shader_feature _TESSELLATION_EDGE 接下来，在 MyLightingShaderGUI 中添加一个枚举类型来表示曲面细分模式。 enum TessellationMode { Uniform, Edge } 然后调整 DoTessellation，使其可以使用枚举弹出框在两种模式之间切换。它的工作方式类似于 DoSmoothness 控制光滑度模式的方式。在这种情况下，统一是默认模式，不需要关键字。 void DoTessellation () { GUILayout.Label(\"Tessellation\", EditorStyles.boldLabel); EditorGUI.indentLevel += 2; TessellationMode mode = TessellationMode.Uniform; if (IsKeywordEnabled(\"_TESSELLATION_EDGE\")) { mode = TessellationMode.Edge; } EditorGUI.BeginChangeCheck(); mode = (TessellationMode)EditorGUILayout.EnumPopup( MakeLabel(\"Mode\"), mode ); if (EditorGUI.EndChangeCheck()) { RecordAction(\"Tessellation Mode\"); SetKeyword(\"_TESSELLATION_EDGE\", mode == TessellationMode.Edge); } if (mode == TessellationMode.Uniform) { editor.ShaderProperty( FindProperty(\"_TessellationUniform\"), MakeLabel(\"Uniform\") ); } else { editor.ShaderProperty( FindProperty(\"_TessellationEdgeLength\"), MakeLabel(\"Edge Length\") ); } EditorGUI.indentLevel -= 2; } Using edge mode 现在我们必须调整 TessellationEdgeFactor。当定义了 _TESSELLATION_EDGE 时，确定两个点的世界位置，然后计算它们之间的距离。这是世界空间中的边长。边因子等于此长度除以理想长度。 float TessellationEdgeFactor ( TessellationControlPoint cp0, TessellationControlPoint cp1 ) { #if defined(_TESSELLATION_EDGE) float3 p0 = mul(unity_ObjectToWorld, float4(cp0.vertex.xyz, 1)).xyz; float3 p1 = mul(unity_ObjectToWorld, float4(cp1.vertex.xyz, 1)).xyz; float edgeLength = distance(p0, p1); return edgeLength / _TessellationEdgeLength; #else return _TessellationUniform; #endif } Different quad scales, same desired edge length 因为我们现在使用边长来确定边的曲面细分因子，我们最终可能会为每条边得到不同的因子。你可以看到 Quad 发生了这种情况，因为对角线边缘比其他边缘长。当使用 Quad 的非均匀缩放，将其在一个维度上拉伸时，这一点也变得显而易见。 Stretched quad 为了使其工作，必不可少的是共享一条边的 Patch 最终都为该边使用相同的曲面细分因子。否则，生成的顶点将无法沿该边匹配，这可能会在网格中产生可见的间隙。在我们的案例中，我们对所有边都使用相同的逻辑。唯一的区别可能是控制点参数的顺序。由于浮点限制，这在技术上可能会产生不同的因子，但差异微小到无法察觉。 3.3 屏幕空间中的边长 虽然我们现在可以在世界空间中控制三角形边长，但这并不对应于它们在屏幕空间中的表现。曲面细分的重点是在需要时添加更多三角形。因此，我们不想细分那些看起来已经很小的三角形。所以让我们改用屏幕空间边长。 首先，更改我们的边长属性的范围。我们将使用像素而不是世界单位，因此 5–100 这样的范围更有意义。 _TessellationEdgeLength (\"Tessellation Edge Length\", Range(5, 100)) = 50 将世界空间计算替换为它们的屏幕空间等效项。为此，点必须转换为裁剪空间（Clip Space）而不是世界空间。然后在 2D 中确定它们的距离，使用它们的 X 和 Y 坐标，除以它们的 W 坐标以将它们投影到屏幕上。 float4 p0 = UnityObjectToClipPos(cp0.vertex); float4 p1 = UnityObjectToClipPos(cp1.vertex); float edgeLength = distance(p0.xy / p0.w, p1.xy / p1.w); return edgeLength / _TessellationEdgeLength; 现在我们在裁剪空间中得到了一个结果，裁剪空间是一个尺寸为 2 的统一立方体，正好适合显示。要转换为像素，我们必须按以像素为单位的显示尺寸进行缩放。实际上，由于显示器很少是正方形的，为了获得最准确的结果，我们应该在确定距离之前分别缩放 X 和 Y 坐标。但让我们仅通过缩放屏幕高度来满足要求，看看它看起来如何。 return edgeLength * _ScreenParams.y / _TessellationEdgeLength; Same world size, different screen size 我们的三角形边现在根据它们渲染的大小进行细分。位置、旋转和缩放都会相对于摄像机影响这一点。因此，当物体处于运动状态时，曲面细分的数量会发生变化。 我们不应该使用屏幕高度的一半吗？ 由于裁剪空间立方体的范围是 -1 到 1，两个单位对应于显示的完整高度（和宽度）。这意味着我们最终得到了实际尺寸的两倍，高估了我们的边缘有多大。结果是我们实际上瞄准了预期的边长的一半。至少，对于完全垂直的边缘是这种情况，因为我们无论如何都没有使用确切的屏幕维度。使用屏幕高度的主要目的是使曲面细分依赖于显示分辨率。边长是否与我们的滑块的确切值匹配其实并不重要。 3.4 使用视图距离 纯粹依赖边缘的视觉长度的一个缺点是，世界空间中较长的边缘在屏幕空间中最终可能会变得非常小。这可能会导致这些边缘根本不被细分，而其他边缘被大量细分。当曲面细分用于近距离添加细节或生成复杂的轮廓时，这是不可取的。 另一种方法是回到使用世界空间边长，但根据视图距离调整因子。物体距离越远，它在视觉上看起来就越小，因此需要的曲面细分就越少。因此，将边长除以边缘与摄像机之间的距离。我们可以使用边缘的中点来确定这个距离。 float3 p0 = mul(unity_ObjectToWorld, float4(cp0.vertex.xyz, 1)).xyz; float3 p1 = mul(unity_ObjectToWorld, float4(cp1.vertex.xyz, 1)).xyz; float edgeLength = distance(p0, p1); float3 edgeCenter = (p0 + p1) * 0.5; float viewDistance = distance(edgeCenter, _WorldSpaceCameraPos); return edgeLength / (_TessellationEdgeLength * viewDistance); 我们仍然可以通过简单地将屏幕高度计入其中并保持我们的 5–100 滑块范围，使曲面细分依赖于显示尺寸。请注意，这些值不再直接对应于显示像素。当你改变摄像机的视野（Field of View）时，这一点非常明显，这根本不会影响曲面细分。因此，这种简单的方法不适用于使用可变视野的游戏，例如放大和缩小。 return edgeLength * _ScreenParams.y / (_TessellationEdgeLength * viewDistance); Based on edge length and view distance 3.5 使用正确的内部因子 虽然此时曲面细分看起来运行良好，但内部曲面细分因子似乎有些奇怪。至少，在使用 OpenGL Core 时是这种情况。在使用统一的 Quad 时并不那么明显，但在使用变形的立方体时，它变得显而易见。 Cube with incorrect inner factors 在立方体的情况下，组成面的两个三角形各自得到了截然不同的内部曲面细分因子。Quad 和立方体面之间的唯一区别是三角形顶点的定义顺序。Unity 的默认立方体不使用对称的三角形布局，而 Quad 则是。这表明边的顺序显然影响了内部曲面细分因子。然而，我们只是取边因子的平均值，所以它们的顺序不应该有影响。一定是其他地方出了问题。 让我们做一些看起来毫无意义的事情，在计算内部因子时再次明确调用 TessellationEdgeFactors 函数。从逻辑上讲，这不应该产生差异，因为我们最终只是执行了两次完全相同的计算。Shader 编译器肯定会优化掉它。 // f.inside = (f.edge[0] + f.edge[1] + f.edge[2]) * (1 / 3.0); f.inside = (TessellationEdgeFactor(patch[1], patch[2]) + TessellationEdgeFactor(patch[2], patch[0]) + TessellationEdgeFactor(patch[0], patch[1])) * (1 / 3.0); Cube with correct inner factors 显然，它确实产生了差异，因为两个面三角形现在最终使用了几乎相同的内部因子。这是怎么回事？ Patch Constant 函数与 Hull Shader 的其余部分并行调用。但实际上它可能比这更复杂。Shader 编译器也能够并行化边因子的计算。MyPatchConstantFunction 内部的代码被拆开并部分重复，取而代之的是一个并行计算三个边因子的分叉进程。一旦所有三个进程完成，它们的结果就会合并并用于计算内部因子。 编译器是否决定分叉进程不应影响我们的 Shader 的结果，只影响其性能。不幸的是，在为 OpenGL Core 生成的代码中存在一个 Bug。在计算内部因子时，只使用了第三个边因子，而不是使用三个边因子。数据就在那里，只是它访问了三次索引 2，而不是索引 0、1 和 2。所以我们总是最终得到一个等于第三个边因子的内部因子。 在 Patch Constant 函数的情况下，Shader 编译器优先考虑并行化。它尽可能早地拆分进程，之后它无法再优化掉对 TessellationEdgeFactor 的重复调用。我们最终得到三个进程，每个进程都计算两个点的世界位置、距离和最终因子。然后还有一个计算内部因子的进程，它现在也必须计算三个顶点的世界位置，以及所涉及的所有距离和因子。由于我们现在为内部因子做了所有这些工作，那么为边因子分别执行部分相同的工作就没有意义了。 事实证明，如果我们先计算顶点的世界位置，然后分别为边因子和内部因子调用 TessellationEdgeFactor，Shader 编译器会决定不为每个边因子分叉单独的进程。我们最终得到一个计算所有内容的单一进程。在这种情况下，Shader 编译器确实优化掉了对 TessellationEdgeFactor 的重复调用。 float TessellationEdgeFactor (float3 p0, float3 p1) { #if defined(_TESSELLATION_EDGE) … #else return _TessellationUniform; #endif } TessellationFactors MyPatchConstantFunction ( InputPatch&lt;TessellationControlPoint, 3&gt; patch ) { float3 p0 = mul(unity_ObjectToWorld, patch[0].vertex).xyz; float3 p1 = mul(unity_ObjectToWorld, patch[1].vertex).xyz; float3 p2 = mul(unity_ObjectToWorld, patch[2].vertex).xyz; TessellationFactors f; f.edge[0] = TessellationEdgeFactor(p1, p2); f.edge[1] = TessellationEdgeFactor(p2, p0); f.edge[2] = TessellationEdgeFactor(p0, p1); f.inside = (TessellationEdgeFactor(p1, p2) + TessellationEdgeFactor(p2, p0) + TessellationEdgeFactor(p0, p1)) * (1 / 3.0); return f; } 此时我们可以细分三角形，但我们还没有利用这种能力做任何事情。曲面细分 演示了如何使用曲面细分来使表面变形。" }, { "title": "平面着色与线框着色(翻译二十一)", "url": "/posts/flat-and-wireframe-shading/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-26 12:00:00 +0800", "content": "使用屏幕空间导数获取三角形法线。 通过几何着色器做同样的事情。 使用生成的重心坐标创建线框。 让线框宽度固定并可配置。 导数与几何 (Derivatives and Geometry) 1. 平面着色 (Flat Shading) 网格由三角形组成，根据定义它们是平坦的。我们使用表面法线向量来增加曲率的错觉。这使得创建代表看似光滑表面的网格成为可能。然而，有时你实际上想显示平坦的三角形，无论是为了风格还是为了更好地查看网格的拓扑结构。 为了让三角形看起来像实际那样平坦，我们必须使用实际三角形的表面法线。这将给网格带来刻面外观，称为平面着色 (flat shading)。这可以通过使三角形三个顶点的法线向量等于三角形的法线向量来实现。这使得在三角形之间共享顶点成为不可能，因为那时它们也会共享法线。所以我们最终会得到更多的网格数据。如果我们可以继续共享顶点会很方便。此外，如果我们可以对任何网格使用平面着色材质，覆盖其原始法线（如果有的话），那就太好了。 除了平面着色，显示网格线框也很有用或很时尚。这使得网格的拓扑结构更加明显。理想情况下，我们可以在单个 pass 中使用自定义材质同时进行平面着色和线框渲染，适用于任何网格。要创建这样的材质，我们需要一个新的着色器。我们将使用渲染系列第 20 部分的最终着色器作为基础。复制 My First Lighting Shader 并将其名称更改为 Flat Wireframe。 Shader \"Custom/Flat Wireframe\" { ... } 我们不是已经在编辑器中看到了线框吗？ 我们确实可以在场景视图中看到线框，但在游戏视图和构建版本中看不到。所以如果你想在场景视图之外看到线框，你必须使用自定义解决方案。此外，场景视图仅显示原始网格的线框，无论着色器渲染什么其他东西。所以它不适用于曲面细分的顶点位移。 1.1 导数指令 (Derivative Instructions) 因为三角形是平坦的，它们的表面法线在其表面上的每一点都是相同的。因此，为三角形渲染的每个片段都应该使用相同的法线向量。但是我们目前不知道这个向量是什么。在顶点程序中，我们只能访问存储在网格中的顶点数据，这些数据是单独处理的。这里存储的法线向量对我们没有用，除非它被设计为代表三角形的法线。而在片段程序中，我们只能访问插值后的顶点法线。 为了确定表面法线，我们需要知道三角形在世界空间中的方向。这可以通过三角形顶点的位置来确定。假设三角形没有退化，其法线向量等于三角形两条边的归一化叉积。如果它是退化的，那么它反正也不会被渲染。所以给定三角形的顶点 $a, b, c$（逆时针顺序）。其法线向量为 $n = (c - a)\\times(b - a)$。将其归一化给我们最终的单位法线向量 $\\hat{n} = \\frac{n} {\\lvert n \\rvert}$。 推导三角形法线 我们实际上不需要使用三角形的顶点。任何位于三角形平面内的三个点都可以，只要这些点也形成一个三角形。具体来说，我们只需要两个位于三角形平面内的向量，只要它们不平行且大于零。 一种可能性是使用对应于渲染片段的世界位置的点。例如，我们当前正在渲染的片段的世界位置 $p_0$，其右侧片段的位置，以及其上方片段的位置（在屏幕空间中）。 使用片段的世界位置 如果我们可以访问相邻片段的世界位置，这就行得通。没有办法直接访问相邻片段的数据，但我们可以访问此数据的屏幕空间导数。这是通过特殊指令完成的，这些指令告诉我们任何数据在屏幕空间 X 或 Y 维度上的片段间变化率。 例如，当前片段的世界位置是 $p_0$。下一个片段在屏幕空间 X 维度上的位置是 $p_x$。这两个片段之间世界位置在 X 维度上的变化率因此是 $\\frac{\\partial p}{\\partial x} = p_x - p_0$。 这是世界位置在屏幕空间 X 维度上的偏导数。我们可以通过 ddx 函数在片段程序中检索此数据，为其提供世界位置。让我们在 My Lighting.cginc 中的 InitializeFragmentNormal 函数开始处执行此操作。 void InitializeFragmentNormal(inout Interpolators i) { float3 dpdx = ddx(i.worldPos); ... } 我们可以对屏幕空间 Y 维度做同样的事情，通过调用 ddy 函数并传入世界位置来找到 $\\frac{\\partial p}{\\partial y} = p_y - p_0$。 float3 dpdx = ddx(i.worldPos); float3 dpdy = ddy(i.worldPos); 因为这些值代表片段世界位置之间的差异，它们定义了三角形的两条边。我们实际上不知道那个三角形的确切形状，但它保证位于原始三角形的平面内，这才是最重要的。所以最终的法线向量是那些向量的归一化叉积。用这个向量覆盖原始法线。 float3 dpdx = ddx(i.worldPos); float3 dpdy = ddy(i.worldPos); i.normal = normalize(cross(dpdy, dpdx)); ddx 和 ddy 是如何工作的？ GPU 需要知道纹理坐标的屏幕空间导数以确定使用哪个 mipmap 级别来采样纹理。它通过比较相邻片段的坐标来计算这一点。屏幕空间导数指令是该功能的扩展，使所有片段程序都可以使用此功能，用于它们使用的任何数据。 为了能够比较片段，GPU 以 $2 \t\\times 2$ 的块处理它们。每块，它确定两个 X 维度的导数（针对两个 $2 \t\\times 1$ 片段对）和两个 Y 维度的导数（针对两个 $1 \t\\times 2$ 片段对）。一对的两个片段使用相同的导数数据。这意味着导数每块只变化一次，即每两个像素一次，而不是每个像素一次。结果，这些导数是一个近似值，当用于每片段非线性变化的数据时会出现块状。因为三角形是平坦的，这种近似不会影响我们要推导的法线向量。 导数对块 GPU 总是以 $2\\times 2$ 块处理片段，所以沿着三角形边缘，最终在三角形外部的片段也会被处理。这些无效片段被丢弃，但仍需要处理以确定导数。在三角形外部，片段的插值数据被外推到顶点定义的范围之外。 创建一个使用我们的 Flat Wireframe 着色器的新材质。任何使用此材质的网格都应使用平面着色渲染。它们将出现刻面外观，虽然当你也使用法线贴图时可能很难看到这一点。我在本教程的截图中使用了标准的胶囊体网格和灰色材质。 平滑着色与平面着色 从远处看，胶囊体可能看起来像是由四边形组成的，但这些四边形每个都是由两个三角形组成的。 由三角形组成的四边形 虽然这可行，但我们实际上改变了所有依赖 My Lighting 包含文件的着色器的行为。所以移除我们刚刚添加的代码。 // float3 dpdx = ddx(i.worldPos); // float3 dpdy = ddy(i.worldPos); // i.normal = normalize(cross(dpdy, dpdx)); 1.2 几何着色器 (Geometry Shaders) 还有另一种确定三角形法线的方法。我们可以使用实际的三角形顶点来计算法线向量，而不是使用导数指令。这需要我们按三角形进行工作，而不是按单个顶点或片段。这就是几何着色器发挥作用的地方。 几何着色器阶段位于顶点和片段阶段之间。它被馈送顶点程序的输出，按图元分组。几何程序可以在这些数据被插值并用于渲染片段之前修改它。 每个三角形处理顶点 几何着色器的附加值是顶点按图元（primitive）馈送给它，在我们的例子中每个三角形三个顶点。网格三角形是否共享顶点并不重要，因为几何程序输出新的顶点数据。这允许我们推导三角形的法线向量并将其用作所有三个顶点的法线。 让我们把几何着色器的代码放在它自己的包含文件 MyFlatWireframe.cginc 中。让这个文件包含 My Lighting.cginc 并定义一个 MyGeometryProgram 函数。从一个空的 void 函数开始。 #if !defined(FLAT_WIREFRAME_INCLUDED) #define FLAT_WIREFRAME_INCLUDED #include \"My Lighting.cginc\" void MyGeometryProgram () {} #endif 几何着色器仅在针对 shader model 4.0 或更高版本时受支持。如果定义的较低，Unity 会自动将目标提升到此级别，但让我们明确一点。要实际使用几何着色器，我们必须添加 #pragma geometry 指令，就像顶点和片段函数一样。最后，必须包含 MyFlatWireframe 而不是 My Lighting。将这些更改应用于我们的 Flat Wireframe 着色器的基本、附加和延迟 pass。 #pragma target 4.0 ... #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #pragma geometry MyGeometryProgram ... // #include \"My Lighting.cginc\" #include \"MyFlatWireframe.cginc\" 这将导致着色器编译器错误，因为我们要么还没有正确定义几何函数。我们必须声明它将输出多少个顶点。这个数字可以变化，所以我们必须提供一个最大值。因为我们处理的是三角形，我们将始终每次调用输出三个顶点。这是通过向我们的函数添加 maxvertexcount 属性来指定的，参数为 3。 [maxvertexcount(3)] void GeometryProgram () {} 下一步是定义输入。由于我们处理的是插值前的顶点程序输出，数据类型是 InterpolatorsVertex。所以类型名称在这种情况下技术上是不正确的，但我们在命名时没有考虑到几何着色器。 [maxvertexcount(3)] void MyGeometryProgram (InterpolatorsVertex i) {} 我们还必须声明我们正在处理哪种类型的图元，在我们的例子中是 triangle。这必须在输入类型之前指定。此外，由于三角形各有两个顶点，我们正在处理一个包含三个结构的数组。我们必须明确定义这一点。 [maxvertexcount(3)] void MyGeometryProgram (triangle InterpolatorsVertex i[3]) {} 因为几何着色器可以输出的顶点数量各不相同，我们没有单一的返回类型。相反，几何着色器写入图元流。在我们的例子中，它是 TriangleStream，必须指定为 inout 参数。 [maxvertexcount(3)] void MyGeometryProgram ( triangle InterpolatorsVertex i[3], inout TriangleStream stream ) {} TriangleStream 就像 C# 中的泛型类型。它需要知道我们要给它的顶点数据类型，这仍然是 InterpolatorsVertex。 [maxvertexcount(3)] void MyGeometryProgram ( triangle InterpolatorsVertex i[3], inout TriangleStream&lt;InterpolatorsVertex&gt; stream ) {} 现在函数签名正确了，我们必须将顶点数据放入流中。这是通过调用流的 Append 函数来完成的，按我们接收它们的顺序对每个顶点调用一次。 [maxvertexcount(3)] void MyGeometryProgram ( triangle InterpolatorsVertex i[3], inout TriangleStream&lt;InterpolatorsVertex&gt; stream ) { stream.Append(i[0]); stream.Append(i[1]); stream.Append(i[2]); } 此时我们的着色器再次工作了。我们添加了一个自定义几何阶段，它只是简单地通过顶点程序的输出，未作修改。 为什么几何程序看起来如此不同？ Unity 的着色器语法混合了 CG 和 HLSL 代码。大多时候它看起来像 CG，但在这种情况下它类似于 HLSL。 1.3 每个三角形修改顶点法线 (Modifying Vertex Normals Per Triangle) 要找到三角形的法线向量，首先提取其三个顶点的世界位置。 float3 p0 = i[0].worldPos.xyz; float3 p1 = i[1].worldPos.xyz; float3 p2 = i[2].worldPos.xyz; stream.Append(i[0]); stream.Append(i[1]); stream.Append(i[2]); 现在我们可以执行归一化叉积，每个三角形一次。 float3 p0 = i[0].worldPos.xyz; float3 p1 = i[1].worldPos.xyz; float3 p2 = i[2].worldPos.xyz; float3 triangleNormal = normalize(cross(p1 - p0, p2 - p0)); 用这个三角形法线替换顶点法线。 float3 triangleNormal = normalize(cross(p1 - p0, p2 - p0)); i[0].normal = triangleNormal; i[1].normal = triangleNormal; i[2].normal = triangleNormal; stream.Append(i[0]); stream.Append(i[1]); stream.Append(i[2]); 再次，平面着色 我们最终得到了与之前相同的结果，但现在使用几何着色器阶段而不是依赖屏幕空间导数指令。 哪种方法最好？ 如果你只需要平面着色，屏幕空间导数是实现该效果的最廉价方式。那么你还可以从网格数据中剥离法线——Unity 可以自动做到这一点——并且还可以移除法线插值器数据。一般来说，如果你能不使用自定义几何阶段，那就这样做。但我们将继续使用几何方法，因为我们在通过线框渲染时也需要它。 2. 渲染线框 (Rendering the Wireframe) 处理完平面着色后，我们继续渲染网格的线框。我们不打算创建新的几何体，也不会使用额外的 pass 来绘制线条。我们将通过在三角形内部、沿其边缘添加线条效果来创建线框视觉效果。这可以创建令人信服的线框，尽管定义形状轮廓的线条看起来只有内部线条的一半粗。这通常不是很明显，所以我们将接受这种不一致。 线框效果，轮廓线较细 2.1 重心坐标 (Barycentric Coordinates) 为了给三角形边缘添加线条效果，我们需要知道片段到最近边缘的距离。这意味着关于三角形的拓扑信息需要在片段程序中可用。这可以通过将三角形的重心坐标添加到插值数据中来完成。 什么是重心坐标？ 在三角形的情况下，它是具有三个分量的坐标。每个分量沿着一条边为 0，在与该边相对的顶点处为 1，在中间线性过渡。这些坐标也用于插值顶点数据。 三角形内部的重心坐标 向三角形添加重心坐标的一种方法是使用网格的顶点颜色来存储它们。每个三角形的第一个顶点变为红色，第二个变为绿色，第三个变为蓝色。然而，这将需要以此方式分配顶点颜色的网格，并且使得无法共享顶点。我们想要一个适用于任何网格的解决方案。幸运的是，我们可以使用几何程序来添加所需的坐标。 因为重心坐标不是由网格提供的，顶点程序不知道它们。所以它们不是 InterpolatorsVertex 结构的一部分。为了让几何程序输出它们，我们必须定义一个新的结构。首先在 MyGeometryProgram 上方定义 InterpolatorsGeometry。它应该包含与 InterpolatorsVertex 相同的数据，所以使用它作为内容。 struct InterpolatorsGeometry { InterpolatorsVertex data; }; 调整 MyGeometryProgram 的流数据类型，使其使用新结构。在函数内部定义此类型的变量，将输入数据分配给它们，并将它们追加到流中，而不是直接传递输入。 [maxvertexcount(3)] void MyGeometryProgram ( triangle InterpolatorsVertex i[3], inout TriangleStream&lt;InterpolatorsGeometry&gt; stream ) { ... InterpolatorsGeometry g0, g1, g2; g0.data = i[0]; g1.data = i[1]; g2.data = i[2]; stream.Append(g0); stream.Append(g1); stream.Append(g2); } 现在我们可以向 InterpolatorsGeometry 添加额外数据。给它一个 float3 barycentricCoordinates 向量，使用第十个插值器语义。 struct InterpolatorsGeometry { InterpolatorsVertex data; float3 barycentricCoordinates : TEXCOORD9; }; 给每个顶点一个重心坐标。哪个顶点得到哪个坐标并不重要，只要它们是有效的。 g0.barycentricCoordinates = float3(1, 0, 0); g1.barycentricCoordinates = float3(0, 1, 0); g2.barycentricCoordinates = float3(0, 0, 1); stream.Append(g0); stream.Append(g1); stream.Append(g2); 注意重心坐标总和为 1。所以我们其实只需要传递两个，通过从 1 中减去另外两个来推导第三个坐标。这意味着我们少插值一个数字，所以让我们做这个改变。 struct InterpolatorsGeometry { InterpolatorsVertex data; float2 barycentricCoordinates : TEXCOORD9; }; ... g0.barycentricCoordinates = float2(1, 0); g1.barycentricCoordinates = float2(0, 1); g2.barycentricCoordinates = float2(0, 0); 我们的重心坐标现在是用重心坐标插值的吗？ 是的。不幸的是，我们不能直接使用用于插值顶点数据的重心坐标。GPU 可能会出于各种原因在我们在顶点程序中结束之前将三角形分割成更小的三角形。所以 GPU 用于最终插值的坐标可能与预期的不同。 2.2 定义额外的插值器 (Defining Extra Interpolators) 此时我们将重心坐标传递给片段程序，但它还不知道它们。我们必须将它们添加到 My Lighting 中 Interpolators 的定义中。但我们不能简单地假设这些数据是可用的。只有我们的 Flat Wireframe 着色器是这种情况。所以让我们通过定义一个 CUSTOM_GEOMETRY_INTERPOLATORS 宏，使得任何使用 My Lighting 的人都可以定义通过几何着色器提供的自己的插值器数据。为了支持这一点，如果该宏在此时已定义，则将其插入到 Interpolators 中。 struct Interpolators { ... #if defined (CUSTOM_GEOMETRY_INTERPOLATORS) CUSTOM_GEOMETRY_INTERPOLATORS #endif }; 现在我们可以在 MyFlatWireframe 中定义此宏。我们必须在包含 My Lighting 之前这样做。我们也可以在 InterpolatorsGeometry 中使用它，这样我们只需要编写一次代码。 #define CUSTOM_GEOMETRY_INTERPOLATORS float2 barycentricCoordinates : TEXCOORD9; #include \"My Lighting.cginc\" struct InterpolatorsGeometry { InterpolatorsVertex data; // float2 barycentricCoordinates : TEXCOORD9; CUSTOM_GEOMETRY_INTERPOLATORS }; 为什么我会收到转换编译错误？ 如果你使用的是渲染 20 中的包，那是由于一个教程错误。My Lighting 中的 ComputeVertexLightColor 函数应该使用 InterpolatorsVertex 作为其参数类型，但不正确地使用了 Interpolators。修复此错误，错误就会消失。如果你使用的是自己的代码，你可能会在某个地方使用错误的插值器结构类型时遇到类似的错误。 2.3 拆分 My Lighting (Splitting My Lighting) 我们将如何使用重心坐标来可视化线框？无论我们怎么做，My Lighting 都不应该参与其中。相反，我们可以通过插入我们自己的函数使其代码中的功能可重连。 要覆盖 My Lighting 的功能，我们必须在包含该文件之前定义新代码。但要这样做，我们需要访问 Interpolators，它定义在 My Lighting 中，所以我们必须先包含它。为了解决这个问题，我们必须将 My Lighting 拆分为两个文件。复制 My Lighting 开头的代码，包括 include 语句、插值器结构和所有 Get 函数。将此代码放入一个新的 My Lighting Input.cginc 文件中。给该文件它自己的包含保护定义 MY_LIGHTING_INPUT_INCLUDED。 #if !defined(MY_LIGHTING_INPUT_INCLUDED) #define MY_LIGHTING_INPUT_INCLUDED #include \"UnityPBSLighting.cginc\" #include \"AutoLight.cginc\" #if defined(FOG_LINEAR) || defined(FOG_EXP) || defined(FOG_EXP2) #if !defined(FOG_DISTANCE) #define FOG_DEPTH 1 #endif #define FOG_ON 1 #endif ... float3 GetEmission (Interpolators i) { #if defined(FORWARD_BASE_PASS) || defined(DEFERRED_PASS) #if defined(_EMISSION_MAP) return tex2D(_EmissionMap, i.uv.xy) * _Emission; #else return _Emission; #endif #else return 0; #endif } #endif 从 My Lighting 中删除相同的代码。为了保持现有着色器工作，包含 My Lighting Input。 #if !defined(MY_LIGHTING_INCLUDED) #define MY_LIGHTING_INCLUDED //#include \"UnityPBSLighting.cginc\" // ... // //float3 GetEmission (Interpolators i) { // ... //} #include \"My Lighting Input.cginc\" void ComputeVertexLightColor (inout InterpolatorsVertex i) { #if defined(VERTEXLIGHT_ON) i.vertexLightColor = Shade4PointLights( unity_4LightPosX0, unity_4LightPosY0, unity_4LightPosZ0, unity_LightColor[0].rgb, unity_LightColor[1].rgb, unity_LightColor[2].rgb, unity_LightColor[3].rgb, unity_4LightAtten0, i.worldPos.xyz, i.normal ); #endif } 现在可以在包含 My Lighting 之前包含 My Lighting Input。其包含保护将确保防止重复包含。在 MyFlatWireframe 中这样做。 #include \"My Lighting Input.cginc\" #include \"My Lighting.cginc\" 2.4 重连反照率 (Rewiring Albedo) 让我们通过调整材质的反照率来添加线框效果。这要求我们替换 My Lighting 的默认反照率函数。就像自定义几何插值器一样，我们将通过宏 ALBEDO_FUNCTION 来完成此操作。在 My Lighting 中，在我们确定输入已被包含之后，检查此宏是否已定义。如果没有，将其定义为 GetAlbedo 函数，使其成为默认值。 #include \"My Lighting Input.cginc\" #if !defined(ALBEDO_FUNCTION) #define ALBEDO_FUNCTION GetAlbedo #endif 在 MyFragmentProgram 函数中，将 GetAlbedo 的调用替换为宏。 float3 albedo = DiffuseAndSpecularFromMetallic( ALBEDO_FUNCTION(i), GetMetallic(i), specularTint, oneMinusReflectivity ); 现在我们可以在 MyFlatWireframe 中创建我们自己的反照率函数，在包含 My Lighting Input 之后。它需要具有与原始 GetAlbedo 函数相同的形式。首先简单地传递原始函数的结果。之后，用我们自己的函数名称定义 ALBEDO_FUNCTION 宏，然后包含 My Lighting。 #include \"My Lighting Input.cginc\" float3 GetAlbedoWithWireframe (Interpolators i) { float3 albedo = GetAlbedo(i); return albedo; } #define ALBEDO_FUNCTION GetAlbedoWithWireframe #include \"My Lighting.cginc\" 为了验证我们确实控制了片段的反照率，直接使用重心坐标作为反照率。 float3 GetAlbedoWithWireframe (Interpolators i) { float3 albedo = GetAlbedo(i); float3 barys; barys.xy = i.barycentricCoordinates; barys.z = 1 - barys.x - barys.y; albedo = barys; return albedo; } 重心坐标作为反照率 2.5 创建线条 (Creating Wires) 要创建线框效果，我们需要知道片段离最近的三角形边缘有多近。我们可以通过取重心坐标的最小值来找到这一点。这给了我们在重心域中到边缘的最小距离。让我们直接将其用作反照率。 float3 albedo = GetAlbedo(i); float3 barys; barys.xy = i.barycentricCoordinates; barys.z = 1 - barys.x - barys.y; // albedo = barys; float minBary = min(barys.x, min(barys.y, barys.z)); return albedo * minBary; 最小重心坐标 这看起来有点像白色网格顶部的黑色线框，但太模糊了。这是因为到最近边缘的距离从边缘的零变为三角形中心的 $\\frac{1}{3}$。为了使其看起来更像细线，我们必须更快地淡入白色，例如通过在 0 到 0.1 之间从黑色过渡到白色。为了使过渡平滑，让我们使用 smoothstep 函数。 什么是 smoothstep 函数？ 它是一个标准函数，产生两个值之间的平滑曲线过渡，而不是线性插值。它的定义是 $3t^2 - 2t^3$，其中 $t$ 从 0 变为 1。 Smoothstep vs. 线性过渡 smoothstep 函数有三个参数，$a, b, c$。前两个参数 $a$ 和 $b$ 定义过渡应该覆盖的范围，而 $c$ 是要平滑的值。这导致 $t = \\frac{c - a}{b - a}$，在使用前被限制在 0-1 之间。 float minBary = min(barys.x, min(barys.y, barys.z)); minBary = smoothstep(0, 0.1, minBary); return albedo * minBary; 调整后的过渡 2.6 固定线宽 (Fixed Wire Width) 线框效果开始看起来不错了，但只适用于边长大致相同的三角形。此外，线条受视距影响，因为它们是三角形的一部分。理想情况下，线条具有固定的视觉粗细。 为了在屏幕空间中保持线条粗细恒定，我们必须调整我们用于 smoothstep 函数的范围。范围取决于测量的到边缘的距离在视觉上变化有多快。我们可以使用屏幕空间导数指令来找出这一点。 变化率对于两个屏幕空间维度可能是不同的。我们应该使用哪一个？我们可以使用两者，简单地将它们相加。此外，因为变化可能是正的或负的，我们应该使用它们的绝对值。通过直接使用结果作为范围，我们最终得到覆盖大约两个片段的线条。 float minBary = min(barys.x, min(barys.y, barys.z)); float delta = abs(ddx(minBary)) + abs(ddy(minBary)); minBary = smoothstep(0, delta, minBary); 这个公式也可以作为方便的 fwidth 函数使用，所以让我们使用它。 float delta = fwidth(minBary); 固定宽度的线条 结果线条可能看起来有点太细。我们可以通过将过渡稍微移离边缘来解决这个问题，例如通过我们用于混合范围的相同值。 minBary = smoothstep(delta, 2 * delta, minBary); 线条更粗，但有伪影 这产生了更清晰的线条，但也揭示了三角形角附近的线条中的锯齿伪影。出现伪影是因为最近的边缘在这些区域突然改变，这导致不连续的导数。为了解决这个问题，我们必须使用各个重心坐标的导数，分别混合它们，并在那之后取最小值。 barys.z = 1 - barys.x - barys.y; float3 deltas = fwidth(barys); barys = smoothstep(deltas, 2 * deltas, barys); float minBary = min(barys.x, min(barys.y, barys.z)); // float delta = fwidth(minBary); // minBary = smoothstep(delta, 2 * delta, minBary); return albedo * minBary; 没有伪影的线框 2.7 可配置的线条 (Configurable Wires) 我们有了一个可用的线框效果，但你可能想要使用不同的线条粗细、混合区域或颜色。也许你想为每个材质使用不同的设置。所以让我们使其可配置。为此，向 Flat Wireframe 着色器添加三个属性。第一个是线框颜色，默认为黑色。第二个是线框平滑度，控制过渡范围。0 到 10 的范围应该足够了，默认为 1，代表 fwidth 测量的倍数。第三个是线框粗细，具有与平滑度相同的设置。 _WireframeColor (\"Wireframe Color\", Color) = (0, 0, 0) _WireframeSmoothing (\"Wireframe Smoothing\", Range(0, 10)) = 1 _WireframeThickness (\"Wireframe Thickness\", Range(0, 10)) = 1 将相应的变量添加到 MyFlatWireframe 并在 GetAlbedoWithWireframe 中使用它们。基于平滑后的最小值，通过在线框颜色和原始反照率之间插值来确定最终反照率。 float3 _WireframeColor; float _WireframeSmoothing; float _WireframeThickness; float3 GetAlbedoWithWireframe (Interpolators i) { float3 albedo = GetAlbedo(i); float3 barys; barys.xy = i.barycentricCoordinates; barys.z = 1 - barys.x - barys.y; float3 deltas = fwidth(barys); float3 smoothing = deltas * _WireframeSmoothing; float3 thickness = deltas * _WireframeThickness; barys = smoothstep(thickness, thickness + smoothing, barys); float minBary = min(barys.x, min(barys.y, barys.z)); // return albedo * minBary; return lerp(_WireframeColor, albedo, minBary); } 虽然着色器现在是可配置的，但属性还没有出现在我们的自定义着色器 GUI 中。我们可以为 Flat Wireframe 创建一个新的 GUI，但让我们使用快捷方式并将属性直接添加到 MyLightingShaderGUI。给它一个新的 DoWireframe 方法来为线框创建一个小部分。 void DoWireframe () { GUILayout.Label(\"Wireframe\", EditorStyles.boldLabel); EditorGUI.indentLevel += 2; editor.ShaderProperty( FindProperty(\"_WireframeColor\"), MakeLabel(\"Color\") ); editor.ShaderProperty( FindProperty(\"_WireframeSmoothing\"), MakeLabel(\"Smoothing\", \"In screen space.\") ); editor.ShaderProperty( FindProperty(\"_WireframeThickness\"), MakeLabel(\"Thickness\", \"In screen space.\") ); EditorGUI.indentLevel -= 2; } 为了让 MyLightingShaderGUI 同时支持带和不带线框的着色器，只有当着色器具有 _WireframeColor 属性时才在其 OnGUI 方法中调用 DoWireframe。我们简单地假设如果该属性可用，它就拥有全部三个属性。 public override void OnGUI ( MaterialEditor editor, MaterialProperty[] properties ) { this.target = editor.target as Material; this.editor = editor; this.properties = properties; DoRenderingMode(); if (target.HasProperty(\"_WireframeColor\")) { DoWireframe(); } DoMain(); DoSecondary(); DoAdvanced(); } 可配置的线框 你现在能够渲染带有平面着色和可配置线框的网格了。这将在下一个高级渲染教程曲面细分中派上用场。" }, { "title": "视差和法线、高度图回顾(翻译二十)", "url": "/posts/Unity_Parallax_Normals_Heightmap/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-25 20:00:00 +0800", "content": "由于视角的原因，当调整摄像机位置时，观察到的事物的相对位置会发生变化，这种视觉现象称为视差。在坐火车高速行驶看窗外的景物，附近的物体看起来很大并且移动很快，而远处的背景看起来很小并且移动较慢。渲染时，相机使用透视模式时，也会出现视差。 视差纹理 之前翻译使用过法线贴图将表面不规则感添加到平滑表面。 它会影响照明，但不会影响表面的实际形状。 因此，该效果视差不明显，通过实现法线贴图基于视野深度的幻觉有许多限制。这一篇的目的就是解决该限制。 法线贴图效果回顾 下面给出许多albedo map 和 normal map差异对比： albedo map 和 normal map 如果没有法线贴图，表面看起来很平坦。 添加法线贴图会使它看起来好像具有不规则的表面。 但是，高度海拔差异看起来不明显。 当从入射视角与表面的夹角越趋于0，高度差越不明显。如果高程差异较大，则表面特征的相对视觉位置应由于视差而发生很大变化，但不会发生变化。 我们看到的视差是平坦的表面。 平坦的视角 虽然可以增加法线贴图的强度，但这不会改变视差。同样，当法线贴图变得太强时，它会看起来很奇怪。它影响了平坦表面的光线的明暗变换，而视差效果它们确实是平的。所以法线贴图只适用于小的变化，但不会表现出明显的视差。 法线贴图的光线变化 要获得真正的深度视差感，首先需要确定深度应该是多少。法线贴图不包含这些信息。所以我们需要一个高度图。这样，我们就可以创建一个基于高度信息的假视差效果，就像法线贴图创建一个假斜率一样。下面的贴图也称它是灰度图，黑色代表最低点，白色代表最高点。因为我们将使用这个贴图来创建一个视差效果，也称为视差图。 高度图 确保在导入时禁用sRGB(颜色纹理)，这样在使用线性空间渲染时数据就不会被弄乱 Shader参数 为了能够使用视差贴图，我们必须为它添加一个属性到着色器。也会给它一个强度参数来缩放效果。因为视差效果相当强，我们将其范围设置为(0 , 0.1)。 [NoScaleOffset] _ParallaxMap (\"Parallax\", 2D) = \"black\" {} _ParallaxStrength (\"Parallax Strength\", Range(0, 0.1)) = 0 [NoScaleOffset] _OcclusionMap (\"Occlusion\", 2D) = \"white\" {} _OcclusionStrength (\"Occlusion Strength\", Range(0, 1)) = 1 视差贴图是一个着色器特性，我们将启用__PARALLAX_MAP_关键字。将必需的编译器指令添加到base pass、additive pass和deferred pass。 #pragma shader_feature _NORMAL_MAP #pragma shader_feature _PARALLAX_MAP 为什么不在ShadowCaster增加视差贴图？ 当使用albedo贴图的alpha通道的透明度时，视差贴图只会影响阴影。即使是这样，在阴影贴图中的视差效果也很难被注意到。所以它通常不值得额外的计算时间。但是如果愿意，也可以将它添加到阴影施法者通道中。 为了访问新的属性，给我的照明添加相应的变量 sampler2D _ParallaxMap; float _ParallaxStrength; sampler2D _OcclusionMap; float _OcclusionStrength; 为了能够自定义配置材质，在Extend ShaderGUI扩展中增加相应Enable与Disanble key的方法。 void DoParallax () { MaterialProperty map = FindProperty(\"_ParallaxMap\"); Texture tex = map.textureValue; EditorGUI.BeginChangeCheck(); editor.TexturePropertySingleLine ( MakeLabel(map, \"Parallax (G)\"), map, tex ? FindProperty(\"_ParallaxStrength\") : null ); if (EditorGUI.EndChangeCheck() &amp;&amp; tex != map.textureValue) { SetKeyword(\"_PARALLAX_MAP\", map.textureValue); } } 坐标匹配 通过在fragment程序中调整纹理坐标，让平坦表面的某些部分看起来高低交错。创建一个应用视差函数，给它一个inout插值器参数。 void ApplyParallax (inout Interpolators i) { } 在fragment程序使用插入的数据之前调用视差函数。_会有点异常是LOD衰落，_因为这取决于屏幕位置。先不调整这些坐标。 FragmentOutput MyFragmentProgram (Interpolators i) { UNITY_SETUP_INSTANCE_ID(i); #if defined(LOD_FADE_CROSSFADE) UnityApplyDitherCrossFade(i.vpos); #endif ApplyParallax(i); float alpha = GetAlpha(i); #if defined(_RENDERING_CUTOUT) clip(alpha - _Cutoff); #endif } 通过简单地向U坐标添加视差强度来调整纹理坐标。做一次偏移计算 void ApplyParallax (inout Interpolators i) { #if defined(_PARALLAX_MAP) i.uv.x += _ParallaxStrength; #endif } 改变视差强度会导致纹理偏移。增加U坐标会使纹理向负的U方向移动，V坐标同理。这看起来不是视差效果，因为这是一个与视角无关的均匀位移。 u坐标移动 随视角方向移动 视差是由相对于观察者的透视投影，所以必须改变纹理坐标。这意味着必须基于视图的方向来移动坐标，而视图的方向对于表面上每个片段都是不同的。 varies across a surface 纹理坐标存在于切线空间中。为了调整这些坐标，需要知道视图在切线空间中的方向。这需要矩阵乘法对空间进行转换。在fragment-程序已经有了一个切线空间矩阵，但是它是用于从切线空间到世界空间的转换。在本例中，需要从对象空间转到切线空间。 视图方向向量定义为从表面到摄像机，需要归一化。我们可以在vertex程序中确定这个向量，转换它并将它传递给fragment程序。但是为了最终得到正确的方向，需要推迟归一化，直到插值完成后。添加切线空间视图方向作为一个新的插值成员变量。 struct InterpolatorsVertex { #if defined(_PARALLAX_MAP) float3 tangentViewDir : TEXCOORD8; #endif }; struct Interpolators { #if defined(_PARALLAX_MAP) float3 tangentViewDir : TEXCOORD8; #endif }; 寄存器数量限制是多少? model 1与model 2都只支持8个Texture Coordinate Register -&gt;Texcoord[0-7]。当使用model 3时，可以使用TEXCOORD8。若硬件不支持model 3其机能也就不是很强大，所以不要使用视差映射。 首先, 使用mesh网格数据中的原始顶点切向量和法向量，在顶点程序中创建一个从对象空间到切线空间的转换矩阵。因为我们只用它来变换一个向量而不是一个位置我们用一个3×3矩阵就足够了。 InterpolatorsVertex MyVertexProgram (VertexData v) { ComputeVertexLightColor(i); #if defined (_PARALLAX_MAP) float3x3 objectToTangent = float3x3( v.tangent.xyz, cross(v.normal, v.tangent.xyz) * v.tangent.w, v.normal ); #endif return i; } 然后，可以使用ObjSpaceViewDir函数得到对象空间中顶点位置的视图方向，再用矩阵变换它我们就得到了我们需要的切线空间下视图方向。 #if defined (_PARALLAX_MAP) float3x3 objectToTangent = float3x3 ( v.tangent.xyz, cross(v.normal, v.tangent.xyz) * v.tangent.w, v.normal ); i.tangentViewDir = mul(objectToTangent, ObjSpaceViewDir(v.vertex)); #endif //ObjSpaceViewDir内部实现? //ObjSpaceViewDir函数是在UnityCG中定义的。它先将摄像机位置转换到对象空间，然后减去对象空间下顶点位置得到一个从顶点指向摄像机的向量，注意它还没有标准化. inline float3 ObjSpaceViewDir (float4 v) { float3 objSpaceCameraPos = mul(unity_WorldToObject, float4(_WorldSpaceCameraPos.xyz, 1)).xyz; return objSpaceCameraPos - v.xyz; } 最后，我们可以在ApplyParallax函数使用切线空间视图方向了。首先，对它进行规格化normalize，把它变成一个合适的方向向量。然后，添加它的XY组件到纹理坐标，再由视差强度缩放。 void ApplyParallax (inout Interpolators i) { #if defined(_PARALLAX_MAP) i.tangentViewDir = normalize(i.tangentViewDir); i.uv.xy += i.tangentViewDir.xy * _ParallaxStrength; #endif } 这能有效地将视图方向投影到纹理表面上。当以90度角直视表面时，在切空间中的视图方向等于表面法线(0,0,1)，这不会导致位移。视角越浅，投影越大，位移效果也越大。 影视图方向用作UV偏移 所有这一切的影响是表面似乎被拉向上的切线空间，看起来比它实际上更高，基于视差强度。 随投影视角方向移动UV 基于高度滑动 在基于高度这一点上，我们可以让表面看起来更高，但它仍然是一个均匀位移。下一步是使用视差贴图来缩放位移。采样贴图，使用它的G通道作为高度，应用视差强度，并使用它来调节位移。 i.tangentViewDir = normalize(i.tangentViewDir); float height = tex2D(_ParallaxMap, i.uv.xy).g; height *= _ParallaxStrength; i.uv.xy += i.tangentViewDir.xy * height; 由高度调整的移动 低的区域现在保持不变，而高的区域被向上拉。standard shader抵消了这种效果，所以低的区域也向下移动，而在中间的区域保持他们原来的位置。这是通过从原始高度数据中减去差值来实现的。 float height = tex2D(_ParallaxMap, i.uv.xy).g; height -= 0.5; height *= _ParallaxStrength; 视差贴图效果 由合理到过量 这就产生了我们想要的视差效果，但它只在低强度下有效。不足的是位移位移变换的很快，会撕裂表面。 偏移视差映射算法 我们目前使用的视差映射技术被称为带偏移限制的视差映射。我们只是使用了视图方向的XY部分，它的最大长度是1。因此，纹理偏移量是有限的。这种效果不错，但不能代表正确的透视投影。 一个更精确的计算偏移量的物理方法是将高度场视为几何图形表面下的体积，并通过它拍摄一个视图射线。光线从相机发射到表面，从上面进入高度场体积，并持续发射直到它到达由场定义的表面。 如果高度场均匀为零，那么射线就会一直持续到体积的底部。它与物体的距离取决于光线进入物体时的角度。它没有限制。角度越浅，越远。最极端的情况是当视角趋于0时，光线射向无穷大。 光线投射到底部，有限且正确 为了找到合适的偏移量，我们必须缩放视图方向向量，通过除以它自己的Z分量来使它的Z分量变成1。因为我们以后不需要用Z，我们只需要用X和Y除以Z。 i.tangentViewDir = normalize(i.tangentViewDir); i.tangentViewDir.xy /= i.tangentViewDir.z; 虽然这样可以得到一个更正确的投影，但它确实会使浅视角的视差效果恶化。standard着色器通过增加0.42偏差到Z减轻浅视角的视差效果恶化，所以它永远不会接近零。这扭曲了透视图，但使工件更易于管理。我们再加上这个偏差. i.tangentViewDir.xy /= (i.tangentViewDir.z + 0.42); 视差贴图像标准着色器 通过上述多个步骤修正后, 现在我们的着色器与标准着色器支持同样的视差效果。视差映射可以应用于任何表面，投影假设切线空间是均匀的。曲面具有弯曲的切线空间，因此会产生物理上不正确的结果。只要视差强度和曲率很小，你就可以摆脱它。 球面视差贴图 同样，阴影坐标不会受到这个效果的影响。因此，阴影在强烈的视差的组合下看起来很奇怪，好像漂浮在表面上。 阴影不受视差影响 Parallax Configuration 你不同意Unity使用0.42的偏移值吗?或者你想使用一个不同的值，还是让它保持在0?或者你想用偏移限制代替吗?它是可以配置! 当你想使用偏移限制，定义PARALLAX_OFFSET_LIMITING在着色器。或者，通过定义PARALLAX_BIAS来设置要使用的偏差。 void ApplyParallax (inout Interpolators i) { #if defined(_PARALLAX_MAP) i.tangentViewDir = normalize(i.tangentViewDir); #if !defined(PARALLAX_OFFSET_LIMITING) i.tangentViewDir.xy /= (i.tangentViewDir.z + PARALLAX_BIAS); #endif #endif } 当没有定义时，假设偏差是0.42。在ApplyParallax 中定义它。注意，宏定义不关心函数作用域，它们总是全局的。 #if !defined(PARALLAX_OFFSET_LIMITING) #if !defined(PARALLAX_BIAS) #define PARALLAX_BIAS 0.42 #endif i.tangentViewDir.xy /= (i.tangentViewDir.z + PARALLAX_BIAS); #endif 现在我们可以通过着色器的CGINCLUDE块来微调我们的视差效果。添加无偏差和限制偏移的选项，但将它们转换为注释，以坚持默认选项。 CGINCLUDE #define BINORMAL_PER_FRAGMENT #define FOG_DISTANCE //\t#define PARALLAX_BIAS 0 //\t#define PARALLAX_OFFSET_LIMITING ENDCG Detail UV 视差贴图可以在主贴图上工作，但是我们还没有注意到副贴图。我们必须应用纹理坐标偏移到细节UV上。 首先，下面是一个包含网格模式的详细地图。它可以很容易地验证效果是否正确地应用于细节。 细节网格纹理 使用这个纹理作为材质的细节albedo贴图。设置二级贴图的平铺为10×10。这表明，细节紫外线确实仍然不受影响。 细节UV不受影响 Standard也简单地添加了UV偏移到细节UV，这是存储在UV插值器的ZW组件。 float height = tex2D(_ParallaxMap, i.uv.xy).g; height -= 0.5; height *= _ParallaxStrength; float2 uvOffset = i.tangentViewDir.xy * height; i.uv.xy += uvOffset; i.uv.zw += uvOffset; 细节可能有所变化，但是它们肯定还不匹配视差效果。 那是因为我们平铺了二级纹理。 这样会将细节UV缩放10倍，使视差偏移量变弱十倍。 我们还必须将细节拼贴应用到偏移量。 i.uv.zw += uvOffset * _DetailTex_ST.xy; 实际上，缩放应该相对于主UV平铺，以防它被设置为1×1以外的一些东西。 i.uv.zw += uvOffset * (_DetailTex_ST.xy / _MainTex_ST.xy); 正确的UV Ray Marching-光线步进 然而，除了上述的偏移视差映射还有另外的视差算法：发射射线与高度场体积相交，确定其交点在表面上的位置，然后对该位置采样。 它通过在射线进入体积时的交点，对高度图进行一次采样。 但是，当看向任意一个角度时，这并不能准确告诉射线实际上与高度场相交的高度。 实际与预测的高度对比 先假设入口点的高度与交点的高度相同，但这实际上只有在入口点和交点具有相同的高度时才是正确的。当偏移量不大且高度场变化不大时，它的效果仍然很好。但是，当偏移量太大或高度变化太快时，该算法就会出现问题，而这很可能是错误的。这就会造成表面撕裂。 如果我们能算出射线实际到达的高度场的位置，那么总能找到真正的可见表面点。这不能通过单个纹理样本来实现，我们必须沿着视图射线逐步移动，并每次都采样高度场，直到射线到达表面。该技术是RayMarching。 随视图射线前进 有各种不同的视差贴图使用raymarching。常见的是陡视差映射_Steep Parallax Mapping_、地形映射_Relief Mapping_和视差遮挡映射_Parallax Occlusion Mapping_。与使用单一纹理样本相比，它们能通过高度场来创建更好的视差效果。除此之外，它们还可以应用额外的阴影和技术来改进该算法。当我们做的匹配这些方法时，我会调用它。 自定义视差函数 标准着色器仅支持简单的偏移视差映射。 现在，我们要在自己的着色器中添加对视差光线Ray marching的支持。 但是，我们还要继续支持这种简单方法。 两者都需要采样height字段，因此将采样代码行放在单独的GetParallaxHeight函数中。 而且，两种方法的投影视图方向和偏移量的最终应用都相同。 因此，将偏移量计算也单独为一个函数。 它仅需要原始UV坐标和已处理的视图方向作为参数，结果返回要应用的UV偏移。 float GetParallaxHeight (float2 uv) { return tex2D(_ParallaxMap, uv).g; } float2 ParallaxOffset (float2 uv, float2 viewDir) { float height = GetParallaxHeight(uv); height -= 0.5; height *= _ParallaxStrength; return viewDir * height; } void ApplyParallax (inout Interpolators i) { #if defined(_PARALLAX_MAP) i.tangentViewDir = normalize(i.tangentViewDir); #if !defined(PARALLAX_OFFSET_LIMITING) #if !defined(PARALLAX_BIAS) #define PARALLAX_BIAS 0.42 #endif i.tangentViewDir.xy /= (i.tangentViewDir.z + PARALLAX_BIAS); #endif float2 uvOffset = ParallaxOffset(i.uv.xy, i.tangentViewDir.xy); i.uv.xy += uvOffset; i.uv.zw += uvOffset * (_DetailTex_ST.xy / _MainTex_ST.xy); #endif } 现在，我们将应用视差函数宏替换对视差偏移的硬编码调用，从而使视差方法更加灵活。如果没有定义它，我们将它设置为使用偏移量方法。 void ApplyParallax (inout Interpolators i) { #if defined(_PARALLAX_MAP) //... #if !defined(PARALLAX_FUNCTION) #define PARALLAX_FUNCTION ParallaxOffset #endif float2 uvOffset = PARALLAX_FUNCTION(i.uv.xy, i.tangentViewDir.xy); i.uv.xy += uvOffset; i.uv.zw += uvOffset * (_DetailTex_ST.xy / _MainTex_ST.xy); #endif } 为RayMarching方法创建一个新函数。与ParallaxOffset函数类似的参数和返回类型。 float2 ParallaxOffset (float2 uv, float2 viewDir) { } float2 ParallaxRaymarching (float2 uv, float2 viewDir) { float2 uvOffset = 0; return uvOffset; } 现在可以通过定义_PARALLAX_FUNCTION_来改变着色器中的视差方法。 #define PARALLAX_BIAS 0 //#define PARALLAX_OFFSET_LIMITING #define PARALLAX_FUNCTION ParallaxRaymarching 相交计算 为了找到视图射线到达高度场的点，我们需要对射线上的多个点进行采样并计算出在表面下方的位置。第一个采样点在顶部，我们在这里输入高度量，就像使用偏移方法一样。最后一个采样点就是射线到达体积底部的地方。我们会在这些端点之间均匀地添加额外的采样点。 假设每条射线进行10次采样。这意味着我们将对高度图采样10次而不是一次，所以这不是一个便宜计算方法。因为我们用了10个样本，所以步长是0.1。这是我们沿着视图射线移动的因子，也就是UV偏移增量。 float2 ParallaxRaymarching (float2 uv, float2 viewDir) { float2 uvOffset = 0; float stepSize = 0.1; float2 uvDelta = viewDir * stepSize; return uvOffset; } 为了应用视差强度，我们可以调整每一步采样的高度。但是缩放UV delta也有同样的效果，只需要计算一次。 float2 uvDelta = viewDir * (stepSize * _ParallaxStrength); 通过这种方式，无论视差强度如何，我们都可以继续使用0–1作为高度场的范围。 因此，射线的第一步高度始终为1。低于或高于该高度的表面点的高度由高度场定义。 float stepSize = 0.1;//步长 float2 uvDelta = viewDir * (stepSize * _ParallaxStrength); float stepHeight = 1;//步高 float surfaceHeight = GetParallaxHeight(uv); 现在我们要沿着射线迭代。首先，每一步我们都会增加UV偏移量。视图向量指向摄像机，但我们是在向表面移动，所以我们需要减去UV delta。然后我们用步高来减小步长。然后我们再次对高度图采样。使用while循环重复上述步骤，直到采样完毕。 float stepHeight = 1; float surfaceHeight = GetParallaxHeight(uv); while (stepHeight &gt; surfaceHeight) { uvOffset -= uvDelta; stepHeight -= stepSize; surfaceHeight = GetParallaxHeight(uv + uvOffset); } 当编译时，会得到一个编译器警告和错误。这个警告告诉我们在循环中使用了梯度指令。这指的是循环中的纹理采样。GPU必须弄清楚使用哪个mipmap级别，它需要比较相邻片段使用的UV坐标。只有当所有片段执行相同的代码时，它才能对比。对于循环来说，这是不可能的，因为它可以提前终止，每个片段都可能不同。因此编译器将展开循环，这意味着它将一直执行所有9个步骤，而不管逻辑是否可以提前停止。相反，它随后使用确定性逻辑选择最终结果。 编译失败是因为编译器无法确定循环的最大迭代次数。它不知道这个最多是9。通过将while循环转换为执行限制的for循环来明确这一点。 for (int i = 1; i &lt; 10 &amp;&amp; stepHeight &gt; surfaceHeight; i++) { uvOffset -= uvDelta; stepHeight -= stepSize; surfaceHeight = GetParallaxHeight(uv + uvOffset); } Raymarching 步进10次 无偏差, 无限制 与简单的视差偏移方法相比，视差效果更加明显。较高的区域现在也正确地阻挡了我们后面较低区域的视野。我们还得到了明显的图层，总共10层。 更多步进 这个基本的光线行进方法最适合陡峭的视差贴图。效果的质量是由我们的样本分辨率决定的。一些方法根据视角使用可变的步骤。较浅的角度需要更多的步长，因为光线较长。但我们的样本量是固定的，所以我们不会这样做。 提高质量的明显方法是增加采样的次数，因此让其可配置。使用_PARALLAX_RAYMARCHING_STEPS_，默认值为10，而不是固定的步长和迭代次数。 float2 ParallaxRaymarching (float2 uv, float2 viewDir) { #if !defined(PARALLAX_RAYMARCHING_STEPS) #define PARALLAX_RAYMARCHING_STEPS 10 #endif float2 uvOffset = 0; float stepSize = 1.0 / PARALLAX_RAYMARCHING_STEPS; float2 uvDelta = viewDir * (stepSize * _ParallaxStrength); float stepHeight = 1; float surfaceHeight = GetParallaxHeight(uv); for ( int i = 1; i &lt; PARALLAX_RAYMARCHING_STEPS &amp;&amp; stepHeight &gt; surfaceHeight; i++ ) { uvOffset -= uvDelta; stepHeight -= stepSize; surfaceHeight = GetParallaxHeight(uv + uvOffset); } return uvOffset; } 现在我们可以在着色器中控制步数。对于真正的高质量，将PARALLAX_RAYMARCHING_STEPS定义为100。 #define PARALLAX_BIAS 0 //#define PARALLAX_OFFSET_LIMITING #define PARALLAX_RAYMARCHING_STEPS 100 #define PARALLAX_FUNCTION ParallaxRaymarching Raymarching 100次采样 这让我们知道了它的效果能有多好，但它计算量太大了，一般不适合手机。所以把样本数设为10后，我们仍然可以看到视差效果看起来连续和平滑。然而，由视差遮挡引起的轮廓总是锯齿状的，MSAA并不能消除这一点，因为它只适用于几何图形的边缘，而不是纹理效果。只要不依赖深度缓冲区，后处理抗锯齿技术能解决。 不能按片段写入深度缓冲区吗? 这在足够先进的硬件上确实是可能的，使它能够正确地与高度场相交并应用阴影。不过，它计算量太大。 我们当前的方法是沿着射线步进，直到到达表面以下的点，或者到达射线末端可能的最低点。然后我们用UV偏移处理那个点。但隐藏在表面之下的这个点，很可能会出现错误。这就是导致表面撕裂的原因。 增加步长数只会减少最大误差。使用足够的步骤，错误会变得更小，以至于我们无法再看到它。所以当一个表面总是从远处看，你可以用更少的步骤。距离越近，视角越小，需要的样本就越多。 Raymarching 100次采样 步长之间插值 提高质量的一种方法是根据经验预测光线真正到达表面的位置。比如第一步在表面之上，下一步在表面之下。在这两步之间的某个点射线一定到达了表面。 两个射线点、和两个射线点到表面最近的点，能定义两条线段。因为光线和表面碰撞，这两条线段会相交。所以如果我们跟踪前面的步骤，我们可以在循环之后执行直线交叉。我们可以用这个信息来近似出真正的交点。 执行直线交叉 在for循环内，我们必须跟踪之前的UV偏移量、步长高度和表面高度。一般来说，这些等于循环之前的第一个样本。 float2 prevUVOffset = uvOffset; float prevStepHeight = stepHeight; float prevSurfaceHeight = surfaceHeight; 在循环之后，我们计算这些线的交点。我们可以使用这个插值之间的前点和后点的UV偏移。 float prevDifference = prevStepHeight - prevSurfaceHeight; float difference = surfaceHeight - stepHeight; float t = prevDifference / (prevDifference + difference); uvOffset = lerp(prevUVOffset, uvOffset, t); return uvOffset; 数学原理： 这两个线段定义在两个样本步骤之间的空间内。我们将这个空间的宽度设置为1。从前一步到最后一步的直线由点(0，a)和点(1，b)定义，其中a是前一步的高度，b是后一步的高度。因此，可以用线性函数'v(t) = a + (b - a)t'来定义视图线。同样地，面线由点(0，c)和(1，d)定义，函数's(t) = hlsl + (d - hlsl)t'。 交点存在于s(t) = v(t)'处。那么t的值是多少? c + (d - c)t = a + (b - a)t (d - c)t - (b - a)t = a - c (a - c + d - b)t = a - c t = (a - c) / (a - c + d - b) 注意:a - c是在t = 0处直线高度的绝对差。d - b是t = 1处的绝对高度差。 线段交点 实际上，在这种情况下，我们可以使用插值器来缩放我们要添加到上一点上的UV偏移量。它可以归结为相同的东西，只是用了更少的数学。 float t = prevDifference / (prevDifference - difference); uvOffset = prevUVOffset - uvDelta * t; 效果看起来好多了。我们现在假设表面在样本点之间是线性的，这可以防止最明显的分层假象。然而，它不能帮助我们检测我们是否错过了步骤之间的交集。我们仍然需要很多的样本来处理小的特征，轮廓和浅角度。 有了这个技巧，我们的方法类似于视差遮挡映射。虽然这是一个相对便宜的改进，但通过定义_PARALLAX_RAYMARCHING_INTERPOLATE_，我们让它成为可选的。 #if defined(PARALLAX_RAYMARCHING_INTERPOLATE) float prevDifference = prevStepHeight - prevSurfaceHeight; float difference = surfaceHeight - stepHeight; float t = prevDifference / (prevDifference + difference); uvOffset = prevUVOffset - uvDelta * t; #endif 在shader内定义PARALLAX_RAYMARCHING_INTERPOLATE。 #define PARALLAX_BIAS 0 //#define PARALLAX_OFFSET_LIMITING #define PARALLAX_RAYMARCHING_STEPS 10 #define PARALLAX_RAYMARCHING_INTERPOLATE #define PARALLAX_FUNCTION ParallaxRaymarching 步长搜索 通过在两个步长之间进行线性插值，我们假定表面在两个步长之间是笔直的。 但是，通常情况并非如此。 为了更好地处理不规则的高度场，我们必须在两个步长之间搜索实际的交点。 或至少接近它。 完成循环后，不要使用最后的偏移量，而是将偏移量调整到最后两个步长的中间位置。对该点的高度进行采样。如果我们结束在表面以下，向表面之上方向移动四分之一，并再次采样。如果我们在表面上结束，向表面之下方向移动四分之，并再次采样。不断重复这个过程。 越来越接近交点 上述方法是二分查找的一个应用。它与地形测绘方法最匹配。每走一步，路程减半，直到到达目的地。在我们的例子中，我们将简单地做固定次数，以达到预期的解决方案。一步，得到0.5。两步，得到0.25、0.75。三步，是0.125、0.375、0.625、0.875。注意，从第二步开始，每次采样提升分的辨率将翻倍。 为了控制是否使用此方法，我们定义_PARALLAX_RAYMARCHING_SEARCH_STEPS_。默认情况下将其设置为零，这意味着我们根本不进行搜索。如果它被定义为大于0，我们将不得不使用另一个循环。注意，这种方法与_PARALLAX_RAYMARCHING_INTERPOLATE_是不兼容的，因为我们不能再保证表面是交叉的最后两个步骤。当我们搜索的时候，禁用插值。 #if !defined(PARALLAX_RAYMARCHING_SEARCH_STEPS) #define PARALLAX_RAYMARCHING_SEARCH_STEPS 0 #endif #if PARALLAX_RAYMARCHING_SEARCH_STEPS &gt; 0 for (int i = 0; i &lt; PARALLAX_RAYMARCHING_SEARCH_STEPS; i++) { } #elif defined(PARALLAX_RAYMARCHING_INTERPOLATE) float prevDifference = prevStepHeight - prevSurfaceHeight; float difference = surfaceHeight - stepHeight; float t = prevDifference / (prevDifference + difference); uvOffset = prevUVOffset - uvDelta * t; #endif 此循环也执行与原始循环相同的基本工作。调整偏移量和步高，然后采样高度字段。 for (int i = 0; i &lt; PARALLAX_RAYMARCHING_SEARCH_STEPS; i++) { uvOffset -= uvDelta; stepHeight -= stepSize; surfaceHeight = GetParallaxHeight(uv + uvOffset); } 但每次迭代，UV增量和步长减半。 for (int i = 0; i &lt; PARALLAX_RAYMARCHING_SEARCH_STEPS; i++) { uvDelta *= 0.5; stepSize *= 0.5; uvOffset -= uvDelta; stepHeight -= stepSize; surfaceHeight = GetParallaxHeight(uv + uvOffset); } 同样，如果点在表面之下，我们必须朝相反的方向移动。 uvDelta *= 0.5; stepSize *= 0.5; if (stepHeight &lt; surfaceHeight) { uvOffset += uvDelta; stepHeight += stepSize; } else { uvOffset -= uvDelta; stepHeight -= stepSize; } surfaceHeight = GetParallaxHeight(uv + uvOffset); 调整着色器，所以它使用三个搜索步骤 #define PARALLAX_BIAS 0 //#define PARALLAX_OFFSET_LIMITING #define PARALLAX_RAYMARCHING_STEPS 10 #define PARALLAX_RAYMARCHING_INTERPOLATE #define PARALLAX_RAYMARCHING_SEARCH_STEPS 3 #define PARALLAX_FUNCTION ParallaxRaymarching 10步长加上3个二分查找的最终效果 结果看起来相当不错，但仍不完美。二分法搜索可以比简单的插值处理较浅的角度，但仍然需要相当多的搜索步骤，以摆脱分层。所以这是一个试验的问题，找出哪种方法在特定情况下最有效，需要多少步骤。 缩放对象和动态批处理 尽管我们的视差映射方法似乎可行，但存在一个隐藏的错误。 而且还把错误显示出来了。它显示了何时使用动态批处理来组合已缩放的对象。 例如，给我们的四边形一个像$(10,10,10)$的比例，然后复制它，将副本移到它下面一点。 假设在播放器设置中启用了此选项，这将触发Unity动态批处理四边形。 批处理开始时，视差效果将扭曲。 旋转相机时，这一点非常明显。 但是，这仅发生在游戏视图和构建中，而不发生在场景视图中。 请注意，standard着色器也存在此问题，但是当使用弱偏移视差效果时，您可能不会立即注意到它。 动态批处理会产生奇怪的结果 在批处理将它们合并到一个单一的网格中之后，Unity不能标准化处理后的几何法向量和切向量。因此顶点数据正确的假设不再成立。 顶点法向量和切向量没有规范化不是什么大的问题，因为我们在顶点程序中将视图向量转换到切线空间。对于其他所有内容，数据在使用之前都要标准化。 解决方法是在构造对象转换到切线矩阵之前对向量进行归一化。 因为只有动态批处理的缩放几何才需要此选项，所以根据是否定义了PARALLAX_SUPPORT_SCALED_DYNAMIC_BATCHING，将其设为可选。 #if defined (_PARALLAX_MAP) #if defined(PARALLAX_SUPPORT_SCALED_DYNAMIC_BATCHING) v.tangent.xyz = normalize(v.tangent.xyz); v.normal = normalize(v.normal); #endif float3x3 objectToTangent = float3x3( v.tangent.xyz, cross(v.normal, v.tangent.xyz) * v.tangent.w, v.normal ); i.tangentViewDir = mul(objectToTangent, ObjSpaceViewDir(v.vertex)); #endif #define PARALLAX_BIAS 0 //#define PARALLAX_OFFSET_LIMITING #define PARALLAX_RAYMARCHING_STEPS 10 #define PARALLAX_RAYMARCHING_INTERPOLATE #define PARALLAX_RAYMARCHING_SEARCH_STEPS 3 #define PARALLAX_FUNCTION ParallaxRaymarching #define PARALLAX_SUPPORT_SCALED_DYNAMIC_BATCHING 动态批量与正确的结果" }, { "title": "Unity GPU Instance(翻译十九)", "url": "/posts/Unity_GPU_Instance/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-24 20:00:00 +0800", "content": "本篇摘要： 渲染大量球体-优化DrawCall 支持GPU-Instance 使用材质属性块 LOD-Groups支持GPU-Instance Batching Instance-批处理 指示GPU绘制需要花时间；向其传递mesh和material属性也要花时间。现在已知两种节省Draw Call的方式：static和dynamic batching Unity可以将多个静态物体的网格合并为一个更大的静态网格，从而减少draw call。 注意： 只有使用相同材质的对象才能以这种方式组合。 这是以必须存储更多网格数据为代价的。 启用动态批处理后，Unity在运行时会对视图中的动态对象执行相同的操作。 这仅适用于小型网格物体，否则开销将变得太大。 还有另一种组合draw call的方法：GPU instance或Geometry instance。与动态批处理一样，此操作在运行时针对可见对象。 它的目标是让GPU一次性渲染同一网格의多个副本。 因此，它不能组合不同的网格或材质。 创建大量球体 im title? test test test. test test test. test test test!!dfS:FDFH&amp;*YER#. using UnityEngine; public class GPUInstancingTest : MonoBehaviour { public Transform prefab; public int instances = 5000; public float radius = 50f; //单位圆内随机一点并放大坐标50倍，生成5000个球体 //然后查看statistics统计的draw Call信息 void Start () { for (int i = 0; i &lt; instances; i++) { Transform t = Instantiate(prefab); t.localPosition = Random.insideUnitSphere * radius; t.SetParent(transform); } } } 使用forward render path统计到的draw call，去掉背景和camera Effect两个draw call： 5000 draw call 但是当使用cube代替球体 6 draw call 支持Instance 默认情况下，GPU Instance不会开启，必须设计shader以支持它。 即使这样，也必须为每种材料显式启用实例化。 Unity的standard着色器有一个开关。像标准着色器的GUI一样，我们将为shader扩展面板创建“高级选项”部分。 可以通过调用MaterialEditor.EnableInstancingField方法来添加切换。 void DoAdvanced () { GUILayout.Label(\"Advanced Options\", EditorStyles.boldLabel); editor.EnableInstancingField(); } 仅当shader实际支持instance时，才会显示该切换。 我们可以通过将#pragma multi_compile_instancing指令添加到着色器base-pass启用此支持。 这将为一些关键字启用着色器变体，自定义关键字_INSTANCING_ON_，其他关键字也可以。 #pragma multi_compile_fwdbase #pragma multi_compile_fog #pragma multi_compile_instancing 合并了，但是显示有错误 批处理数量已减少到42，这意味着现在仅用40个批处理即可渲染所有5000个球体。帧速率也高达80 fps，但是只有几个球体可见。错误原因：虽然5000个球体仍在渲染，但是在合批中同一批次的所有球体的顶点转换时都使用了同一个位置：它们都使用同一批次中第一个球的转换矩阵。 发生这种情况是因为现在同一批中所有球体的矩阵都作为数组发送到GPU。 在不告知着色器要使用哪个数组索引的情况下，它始终使用第一个索引。 Instance IDs 上述错误解决办法：每个Instance相对应的数组索引称为其Instance ID，GPU通过顶点数据将其传递到着色器的vertex程序。在大多数平台上，它是一个无符号整数，名为instanceID，具有SV_InstanceID语义。 我们可以简单地使用_UNITY_VERTEX_INPUT_INSTANCE_ID_宏将其包含在我们的VertexData结构中。 它在UnityCG中包含的_UnityInstancing.cginc_文件中定义。 它为我们提供了实例ID的正确定义，或者在未启用实例化时不提供任何内容。将其添加到VertexData结构。 struct VertexData { UNITY_VERTEX_INPUT_INSTANCE_ID float4 vertex : POSITION; }; 启用instance后，我们现在可以在顶点程序中访问instanceID。 有了它，我们可以在变换顶点位置时使用正确的矩阵。 但是，UnityObjectToClipPos函数没有矩阵参数，它函数内部始终使用unity_ObjectToWorld矩阵。要解决此问题，UnityInstancing包含文件会使用矩阵数组的宏覆盖unity_ObjectToWorld。 这可以被认为是肮脏的宏技巧，但无需更改现有着色器代码即可工作，从而确保了向后兼容性。 要使它工作，instance的数组索引必须对所有着色器代码全局可用。必须通过_UNITY_SETUP_INSTANCE_ID_宏进行手动设置，该宏必须在vertex程序最先计算，然后再执行其他的代码。 InterpolatorsVertex MyVertexProgram (VertexData v) { InterpolatorsVertex i; UNITY_INITIALIZE_OUTPUT(Interpolators, i); UNITY_SETUP_INSTANCE_ID(v); i.pos = UnityObjectToClipPos(v.vertex); } 矩阵替换内部实现？ //UnityInstancing中的实际代码要复杂得多。 它要处理平台差异，其他使用实例化的方法以及用于立 //体声渲染的特殊代码，从而导致间接定义的多个步骤。 它还必须重新定义UnityObjectToClipPos，因 //为UnityCG首先包含UnityShaderUtilities。 //缓冲区宏将在后面说明。 static uint unity_InstanceID; CBUFFER_START(UnityDrawCallInfo) // Where the current batch starts within the instanced arrays. int unity_BaseInstanceID; CBUFFER_END #define UNITY_VERTEX_INPUT_INSTANCE_ID uint instanceID : SV_InstanceID; #define UNITY_SETUP_INSTANCE_ID(input) unity_InstanceID = input.instanceID + unity_BaseInstanceID; // Redefine some of the built-in variables // macros to make them work with instancing. UNITY_INSTANCING_CBUFFER_START(PerDraw0) float4x4 unity_ObjectToWorldArray[UNITY_INSTANCED_ARRAY_SIZE]; float4x4 unity_WorldToObjectArray[UNITY_INSTANCED_ARRAY_SIZE]; UNITY_INSTANCING_CBUFFER_END #define unity_ObjectToWorld unity_ObjectToWorldArray[unity_InstanceID] #define unity_WorldToObject unity_WorldToObjectArray[unity_InstanceID] 批处理大小 每台设备不一样，最终得到的批次数量可能与当前实验得到的数量不同。现在这情况下，以40批渲染5000个球体实例，这意味着每批125个球体。 每个批次都需要自己的矩阵数组。 此数据发送到GPU并存储在内存缓冲区中，在Direct3D中称为常量缓冲区，在OpenGL中称为统一缓冲区。 这些缓冲区具有最大大小，这限制了一批中可以容纳多少个实例。 假设台式机GPU每个缓冲区的限制为64KB。 一个矩阵由16个浮点数组成，每个浮点数均为4个字节。 因此，每个矩阵64个字节。 每个实例都需要一个对象到世界的转换矩阵。 但是，我们还需要一个世界到对象的矩阵来转换法线向量。 因此，最终每个实例有128个字节。 这导致最大批处理大小为“ 64000/128 = 500”，这只能在10个批处理中渲染5000个球体实例。 内存单位是2进制，所以1KB表示1024字节，而不是1000。因此，'(64 * 1024)/ 128 = 512 '。UNITY_INSTANCED_ARRAY_SIZE默认定义为500，但您可以使用编译器指令覆盖它。例如，#pragma instancing_options maxcount:512将最大值设置为512。但是，这将导致断言失败错误，因此实际限制为511。到目前为止，500和512之间没有太大的差别。 即使假设台式机的最大容量为64KB成立，但是大多数移动设备的最大容量远远达不到64，可能仅为16KB。 Unity通过在针对OpenGL ES 3，OpenGL Core或Metal时将最大值除以四来解决此问题。 因为我在编辑器中使用的是OpenGL Core，所以最终的最大批处理大小为“ 500/4 = 125”。 可以通过添加编译器指令#pragma instancing_options force_same_maxcount_for_gl来禁用此自动减少功能。 多个instance选项组合在同一指令中。 但是，这可能会导致在部署到移动设备上时发生故障，因此请小心使用。 那假设均等缩放选项呢？ 可以使用#pragma instancing_options指示所有instance对象具有统一的缩放比例。 这消除了将世界到对象矩阵用于法线转换的需要(少存储一个矩阵)。 设置此选项后，虽然UnityObjectToWorldNormal函数确实会更改其行为，但它不会消除第二个矩阵数组。 因此，至少 在Unity 2017.1.0中，此选项实际上没有任何作用。 Instance Shadows 到目前为止，一直没有阴影。 重新打开主阴影的Soft shadow，并确保阴影距离足以包含所有球体 批处理爆炸 为大量物体渲染阴影会增加GPU耗能。但是我们也可以在渲染球体阴影时使用GPU instance。在shadow caster-pass中添加instance指令；同时也增加UNITY_VERTEX_INPUT_INSTANCE_ID and UNITY_SETUP_INSTANCE_ID #pragma multi_compile_shadowcaster #pragma multi_compile_instancing struct VertexData { UNITY_VERTEX_INPUT_INSTANCE_ID }; InterpolatorsVertex MyShadowVertexProgram (VertexData v) { InterpolatorsVertex i; UNITY_SETUP_INSTANCE_ID(v); } instanced 阴影 多光源 我们仅在base-pass和shadow caster-pass中添加了instance支持。 因此，批处理不适用于其他光源。 要验证这一点，停用主光源并添加一些会影响多个球体的聚光灯或点光源。 不要为它们打开阴影，因为那样会降低帧速率。 批处理爆炸 上图，完全不支持多光源批处理。 要将instance与多个光源结合使用，只能切换到延迟渲染路径。 为此，请将所需的编译器指令添加到着色器的延迟传递中。 #pragma multi_compile_prepassfinal #pragma multi_compile_instancing 多光源instance Mixing Material Properties 所有批处理都有一个限制：它们仅限于具有相同材料的对象。 当我们希望渲染的对象具有多样性时，此限制就会成为问题。 随机着色 随机改变球体的颜色 void Start () { for (int i = 0; i &lt; instances; i++) { Transform t = Instantiate(prefab); t.localPosition = Random.insideUnitSphere * radius; t.SetParent(transform); t.GetComponent&lt;MeshRenderer&gt;().material.color = new Color(Random.value, Random.value, Random.value); } } 球体与随机的颜色，没有批量和阴影 即使我们为物料启用了批处理，它也不再起作用。由于每个球体现在都有自己的材质，因此每个球体的着色器状态也必被更改。 这显示在统计面板中为SetPass call的数量。在这修改之前只有少量几个批次渲染，但是现在是5000加批次。 材质属性块-Material Property Blocks 除了为每个球体创建新的材质实例外，我们还可以使用材质属性块。 这些是小的修改，设置属性块的颜色并将其传递给球体的渲染器，而不是直接分配材质的颜色。MaterialPropertyBlock官网介绍; Property Buffers-属性缓冲区 渲染instance对象时，Unity通过数组形式将颜色数据传递到GPU内存并转换矩阵，Unity对存储在材料属性块中的属性执行相同的操作，但要使其起作用的话，我们必须在shader中定义一个instance的缓冲区。 声明instance缓冲区的工作类似于创建诸如插值器之类的结构，但是确切的语法因平台而异。 我们可以使用UNITY_INSTANCING_CBUFFER_START和UNITY_INSTANCING_CBUFFER_END宏来解决差异。 启用实例化后，它们将不执行任何操作。 将_Color变量的定义放在instance缓冲区中。 UNITY_INSTANCING_CBUFFER_START宏需要一个名称参数，实际名称无关紧要但要注意避免重名冲突。 宏以UNITY_INSTANCING_为其前缀。 UNITY_INSTANCING_CBUFFER_START(InstanceProperties) float4 _Color; UNITY_INSTANCING_CBUFFER_END 像变换矩阵一样，启用instance后，颜色数据作为数组上传到GPU。UNITY_DEFINE_INSTANCED_PROP宏会为我们处理正确的声明语法。 UNITY_INSTANCING_CBUFFER_START(InstanceProperties) //float4 _Color; UNITY_DEFINE_INSTANCED_PROP(float4, _Color) UNITY_INSTANCING_CBUFFER_END 最后要访问fragment程序中的数组，我们还需要在其中知道instanceID。 因此，将其添加到插值器结构中。 struct InterpolatorsVertex { UNITY_VERTEX_INPUT_INSTANCE_ID }; struct Interpolators { UNITY_VERTEX_INPUT_INSTANCE_ID }; 在vertex顶点程序中，将ID从顶点数据复制到插值器。 启用实例化时，UNITY_TRANSFER_INSTANCE_ID宏定义此简单操作，否则不执行任何操作。 InterpolatorsVertex MyVertexProgram (VertexData v) { InterpolatorsVertex i; UNITY_INITIALIZE_OUTPUT(Interpolators, i); UNITY_SETUP_INSTANCE_ID(v); UNITY_TRANSFER_INSTANCE_ID(v, i); } 在片段程序的开头，使ID全局可用，就像在顶点程序中一样。 FragmentOutput MyFragmentProgram (Interpolators i) { UNITY_SETUP_INSTANCE_ID(i); } 现在，我们必须在不使用instance时以_Color的形式访问颜色，而在启用实例化时以_Color [unity_InstanceID]的形式访问颜色。 使用UNITY_ACCESS_INSTANCED_PROP宏可同时支持上述两种访问。 float3 GetAlbedo (Interpolators i) { float3 albedo = tex2D(_MainTex, i.uv.xy).rgb * UNITY_ACCESS_INSTANCED_PROP(_Color).rgb; } float GetAlpha (Interpolators i) { float alpha = UNITY_ACCESS_INSTANCED_PROP(_Color).a; } 新版本如果编译有错误： 从2017.3及以上版本, UNITY_ACCESS_INSTANCED_PROP macro改了它需要的两个参数：buffer名，颜色名使用UNITY_ACCESS_INSTANCED_PROP(InstanceProperties, _Color). 现在，我们的颜色随机的球再次被批处理。 我们可以用相同的方式使其他属性可变。 对于颜色，浮点数，矩阵和四分量浮点向量，这是可能的。 如果要改变纹理，可以使用单独的纹理数组，并将索引添加到实例化缓冲区。其他属性修改类似。 可以在同一个缓冲区中组合多个属性，但要牢记大小限制。 还应注意，缓冲区被划分为32位块，因此单个浮点数需要与向量相同的空间。 您也可以使用多个缓冲区，但是也有一个限制，它们不是免费提供的。 启用instance后，每个要缓冲的属性都将成为一个数组，因此 仅对需要根据instance变化的属性执行此操作。 阴影 我们的阴影也取决于颜色。 调整shader阴影以便每个实例也可以支持唯一的颜色。 //float4 _Color; UNITY_INSTANCING_CBUFFER_START(InstanceProperties) UNITY_DEFINE_INSTANCED_PROP(float4, _Color) UNITY_INSTANCING_CBUFFER_END struct InterpolatorsVertex { UNITY_VERTEX_INPUT_INSTANCE_ID }; struct Interpolators { UNITY_VERTEX_INPUT_INSTANCE_ID }; float GetAlpha (Interpolators i) { float alpha = UNITY_ACCESS_INSTANCED_PROP(_Color).a; } InterpolatorsVertex MyShadowVertexProgram (VertexData v) { InterpolatorsVertex i; UNITY_SETUP_INSTANCE_ID(v); UNITY_TRANSFER_INSTANCE_ID(v, i); } float4 MyShadowFragmentProgram (Interpolators i) : SV_TARGET { UNITY_SETUP_INSTANCE_ID(i); } LOD Instance void Start () { MaterialPropertyBlock properties = new MaterialPropertyBlock(); for (int i = 0; i &lt; instances; i++) { Transform t = Instantiate(prefab); t.localPosition = Random.insideUnitSphere * radius; t.SetParent(transform); //MaterialPropertyBlock properties = new MaterialPropertyBlock(); properties.SetColor ( \"_Color\", new Color(Random.value, Random.value, Random.value) ); //t.GetComponent&lt;MeshRenderer&gt;().SetPropertyBlock(properties); MeshRenderer r = t.GetComponent&lt;MeshRenderer&gt;(); if (r != null) { r.SetPropertyBlock(properties); } else { //对LOD子对象设置颜色 for (int ci = 0; ci &lt; t.childCount; ci++) { r = t.GetChild(ci).GetComponent&lt;MeshRenderer&gt;(); if (r) { r.SetPropertyBlock(properties); } } } } } 不幸的是没有有效的批处理。Unity能够对以相同的LOD颜色球体进行批处理，但是如果可以像往常一样进行批处理会更好。 我们可以通过用缓冲数组替换unity_LODFade来实现。可以通过为支持实例化的每个过程添加lodfade实例化选项来指示Unity的着色器代码执行此操作。 #pragma multi_compile_instancing #pragma instancing_options lodfade instance LOD fading" }, { "title": "Unity 实时 GI & LPPV & LOD(翻译十八)", "url": "/posts/Unity_RealTime_GI_LOD/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-23 20:00:00 +0800", "content": "本篇摘要： 支持实时全局光照 用动画控制发光对GI的贡献 使用光照探针代理体LPPV LOD组与GI结合 LOD之间的淡入淡出 从这篇开始，这个系列教程将由Unity 2017.1.0f3来完成。后续的Shader新特性Unity的旧版本没有，因为我们要使用一个新的着色器函数。 实时全局光照 　　烘焙光照在静态物体上工作的非常好，对于动态几何体，由于有光照探针的缘故，烘焙光照这种方法也能工作的非常好。但是，烘焙光照不能处理动态光源。混合模式的光源可以通过一些实时的调节来消除，但调节的太多使得烘焙出来的间接光照不会改变。所以当你有一个户外场景的话，使用烘焙光照这种方法太阳的光照就不能有变化。太阳不能像在现实生活中一样在天空中移动，因为如果需要太阳在天空中移动的话，就需要逐渐变化的全局光照。所以场景必须一直不变。 　　为了使间接光照能够在移动的太阳这样的情况发挥作用，Unity使用Enlighten系统来计算实时全局光照。除了在运行时计算光照和光照探针以外，它还采用烘焙间接光照一样的方式来工作。 　　了解间接光需要知道光在静态表面之间如何反射。重点在于哪些表面可能会受到其他表面的影响，以及程度如何。弄清这些关系需要做很多的工作，不能实时完成。所以这个数据由编辑器处理并存储在运行时使用。然后 Enlighten系统会使用这个数据来计算实时光照贴图和探针数据。即使如此，只有低分辨率的光照贴图才可以在实时情况下运行。 启用全局光照 实时全局光照、烘焙全局光照都可以独立启用。你可以同时启用两个，或者启用其中的一个，或者两个都不启用。这两个选项都是通过“光照”窗口的“实时照光照”部分中的复选框启用。 实时全局光照和烘焙光照同时启用的状态 要实际查看实时全局光照，请将测试场景中的主光源的模式设置为实时模式。 由于我们没有其他光源，即使启用了烘焙光照也能有效地关闭。 主光源设置为实时模式 确保场景中的所有对象都使用我们的白色材质。 像上次一样，球体都是动态的，而其他的都是静态几何体。 只有动态对象能接收实时的全局光照 事实证明，只有动态对象会受益于实时全局光照。静态物体会变的暗一点。这是因为光照探针自动并入实时全局光照。而静态对象必须对实时的光照贴图进行采样，而这些光照贴图与烘焙的光照贴图不同。我们的着色器还不支持。 烘焙的全局光照 Unity在编辑模式下已经生成了实时的光照贴图，所以你可以随时查看实时的全局光照贴图。在编辑模式和播放模式之间进行切换的时候，这些贴图不会被保留，但是它们最终会得到相同的结果。你可以通过“光照”窗口的“对象贴图”选项来选择一个光照贴图静态对象对实时 光照贴图进行检查。 选择“实时强度“可以可视化的查看实时光照贴图的数据。 实时光照贴图，屋顶被选中时候的状态 虽然实时光照贴图已经被烘焙出来，并且它们还可能显示正确，但我们的meta渲染通道实际上使用的是错误的坐标。实时全局光照具有自己的光照贴图坐标，最终可能与静态光照贴图的坐标不同。Unity会根据光照贴图和对象的设置来自动生成这些坐标。这些数据存储在第 三套UV中。所以将这些数据添加到My Lightmapping中的VertexData里面。 struct VertexData { float4 vertex : POSITION; float2 uv : TEXCOORD0; float2 uv1 : TEXCOORD1; float2 uv2 : TEXCOORD2; }; 现在，MyLightmappingVertexProgram必须使用第二个或是第三个UV坐标，以及静态或动态光照贴图的大小和偏移量。 我们可以依靠UnityMetaVertexPosition函数来使用正确的数据。 Interpolators MyLightmappingVertexProgram (VertexData v) { Interpolators i; // v.vertex.xy = v.uv1 * unity_LightmapST.xy + unity_LightmapST.zw; // v.vertex.z = v.vertex.z &gt; 0 ? 0.0001 : 0; // i.pos = UnityObjectToClipPos(v.vertex); i.pos = UnityMetaVertexPosition( v.vertex, v.uv1, v.uv2, unity_LightmapST, unity_DynamicLightmapST ); i.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); return i; } UnityMetaVertexPosition是什么样子的？ 它除了通过unity_MetaVertexControl提供的标志来决定使用哪些坐标集和光照贴图之外，它还做了我们以前做的工作。 float4 UnityMetaVertexPosition ( float4 vertex, float2 uv1, float2 uv2, float4 lightmapST, float4 dynlightmapST ) { if (unity_MetaVertexControl.x) { vertex.xy = uv1 * lightmapST.xy + lightmapST.zw; // OpenGL right now needs to actually use incoming vertex position, // so use it in a very dummy way vertex.z = vertex.z &gt; 0 ? 1.0e-4f : 0.0f; } if (unity_MetaVertexControl.y) { vertex.xy = uv2 * dynlightmapST.xy + dynlightmapST.zw; // OpenGL right now needs to actually use incoming vertex position, // so use it in a very dummy way vertex.z = vertex.z &gt; 0 ? 1.0e-4f : 0.0f; } return UnityObjectToClipPos(vertex); } 请注意，meta渲染通道既用于烘焙光照贴图，也用于实时光照贴图。所以当使用实时全局光照的时候，meta渲染通道也将被包含在构建中。 对实时光照贴图进行采样 为了对实时光照贴图进行采样，我们还必须将第三个UV坐标添加到My Lightmapping中的VertexData里面。 struct VertexData { float4 vertex : POSITION; float3 normal : NORMAL; float4 tangent : TANGENT; float2 uv : TEXCOORD0; float2 uv1 : TEXCOORD1; float2 uv2 : TEXCOORD2; }; 当一张实时光照贴图被使用的时候，我们必须将这个光照贴图的坐标添加到我们的插值器中去。标准着色器在单个插值器中将两个光照贴图的坐标集合组合起来 - 与其他数据复用 - 但是我们可以为两者准备单独的插值器。当_DYNAMICLIGHTMAP_ON_关键字被定义的时候，我们知道有动态光照数据。它是multi_compile_fwdbase编译器指令的关键字列表的一部分。 struct Interpolators { #if defined(DYNAMICLIGHTMAP_ON) float2 dynamicLightmapUV : TEXCOORD7; #endif }; 填充坐标就像对静态光照贴图的坐标所做的事情一样，除了动态光照图的缩放比例和偏移量的设置以外，这些可以通过unity_DynamicLightmapST变得可用。 Interpolators MyVertexProgram (VertexData v) { #if defined(LIGHTMAP_ON) || ADDITIONAL_MASKED_DIRECTIONAL_SHADOWS i.lightmapUV = v.uv1 * unity_LightmapST.xy + unity_LightmapST.zw; #endif #if defined(DYNAMICLIGHTMAP_ON) i.dynamicLightmapUV = v.uv2 * unity_DynamicLightmapST.xy + unity_DynamicLightmapST.zw; #endif } 对实时光照贴图的采样是在我们的CreateIndirectLight函数中完成的。复制 #if defined(LIGHTMAP_ON) 代码块并进行一些更改。 首先，新的部分是基于DYNAMICLIGHTMAP_ON关键字的。 此外，它应该使用DecodeRealtimeLightmap而不是DecodeLightmap，这是因 为实时光照贴图使用不同的颜色格式。而且因为这些数据可能被添加到烘焙光照中，不要立即分配给indirectLight.diffuse，而是使用最后添加的中间变量。 最后，当不使用烘焙光照贴图和实时光照贴图的时候，我们只应该对球面谐波进行采样。 #if defined(LIGHTMAP_ON) indirectLight.diffuse = DecodeLightmap(UNITY_SAMPLE_TEX2D(unity_Lightmap, i.lightmapUV)); #if defined(DIRLIGHTMAP_COMBINED) float4 lightmapDirection = UNITY_SAMPLE_TEX2D_SAMPLER( unity_LightmapInd, unity_Lightmap, i.lightmapUV ); indirectLight.diffuse = DecodeDirectionalLightmap( indirectLight.diffuse, lightmapDirection, i.normal ); #endif ApplySubtractiveLighting(i, indirectLight); // #else // indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); #endif #if defined(DYNAMICLIGHTMAP_ON) float3 dynamicLightDiffuse = DecodeRealtimeLightmap( UNITY_SAMPLE_TEX2D(unity_DynamicLightmap, i.dynamicLightmapUV) ); #if defined(DIRLIGHTMAP_COMBINED) float4 dynamicLightmapDirection = UNITY_SAMPLE_TEX2D_SAMPLER( unity_DynamicDirectionality, unity_DynamicLightmap, i.dynamicLightmapUV ); indirectLight.diffuse += DecodeDirectionalLightmap( dynamicLightDiffuse, dynamicLightmapDirection, i.normal ); #else indirectLight.diffuse += dynamicLightDiffuse; #endif #endif #if !defined(LIGHTMAP_ON) &amp;&amp; !defined(DYNAMICLIGHTMAP_ON) indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); #endif 把实时全局光照应用于一切物体之上 现在我们的着色器使用的是实时光照贴图.最初，当使用Distance Shadowmask模式的时候，它的效果可能看起来与使用混合光源的烘焙光照的效果相同。当在播放模式下关闭光源的时候，差异就变得非常明显。 禁用混合光源以后，间接光照仍然被保留 禁用混合光源以后，其间接光照将保持不变。相比之下，实时光照的间接贡献就会消失，并重新出现 - 这是应该出现的情况。 不过，新情况的完全烘焙好可能需要一段时间。 Enlighten系统会逐步调整光照贴图和光照探针。 这种情况发生的速度取决于场景的复杂性和实 时全局光照CPU质量设置。 Your browser does not support the video tag. Here is a link to the video file instead. 光照切换 所有实时光源都对实时全局光照有贡献。 然而，它的典型用途是那些仅在主要方向上存在光线的光源，比如可以代表太阳，因为它在天空中移动。它适用于方向光源。点光源和聚光光源也能工作，但只是没有阴影。所以当使用带有阴影的点光源或聚光光源的时候，你可 能会遇到不正确的间接光照结果。 没有影响的间接光源和实时的聚光光源 如果要从实时全局光照里面去掉一个实时光源，可以通过设置它的Indirect Multiplier将它的光强度设置为零。 自发光光源 实时全局光照也可以用于自发光的静态物体。这使得可以匹配实时间接光照来改变物体的自发光变得可能。让我们来试试看吧。向场景中添加一个静态球体，并赋予它一个使用我们着色器的材质，这个材质具有黑色的反照率和白色的自发光颜色。最初，我们只能看到通过静态光照贴图实现的自发光的间接效果。 用自发光球来烘焙全局光照 要将自发光光源烘焙到静态光照贴图中，我们必须在我们的着色器的GUI中设置材质的全局光照标志。因为我们总是将标志设置为BakedEmissive，光源最终将以烘焙好的光照贴图的形式出现。如果自发光光源是恒定的这个效果是很不错的，但这样就不允许我们做动画控制。 为了同时对自发光光源支持烘焙和实时光照，我们必须使其可配置化。我们可以通过向MyLightingShaderGUI中添加一个选项来做到这一点，使用的是MaterialEditor.LightmapEmissionProperty方法。这个方法的单个参数是属性的缩进级别。 void DoEmission () { MaterialProperty map = FindProperty(\"_EmissionMap\"); Texture tex = map.textureValue; EditorGUI.BeginChangeCheck(); editor.TexturePropertyWithHDRColor( MakeLabel(map, \"Emission (RGB)\"), map, FindProperty(\"_Emission\"), emissionConfig, false ); editor.LightmapEmissionProperty(2); if (EditorGUI.EndChangeCheck()) { if (tex != map.textureValue) { SetKeyword(\"_EMISSION_MAP\", map.textureValue); } foreach (Material m in editor.targets) { m.globalIlluminationFlags = MaterialGlobalIlluminationFlags.BakedEmissive; } } } 每次当自发光属性发生改变的时候，我们也必须停止覆盖这个标志位。其实真正要做的事情比这更复杂一点。其中一个标志选项是EmissiveIsBlack，这个表示表示的是自发光计算可以跳过。这个标志总是会针对新材质进行设置。要让间接自发光能够工作，我们必须保证这 个标志不被设置，无论我们选择实时光照还是烘焙。我们可以通过总是屏蔽标志值的EmissiveIsBlack位来做到这一点。 foreach (Material m in editor.targets) { m.globalIlluminationFlags &amp;= ~MaterialGlobalIlluminationFlags.EmissiveIsBlack; } 带有自发光球的实时全局光照效果 烘焙全局光照和实时全局光照之间的视觉差异主要是因为实时光照贴图通常具有比烘焙全局光照更低的分辨率。所以当自发光不发生不变化的时候，你也可以使用烘焙全局光照，确保能够利用其更高的分辨率。 EmissiveIsBlack的目的是什么？ 这是一个优化，使得计算可以跳过全局光照烘焙过程。然而，只有当自发光颜色确实是黑色的时候，它才依赖于标志。由于这个标志位由着色器的GUI进行设置，这是当材质在检视器里面进行编辑的时候确定的。或者至少，这是Unity的标准着色器的做法。因此，如果自发光颜色稍后被脚本或动画系统更改，则该标志位不会做相应的调整。这是许多人不理解为什么对自发光做动画不会影响到实时全局光照的原因。结果就是如果你想在运行时更改自发光颜色，那么就不要将自发光颜色设置为纯黑色。 我们没有使用这种方法，我们使用的是LightmapEmissionProperty，它还提供了对自发光完全关闭全局光照的选项。 所以这个选择对于用户来说是非常明确的，没有任何隐藏的行为。如果用户不要使用自发光？ 那么只要确保它的全局光照被设置为None就可以了。 对自发光进行动画控制 用于自发光的实时全局光照只能用于静态对象。虽然物体是静态的，但其材质的自发光属性还是可以被动画化，并且将被全局光照系统所捕获到。让我们用一个在自发光颜色为白色和自发光颜色为黑色之间振荡的简单组件来尝试下这个事情。 using UnityEngine; public class EmissiveOscillator : MonoBehaviour { Material emissiveMaterial; void Start () { emissiveMaterial = GetComponent&lt;MeshRenderer&gt;().material; } void Update () { Color c = Color.Lerp( Color.white, Color.black, Mathf.Sin(Time.time * Mathf.PI) * 0.5f + 0.5f ); emissiveMaterial.SetColor(\"_Emission\", c); } } 将这个组件添加到我们的自发光球体。在播放模式下，自发光将会动画化，但间接光照不受影响。我们必须通知实时光照系统，它有工作要做。这可以通过调用适当网格渲染器的Renderer.UpdateGIMaterials方法来完成。 MeshRenderer emissiveRenderer; Material emissiveMaterial; void Start () { emissiveRenderer = GetComponent&lt;MeshRenderer&gt;(); emissiveMaterial = emissiveRenderer.material; } void Update () { emissiveMaterial.SetColor(\"_Emission\", c); emissiveRenderer.UpdateGIMaterials(); } 动画控制实时GI 调用UpdateGIMaterials方法会触发物体自发光的完整更新，并使用其meta渲染通道进行渲染。当自发光比纯色更复杂的时候，这是必要的，举个简单的例子来说，比如说我们使用纹理。如果一个纯色就足够了，那么我们可以通过使用渲染器和自发光颜色调用DynamicGI.SetEmissive方法来得到一个比较快捷的计算方式。这比使用meta渲染通道来渲染物体更快，所以在能够使用的时候可以利用这种方法。 //emissiveRenderer.UpdateGIMaterials(); DynamicGI.SetEmissive(emissiveRenderer, c); 光照探针 烘焙全局光照和实时全局光照都通过光照探针应用于动态对象。物体的位置用于对光探针数据进行插值，然后将其用于全局光照。这对于相当小的物体来说下效果很好，但对于较大的物体来说就太粗糙了。 举个简单的例子来是说，将做了比较大拉伸的立方体添加到测试场景，以便它可以受到不同的光照条件的影响。它应该使用我们的白色材质。由于它是一个动态立方体，所以最终使用一个点来确定它的全局光照贡献。让我们移动这个点的位置，使得这一点最终处于一个被遮蔽的位置，那么整个立方体就会变黑，这显然是错误的。为了使这一点非常明显，让我们使用一个烘焙主光源，所以所有光照都来自烘焙全局光照和实时全局光照的数据。 对于大型动态物体来说，光照效果不好 为了使光照探针器适用于这样的情况，我们可以使用光照探针代理体，或者简称为LPPV。这可以通过向着色器发送插值后的探针器数据网格而不是单个插值后的探针器数据来做到。这需要具有线性滤波的浮点数3D纹理，这就将这个方法限制到只能在现代显卡上使用。此外，还要确保在图形层设置中启用LPPV（光照探针代理体）支持。 启用了LPPV（光照探针代理体）支持 向物体中添加一个光照探针代理体 光照探针代理体可以以各种方式设置，最直接的方法是在作为使用光照探针代理体的物体的一个组件。你可以通过Component / Rendering / Light Probe Proxy Volume来添加它。 光照探针代理体组件 LPPV（光照探针代理体）通过在运行时在光照探针之间进行插值来工作，就好像它们是常规动态对象的网格一样。插值后得到的结果被缓存，刷新模式（Refresh Mode）控制在何时进行更新。默认值为“自动（Automatic）”，这意味着当动态全局光照更改和探针器组发生移 动的时候会触发更新。包围盒模式（Bounding Box Mode）控制着代理体的定位。自动本地化（AutomaticLocal ）意味着它会去匹配其附着的对象的包围盒。这些默认设置适用于我们的立方体，因此我们将保留这些设置。 要使我们的立方体实际使用LPPV（光照探针代理体），我们必须将其网格渲染器的光照探针(Light Probes)模式设置为使用光照探针代理体（Use ProxyVolume）。默认行为是使用对象本身的LPPV（光照探针代理体）组件，但也可以强制使用另一个代理体。 使用一个光照探针代理体而不是常规的探针器 自动分辨率模式（automaticresolution mode）对于我们的拉伸立方体不起作用。 因此，将“分辨率模式（Resolution Mode ）”设置为“自定义（Custom ）”，并确保立方体的角上有采样点，并沿着其长边有多个样本点。当你选中这个对象的时候，可以看到这些采样点。 自定义探针器分辨率以适应拉伸的立方体 对光照探针代理体进行采样 立方体已变黑，因为我们的着色器现在还不支持LPPV（光照探针代理体）采样。为了使其工作，我们必须在CreateIndirectLight函数内调整球面谐波代码。当使用LPPV（光照探针代理体）的时候，_UNITY_LIGHT_PROBE_PROXY_VOLUME_被定义为1。我们在这种情况下什么都不做，看看会发生什么。 #if !defined(LIGHTMAP_ON) &amp;&amp; !defined(DYNAMICLIGHTMAP_ON) #if UNITY_LIGHT_PROBE_PROXY_VOLUME //... #else indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); #endif #endif 没有更多球面谐波的效果 得到的结果是所有的球面谐波被禁用，对于不使用LPPV（光照探针代理体）的动态对象也是如此。这是因为_UNITY_LIGHT_PROBE_PROXY_VOLUME_在项目范围内定义，而不是对每个对象实例进行定义。单个对象是否使用LPPV由UnityShaderVariables中定义的unity_ProbeVolumeParams的X分量指定。如果unity_ProbeVolumeParams的X分量设置为1，那么我们有一个LPPV（光照探针代理体），否则我们应该使用常规的球面谐波。 #if UNITY_LIGHT_PROBE_PROXY_VOLUME if (unity_ProbeVolumeParams.x == 1) { //... } else { indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); } #else indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); #endif 要对光照探针代理体进行采样，我们可以使用SHEvalLinearL0L1_SampleProbeVolume函数而不是ShadeSH9。这个函数在UnityCG中进行定义，并且需要世界空间中的位置作为额外的参数。 if (unity_ProbeVolumeParams.x == 1) { indirectLight.diffuse = SHEvalLinearL0L1_SampleProbeVolume ( float4(i.normal, 1), i.worldPos ); indirectLight.diffuse = max(0, indirectLight.diffuse); } SHEvalLinearL0L1_SampleProbeVolume如何工作？ 顾名思义，该函数仅包括前两个球面谐波带L0和L1。 Unity不使用LPPV（光照探针代理体）的第三个波带。所以我们得到较低质量的光照近似值，但是我们在多个世界空间中的样本之间进行插值，而不是使用单个点。下面是这个函数的代码。 half3 SHEvalLinearL0L1_SampleProbeVolume (half4 normal, float3 worldPos) { const float transformToLocal = unity_ProbeVolumeParams.y; const float texelSizeX = unity_ProbeVolumeParams.z; //The SH coefficients textures and probe occlusion // are packed into 1 atlas. //------------------------- //| ShR | ShG | ShB | Occ | //------------------------- float3 position = (transformToLocal == 1.0f) ? mul(unity_ProbeVolumeWorldToObject, float4(worldPos, 1.0)).xyz : worldPos; float3 texCoord = (position - unity_ProbeVolumeMin.xyz) * unity_ProbeVolumeSizeInv.xyz; texCoord.x = texCoord.x * 0.25f; // We need to compute proper X coordinate to sample. Clamp the // coordinate otherwize we'll have leaking between RGB coefficients float texCoordX = clamp(texCoord.x, 0.5f * texelSizeX, 0.25f - 0.5f * texelSizeX); // sampler state comes from SHr (all SH textures share the same sampler) texCoord.x = texCoordX; half4 SHAr = UNITY_SAMPLE_TEX3D_SAMPLER( unity_ProbeVolumeSH, unity_ProbeVolumeSH, texCoord ); texCoord.x = texCoordX + 0.25f; half4 SHAg = UNITY_SAMPLE_TEX3D_SAMPLER( unity_ProbeVolumeSH, unity_ProbeVolumeSH, texCoord ); texCoord.x = texCoordX + 0.5f; half4 SHAb = UNITY_SAMPLE_TEX3D_SAMPLER( unity_ProbeVolumeSH, unity_ProbeVolumeSH, texCoord ); // Linear + constant polynomial terms half3 x1; x1.r = dot(SHAr, normal); x1.g = dot(SHAg, normal); x1.b = dot(SHAb, normal); return x1; } 采样后的LPPV（光照探针代理体）的效果，在伽马空间中的效果太暗 我们的着色器现在在需要的时候对LPPV（光照探针代理体）进行采样，但结果太暗了。至少在伽马颜色空间中工作就是这样的结果。这是因为球面谐波数据存储在线性空间中。因此，可能需要进行颜色的转换。 if (unity_ProbeVolumeParams.x == 1) { indirectLight.diffuse = SHEvalLinearL0L1_SampleProbeVolume( float4(i.normal, 1), i.worldPos ); indirectLight.diffuse = max(0, indirectLight.diffuse); #if defined(UNITY_COLORSPACE_GAMMA) indirectLight.diffuse = LinearToGammaSpace(indirectLight.diffuse); #endif } 采样后的LPPV（光照探针代理体）的效果，带有正确的颜色 LOD Groups 当一个对象最终只覆盖应用程序窗口的一小部分的时候，你不需要高度详细的网格来渲染它.你可以根据对象在视图中的大小使用不同的网格。这被称为细节层次，或简称LOD。Unity允许我们通过组件LOD组来实现这样的功能。 创建一个LOD层次结构 这个想法是你在各种不同的LOD等级使用同一网格的多个版本。最高级 - LOD 0 - 具有最多的顶点、子对象、动画、复杂的材质等。随后的级别逐渐变得更简单，更容易计算。在理想情况下，相邻的LOD等级被设计为使得当Unity从一个LOD等级切换到另一个LOD等级的时候，你不能轻易地辨别出它们之间的区别。否则突然有LOD等级变化的时候就会让人很晕。但是在研究这种技术的时候，我们会使用明显的不同的网格。 创建一个空的游戏对象并给它两个子对象。第一个子对象是标准球体，第二个子对象是标准立方体，其大小设置为0.75。 预期的结果看起来像是一个重叠的球体和立方体。 球体和立方体作为一个对象 通过Component /Rendering / LOD Group将一个LOD组组件添加到父对象。你会得到一个具有默认设置的LOD组，它有三个LOD等级。 百分比是指由对象的包围盒覆盖的窗口的垂直部分。因此，当垂直尺寸下降到窗口高度的60％的时候，默认设置为切换到LOD 1，当垂直尺寸 下降到窗口高度的30％的时候，默认设置为切换到LOD 2。当垂直尺寸下降到窗口高度的10％的时候，它根本不渲染。 你可以通过拖动LOD框的边来更改这些阈值。 组件LOD组 这些阈值由LOD偏移（LOD Bias）进行修改，LOD偏移（LOD Bias）可以在组件检视器里面查看并修改。目前使用的是质量设置为2的默认值，这意味着阈值被减半。也可以设置为最大LOD等级，这将导致跳过最高级别。 为了使其工作，你必须告诉组件每个LOD等级都会使用哪些对象。这是通过选择一个LOD块并将对象添加到其“渲染器”列表中完成的。你可以在场景中添加任何对象，但一定要确保添加其子对象到LOD块的“渲染器”列表。让LOD 0的“渲染器”使用球体，让LOD 1的“渲染器”使用 立方体。我们将LOD 2的“渲染器”留空，所以我们只有两个LOD等级。如果需要的话，你可以通过右键单击上下文菜单删除并插入LOD等级。 让球这个子物体使用LOD 0等级 一旦配置了LOD等级，你可以通过移动相机来查看它们的效果。如果物体足够大的话，它将使用球体，否则的话它将使用立方体，或根本不会渲染。 Your browser does not support the video tag. Here is a link to the video file instead. LOD切换 烘焙全局光照和LOD组 因为LOD组是如何渲染的取决于它的视图大小，所以它们自然是动态的。但是，你仍然可以使其成为静态。对整个对象层次结构执行此操作，因此也包括了根节点和它的两个子节点。然后设置主光源为烘焙光源，看看会发生什么。 让球这个子物体使用LOD 0等级 使用烘焙光源得到的效果 看起来在烘焙静态光照贴图的时候使用的是LOD 0。 我们最终总是能够看到球体的阴影和间接光照的贡献，即使LOD组切换到一个立方体或是对自身做了剔除。但请注意，立方体也是使用了静态光照贴图。 所以它不使用光照探针，对吧？ 转动光照探针组就能发现这一点。 没有光照探针时候的烘焙光照 禁用光探针组会使得立方体变得更暗。这意味着他们不再接受间接光照。 这是因为在烘焙过程中确定间接光照的时候使用的是LOD 0。为了找到其他LOD等级下的间接光照， Unity可以做到的最好程度是依靠烘焙光照探针。 因此，即使在运行时我们不需要光照探针，我们也需要光照探针来为我们的立方体计算间接光照。 实时全局光照和LOD组 当只使用实时全局光照的时候，方法是类似的，除了我们的立方体现在在运行时使用的是光照探针。你可以通过选择球体或立方体来验证这一点。选择立方体后，你可以看到小工具显示了哪些光照探针被使用。 球体不显示它们，因为它使用的是动态光照贴图. LOD 1使用光照探针来计算实时全局光照 当烘焙全局光照和实时全局光照同时使用的时候，它会变得更加复杂。 在这种情况下，立方体应该对烘焙全局光照使用光照贴图，对实时全局光照使用光照探针。不幸的是，这是不可能的，这是因为光照贴图和和球面谐波不能同时使用。这是一个非此即彼的问题。因为光 照贴图数据对于立方体来说是可用的，所以Unity最终会使用它。因此，立方体不受实时全局光照的影响。 仅对LOD 1等级使用烘焙光照，使用的是低强度的主光源 一个重要的细节是，烘焙的LOD等级和渲染的LOD等级是完全独立的。 他们不需要使用相同的设置。如果实时全局光照最终比烘焙全局光照更重要，你可以强制立方体使用光照探针，确保它对于光照贴图来说不是静态的，同时保持球体静止。 LOD 1强制使用光照探针 LOD淡入淡出功能 LOD组这种方法的缺点是，当LOD等级发生变化的时候，它可以在视觉上很明显的表现出来。几何体会在视图中突然弹出、消失或改变形状。 这可以通过相邻LOD等级之间的淡入淡出来缓解，这通过将LOD组的渐变模式设置为淡入淡出来完成。还有另一种渐变模式，由Unity用于SpeedTree对象，我们不会使用这种模式。 当启用淡入淡出的时候，每个LOD等级都会显示一个淡入变换宽度（Fade Transition Width ）字段，用于控制其块的哪个部分用于衰落。举个简单的例子来说，当设置为0.5的时候，一半LOD范围将用于淡出到下一级.或者，淡入淡出过程可以是有动画的，在这种情况下， 在LOD等级之间的切换需要大约半秒钟。 带有0.5变换宽度的淡入淡出 当启用淡入淡出的时候，在LOD组之间进行转换的时候会同时渲染两个LOD等级。 支持淡入淡出 Unity的标准着色器在默认情况下是不支持淡入淡出的。如果想要支持支持淡入淡出的话，你必须复制标准着色器并为LOD_FADE_CROSSFADE关键字添加一个多编译指令。添加这条指令还有一个原因是为了在My First Lighting着色器里面支持淡入淡出功能。让我们将这条指令添加到除了meta渲染通道以外的所有渲染通道。 #pragma multi_compile _ LOD_FADE_CROSSFADE 我们将使用抖动来在LOD等级之间进行转换。这种方法适用于前向渲染和延迟渲染，也适用于有阴影的情况。 在创建半透明阴影的时候，我们已经使用了抖动这种方法。它需要片段的屏幕空间坐标，这迫使我们为顶点程序和片段程序使用不同的插值器结构。所以让我们复制My Lighting 中的Interpolators结构，将其重命名为InterpolatorsVertex。 struct InterpolatorsVertex { }; struct Interpolators { }; InterpolatorsVertex MyVertexProgram (VertexData v) { InterpolatorsVertex i; } 当我们必须进行淡入淡出处理的时候，片段程序的插值器里面必须包含vpos，否则我们可以使用同样的位置信息。 struct Interpolators { #if defined(LOD_FADE_CROSSFADE) UNITY_VPOS_TYPE vpos : VPOS; #else float4 pos : SV_POSITION; #endif }; 我们可以在我们片段程序中开始的位置使用UnityApplyDitherCrossFade函数来执行淡入淡出操作。 FragmentOutput MyFragmentProgram (Interpolators i) { #if defined(LOD_FADE_CROSSFADE) UnityApplyDitherCrossFade(i.vpos); #endif } UnityApplyDitherCrossFade是如何工作的？ 这个函数在UnityCG中进行定义。它的方法类似于我们在《渲染12:半透明阴影》中使用的抖动方法，区别只是整个对象的抖动级别是均匀的。 因此，不需要混合抖动级别。 它使用存储在4×64大小的二维纹理中的16个抖动级别，而不是4×4×16大小的三维纹理。 FragmentOutput MyFragmentProgram (Interpolators i) { #if defined(LOD_FADE_CROSSFADE) UnityApplyDitherCrossFade(i.vpos); #endif } unity_LODFade变量在UnityShaderVariables中进行定义。它的Y分量包含的是对象的渐变量，共有十六步。 通过抖动方法得到的淡入淡出几何体 淡入淡出现在可以在几何体上正常工作了。为了使其适用于阴影，我们必须调整My Shadows着色器. 首先，当我们进行淡入淡出处理的时候，必须使用vpos。其次，我们还必须在片段程序开始的位置使用UnityApplyDitherCrossFade函数。 struct Interpolators { #if SHADOWS_SEMITRANSPARENT || defined(LOD_FADE_CROSSFADE) UNITY_VPOS_TYPE vpos : VPOS; #else float4 positions : SV_POSITION; #endif }; float4 MyShadowFragmentProgram (Interpolators i) : SV_TARGET { #if defined(LOD_FADE_CROSSFADE) UnityApplyDitherCrossFade(i.vpos); #endif } 对几何体和阴影都做了淡入淡出处理 因为立方体和球体相互交叉，所以我们在对它们做淡入淡出处理的时候，得到一些奇怪的自阴影效果。这对于看到淡入淡出处理能在阴影上起作用是很方便的，但是当你为实际游戏创建LOD几何体的时候，需要注意这些瑕疵。" }, { "title": "Unity 混合光照(翻译十七)", "url": "/posts/Unity_Mix_Lighting/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-21 20:00:00 +0800", "content": "本篇摘要： 只烘焙间接光 混合烘焙阴影和实时阴影 处理代码的变化和问题 支持消减光照（subtractivelighting） 烘焙间接光 光照贴图可以提供预计算光照：以纹理内存为代价减少了GPU在实时中的工作量；还加入了间接光。 烘焙光的限制 1.高光不能被烘焙 2.烘焙光只通过光照探头影响动态物体 3.烘焙光不产生实时阴影 你可以在下面的截图中看到完全实时光照和完全烘焙光照之间的区别。前一篇教程中的一个场景，唯一的不同是我将所有的球体都设置为动态并重新改变了一些球体的位置。其它一切都是静态的。这是使 用前向渲染的方法。 完全实时和完全烘焙光照 混合模式 烘焙光照有间接光而没有实时光照，因为间接光需要光照贴图。由于间接光可以为场景加入很大的真实感，如果我们可以将它和实时光照融合在一起就再好不过了。这是可以的，但也意味着着色的开销会增加。我们需要将混合光（Mixed Lighting）的光照模式（Lighting Mode）设置为烘焙间接（Baked Indirect）。 混合光照，烘焙间接 我们已经在前一篇教程中切换到这个模式了，但是之前 we 只使用了完全烘焙光照。虽然表现结果与完全烘焙光照相同，混合光照模式没有任何区别。为了使用混合光照，光源的模式必须要设置为混合。 混合模式的主光源 在将主定向光改为混合光后，两件事会发生： 第一，Unity会再次烘焙光照贴图。这一次光照贴图只会存储间接光，所以它会比之前的暗很多。 完全烘焙的光照贴图 vs 只有间接光的光照贴图 第二，所有物体都会像主光源被设置为实时那样被照亮。只有一点不同：光照贴图被用来为静态物体添加间接光，而不是球谐光或探头。动态物体的间接光仍要使用光照探头。 混合光照，实时直接光照烘焙间接光 我们不需要改变我们的着色器来支持这点，因为前向基础通道（forward base pass）已经融合了光照贴图数据和主定向光源。和往常一样，额外的光照会得到附加通道（additive pass）。当使用延迟渲染通道时，主光源也会得到一个通道。 混合光可以在运行时调整吗？ 是的，因为它们被用于实时光照。但是，它们的烘焙数据时静态的。所以在运行时你只能稍微调整光照，比如稍微调整它的强度。更大的变化会使人明显看出烘焙光照和实时光照之间的不同步。_ 更新着色器 刚开始一切似乎正常运行。但是，定向光的阴影衰减发生了错误。我们通过极大降低阴影距离观察到阴影被剪掉了。 阴影衰减，标准着色器vs我们的着色器 虽然Unity很长一段时间都有混合光照模式，但实际上它在Unity5中就不起作用了。Unity5.6中新加入了一个混合光照模式，即我们现在使用的这个。当该新模式被加入时，_UNITY_LIGHT_ATTENUATION_宏下面的代码发生了变化。我们在使用完全烘焙光照或者实时光照时没有注意到这一点，但是我们必须更新我们的代码以适应混合光照的新方法。由于这是最近的一个巨大的变化，我们必须要注意它所带来的问题。 我们要改变的第一点是不再使用_SHADOW_COORDS_宏来定义阴影坐标的插值（interpolater）。我们必须使用新的_UNITY_SHADOW_COORDS_宏来代替它。 struct Interpolators { //SHADOW_COORDS(5) UNITY_SHADOW_COORDS(5) }; 同样，TRANSFER_SHADOW_应该替换为_UNITY_TRANSFER_SHADOW Interpolators MyVertexProgram (VertexData v) { //TRANSFER_SHADOW(i); UNITY_TRANSFER_SHADOW(i); } 然而，这会产生一个编译错误，因为该宏需要一个额外的参数。从Unity 5.6开始，只有定向阴影的屏幕空间坐标中被放入一个插值。点光源和聚光源的阴影坐标现在在片段程序（fragment program）中进行计算。有个新变化：在一些情况中光照贴图的坐标被用在阴影蒙版 （shadow mask）中，我们会在后面讲解这一点。为了该宏能正常工作，我们必须为它提供第二个UV通道中的数据，其中包含光照贴图的坐标。 UNITY_TRANSFER_SHADOW(i, v.uv1); 这样会再次产生一个编译错误。这是因为在一些情况下_UNITY_SHADOW_COORDS_错误地创建了一个插值，尽管实际上并不需要。在这种情况下，_TRANSFER_SHADOW_不会初始化它，因而导致错误。这个问题出现在5.6.0中，一直到5.6.2和2017.1.0beta版本中都有。 人们通常不会注意到这个问题，因为Unity的标准着色器使用_UNITY_INITIALIZE_OUTPUT_宏来完全地初始化它的插值结构体。因为我们不使用这个宏，所以出现了问题。为了解决它，我们使用_UNITY_INITIALIZE_OUTPUT_宏来初始化我们的插值。 Interpolators MyVertexProgram (VertexData v) { Interpolators i; UNITY_INITIALIZE_OUTPUT(Interpolators, i); } _UNITY_INITIALIZE_OUTPUT_有什么作用？ 它只是为变量分配数值0，将其转换为正确的类型。至少是当程序支持该宏时会这样，否则它不会做任何事。 // Initialize arbitrary structure with zero values. // Not supported on some backends // (e.g. Cg-based particularly with nested structs). // hlsl2glsl would almost support it, except with structs that have arrays // -- so treat as not supported there either :( #if defined(UNITY_COMPILER_HLSL) || defined(SHADER_API_PSSL) || defined(UNITY_COMPILER_HLSLCC) #define UNITY_INITIALIZE_OUTPUT(type,name) name = (type)0; #else #define UNITY_INITIALIZE_OUTPUT(type,name) #endif 通常我们倾向于只使用显式赋值，很少使用这个初始化插值宏。 手动衰减阴影 现在我们正确地使用了新的宏定义，但是主光源的阴影仍然没有按照它们应该的那样衰减。结果我们发现当同时使用定向阴影和光照贴图时，UNITY_LIGHT_ATTENUATION 不会对光源进行衰减。使用混合模式的主定向光源就会产生这个问题。所以我们必须手动设置。 为什么在这个例子中阴影没有衰减？ 1、UNITY_LIGHT_ATTENUATION宏之前是独立使用的，但是自从Unity5. 6它开始和Unity的标准全局光照函数一同使用。我们没有采用同样的方法，因此它不能正常工作。 2、至于为什么要做这个改动，唯一的线索就是AutoLight中的一段注释：“为了性能的原因以GI函数的深度处理阴影”。由于着色器编译器会随意地移动代码。 对于我们的延迟光照着色器，我们已经有了进行阴影衰减的代码。将相关代码片段从MyDeferredShading中复制到My Lighting中的一个新函数中。唯一实际的区别在于我们必须使用视图向量和视图矩阵构建viewZ。我们只需要Z分量，所以无需进行一次完整的矩阵乘法。 float FadeShadows (Interpolators i, float attenuation) { float viewZ = dot(_WorldSpaceCameraPos - i.worldPos, UNITY_MATRIX_V[2].xyz); float shadowFadeDistance = UnityComputeShadowFadeDistance(i.worldPos, viewZ); float shadowFade = UnityComputeShadowFade(shadowFadeDistance); attenuation = saturate(attenuation + shadowFade); return attenuation; } 该手动衰减必须在使用了_UNITY_LIGHT_ATTENUATION初始化完成_之后。 UnityLight CreateLight (Interpolators i) { UNITY_LIGHT_ATTENUATION(attenuation, i, i.worldPos.xyz); attenuation = FadeShadows(i, attenuation); } 只有当 HANDLE_SHADOW_BLENDING_IN_GI 在UnityShadowLibrary.cginc文件中有定义时，FadeShadows才会开始计算。 float FadeShadows (Interpolators i, float attenuation) { #if HANDLE_SHADOWS_BLENDING_IN_GI // UNITY_LIGHT_ATTENUATION doesn't fade shadows for us. float viewZ = dot(_WorldSpaceCameraPos - i.worldPos, UNITY_MATRIX_V[2].xyz); float shadowFadeDistance = UnityComputeShadowFadeDistance(i.worldPos, viewZ); float shadowFade = UnityComputeShadowFade(shadowFadeDistance); attenuation = saturate(attenuation + shadowFade); #endif return attenuation; } 最后，我们的阴影如它们应该的那样正常衰减了。 使用阴影蒙版 烘焙间接光的混合模式成本很高。它们需要实时光照外加间接光的光照贴图那么大的工作量。它和完全烘焙光照相比最重要的是加入了实时阴影。幸运的是，有一个方法仍可以将阴影烘焙到光照贴图中，将其和实时阴影综合起来。为了开启这个功能，我们将混合光照模式改为Shadowmask。 Shadowmask模式 在这个模式中，混合光照的间接光和阴影衰减都存储在了光照贴图中。阴影被存储在一张额外的贴图（即阴影蒙版）。当只有主定向光源时，红色的阴影蒙版决定是否过滤被照亮的物体。红色是因为阴影信息被存储在纹理的R通道中。事实上，贴图中至多可以储存四个光照 的阴影，因为它只有四个通道。 烘焙的强度以及阴影蒙版 在Unity创建了阴影蒙版后，静态物体的阴影投射会消失。只有光照探头仍会处理它们。动态物体的阴影不受影响。 没有烘焙阴影 对阴影蒙版采样 为了重新得到烘焙阴影，我们必须对阴影蒙版采样样。Unity的宏已经对点光源和聚光源进行了取样，不过我们必须也要将它包含在我们的FadeShadows函数中。为此我们可以使用UnityShadowLibrary中的UnitySampleBakedOcclusions函数。它需要光照贴图的UV坐标和世界位置作为输入参数。 float FadeShadows (Interpolators i, float attenuation) { #if HANDLE_SHADOWS_BLENDING_IN_GI float bakedAttenuation = UnitySampleBakedOcclusion(i.lightmapUV, i.worldPos); attenuation = saturate(attenuation + shadowFade); #endif return attenuation; } UnitySampleBakedOcclusion是什么样子的？ 它使用光照贴图坐标对阴影蒙版取样，然后选择适当的通道。unity_OcclusionMaskSelector变量是一个含有一个分量的向量，该分量被设置为1以匹配当前正在被着色的光源。 fixed UnitySampleBakedOcclusion (float2 lightmapUV, float3 worldPos) { #if defined (SHADOWS_SHADOWMASK) #if defined(LIGHTMAP_ON) fixed4 rawOcclusionMask = UNITY_SAMPLE_TEX2D_SAMPLER( unity_ShadowMask, unity_Lightmap, lightmapUV.xy ); #else fixed4 rawOcclusionMask = UNITY_SAMPLE_TEX2D(unity_ShadowMask, lightmapUV.xy); #endif return saturate(dot(rawOcclusionMask, unity_OcclusionMaskSelector)); #else return 1.0; #endif } 该函数还处理了光照探头代理体积的衰减，但是我们还没有支持这点所以我去掉了那部分的代码。这就是为什么该函数有一个世界位置的参数。 当使用阴影蒙版时，_UnitySampleBakedOcclusions_提供给我们烘焙阴影衰减，在其他情况下它的值都为1。现在我们必须将它和我们已经有的衰减综合起来然后对阴影进行衰减。UnityMixRealtimeAndBakedShadows函数为我们实现了这些。 float bakedAttenuation = UnitySampleBakedOcclusion(i.lightmapUV, i.worldPos); //attenuation = saturate(attenuation shadowFade); attenuation = UnityMixRealtimeAndBakedShadows ( attenuation, bakedAttenuation, shadowFade ); UnityMixRealtimeAndBakedShadows是如何工作的？ 它也是UnityShadowLibrary中的一个函数。它还处理光照探头代理体积以及一些其他极端情况。那些情况和我们无关，所以我删除了一些内容。 inline half UnityMixRealtimeAndBakedShadows ( half realtimeShadowAttenuation, half bakedShadowAttenuation, half fade ) { #if !defined(SHADOWS_DEPTH) &amp;&amp; !defined(SHADOWS_SCREEN) &amp;&amp; !defined(SHADOWS_CUBE) return bakedShadowAttenuation; #endif #if defined (SHADOWS_SHADOWMASK) #if defined (LIGHTMAP_SHADOW_MIXING) realtimeShadowAttenuation = saturate(realtimeShadowAttenuation + fade); return min(realtimeShadowAttenuation, bakedShadowAttenuation); #else return lerp( realtimeShadowAttenuation, bakedShadowAttenuation, fade ); #endif #else //no shadowmask return saturate(realtimeShadowAttenuation + fade); #endif } 如果没有动态阴影，那么结果将得到烘焙的衰减。这意味着动态物体没有阴影，以及被映射到光照贴图上的物体没有烘焙阴影。 当没有使用阴影蒙版时，它会进行原来的衰减。否则，它会根据我们是否做了阴影混合进行表现，我们后面再讲。现在，它只是在实时衰减和烘焙衰减之间进行一个插值。 实时阴影和阴影蒙版阴影 现在静态物体有了实时阴影和烘焙阴影，且它们正确地混合。实时阴影的衰减仍然超过了阴影距离，但是烘焙阴影没有。 只有实时阴影衰减了 添加一个阴影蒙版G-Buffer 现在阴影蒙版可用于前向渲染路径，但是我们需要使它也可用于延迟渲染：添加阴影蒙版信息作为一个额外的G-缓存。所以当_SHADOWS_SHADOWMASK_被定义时，在_FragmentOutput_结构体中添加一个缓存。 struct FragmentOutput { #if defined(DEFERRED_PASS) float4 gBuffer0 : SV_Target0; float4 gBuffer1 : SV_Target1; float4 gBuffer2 : SV_Target2; float4 gBuffer3 : SV_Target3; #if defined(SHADOWS_SHADOWMASK) float4 gBuffer4 : SV_Target4; #endif #else float4 color : SV_Target; #endif }; 添加的第五个G-缓存，会使显存增大，并不是所有的平台(mobile)都支持它。Unity只在有足够多的渲染目标可用时才支持阴影蒙版，因此我们也应该这样做。 #if defined(SHADOWS_SHADOWMASK) &amp;&amp; (UNITY_ALLOWED_MRT_COUNT &gt; 4) float4 gBuffer4 : SV_Target4; #endif 我们只需在G-缓存中存储采样得到的阴影蒙版数据，而且没有一个确切的光照，为此我们可以使用_UnityGetRawBakedOcclusions_函数，它与_UnitySampleBakedOcclusion_相似，唯一不同在于它没有选择某个纹理通道。 FragmentOutput output; #if defined(DEFERRED_PASS) #if !defined(UNITY_HDR_ON) color.rgb = exp2(-color.rgb); #endif output.gBuffer0.rgb = albedo; output.gBuffer0.a = GetOcclusion(i); output.gBuffer1.rgb = specularTint; output.gBuffer1.a = GetSmoothness(i); output.gBuffer2 = float4(i.normal * 0.5 + 0.5, 1); output.gBuffer3 = color; #if defined(SHADOWS_SHADOWMASK) &amp;&amp; (UNITY_ALLOWED_MRT_COUNT &gt; 4) output.gBuffer4 = UnityGetRawBakedOcclusions(i.lightmapUV, i.worldPos.xyz); #endif #else output.color = ApplyFog(color, i); #endif 为了可以在没有光照贴图的时候也能成功编译，当光照贴图坐标不可用时我们使用0代替它。 #if defined(SHADOWS_SHADOWMASK) &amp;&amp; (UNITY_ALLOWED_MRT_COUNT &gt; 4) float2 shadowUV = 0; #if defined(LIGHTMAP_ON) shadowUV = i.lightmapUV; #endif output.gBuffer4 = UnityGetRawBakedOcclusions(shadowUV, i.worldPos.xyz); #endif 使用阴影蒙版G-缓存 调整MyDeferredShading延迟渲染着色器。 第一步先添加额外的一个G-buffer变量。 sampler2D _CameraGBufferTexture0; sampler2D _CameraGBufferTexture1; sampler2D _CameraGBufferTexture2; sampler2D _CameraGBufferTexture4; 第二步，创建一个函数来得到适当的阴影衰减。如果有了阴影蒙版，可通过对纹理采样然后和_unity_OcclusionMaskSelector_进行一次颜色饱和点乘。这个变量是在_UnityShaderVariables.cginc_中定义的，包含了一个用于选择当前正在被渲染的光照通道的向量。 float GetShadowMaskAttenuation (float2 uv) { float attenuation = 1; #if defined (SHADOWS_SHADOWMASK) float4 mask = tex2D(_CameraGBufferTexture4, uv); attenuation = saturate(dot(mask, unity_OcclusionMaskSelector)); #endif return attenuation; } 在CreateLight中，即使当前光照没有实时阴影，我们在有阴影蒙版时也要衰减阴影。 UnityLight CreateLight (float2 uv, float3 worldPos, float viewZ) { #if defined(SHADOWS_SHADOWMASK) shadowed = true; #endif if (shadowed) { } } 为了正确地包含烘焙阴影，再次使用UnityMixRealtimeAndBakedShadows代替之前的衰减计算。 if (shadowed) { float shadowFadeDistance = UnityComputeShadowFadeDistance(worldPos, viewZ); float shadowFade = UnityComputeShadowFade(shadowFadeDistance); // shadowAttenuation = saturate(shadowAttenuation + shadowFade); shadowAttenuation = UnityMixRealtimeAndBakedShadows( shadowAttenuation, GetShadowMaskAttenuation(uv), shadowFade ); } 现在也可以使用自定义的延迟光照着色器得到正确的烘焙阴影了。例外，即当我们的优化分支被使用时会跳过阴影混合。该捷径在阴影蒙版被使用时不可用。 if (shadowed) { #if defined(UNITY_FAST_COHERENT_DYNAMIC_BRANCHING) &amp;&amp; defined(SHADOWS_SOFT) #if !defined(SHADOWS_SHADOWMASK) UNITY_BRANCH if (shadowFade &gt; 0.99) { shadowAttenuation = 1; } #endif #endif } 阴影蒙版-距离模式 DIstance Shadowmask 虽然使用阴影蒙版模式我们可以得到不错的静态物体的烘焙阴影，动态物体却不能从中获利。动态物体只能接收到实时阴影以及光照探头数据。如果我们希望得到动态物体的阴影，那么静态物体必须也要投射实时阴影。这里的混合光照模式我们要用到距离阴影蒙版（Distance Shadowmask）了。 距离阴影蒙版模式 在2017及以上，使用哪个阴影蒙版模式是通过质量设置进行控制。 当使用DistanceShadowmask模式时，所有物体都使用实时阴影。第一眼看去，好像和Baked Indirect模式完全一样。 所有物体都有实时阴影 不过这里仍有一个阴影蒙版。在这个模式中，烘焙阴影和光照探头的使用超出了阴影距离。因此该模式是成本最高的模式，在阴影距离范围内等价于烘焙间接模式，超出该范围则等价于阴影蒙版模式。 前面已经支持这个模式了，因为我们正在使用UnityMixRealtimeAndBakedShadows。为了正确地混合完全实时阴影和烘焙阴影，它像往常那样衰减实时阴影，然后取其和烘焙阴影的最小值。 多重光照 因为阴影蒙版有四个通道，它可以最多同时支持4个光照体积重叠在一起 四个光源，都是混合光 主方向光源的阴影仍存储在R通道中。你还能够看到存储在G通道和B通道中的聚光源的阴影，最后一个聚光源的阴影存储在A通道中。 当光照体积不重叠时，它们使用相同的通道来存储它们的阴影数据。所以你可以有任意多个混合光照。但是你必须确保至多四个光照体积彼此重叠。如果有太多个混合光影响同一篇区域，那么一些就会改回到完全烘焙模式。为了说明这一点，下面这张截图显示的是在多加入一个聚光源以后的光照贴图。你可以在强度贴图中清楚地看到其中一个已经变成了烘焙光。 5个重叠的光照，其中一个为完全烘焙光 支持多个有蒙版的定向光 不幸的是，阴影蒙版只有当包含至多一个混合模式的方向光源存在时才能正常工作。对于额外的方向光，阴影衰减会发生错误，至少是在使用前向渲染通道时。延迟渲染倒没有问题。 两个方向光源产生错误的衰减 这是使用UNITY_LIGHT_ATTENUATION的新方法中的一个漏洞：Unity使用通过UNITY_SHADOW_COORDS定义的阴影插值来存储方向阴影的屏幕空间坐标，或者其它拥有阴影蒙版的光源的光照贴图坐标。 使用阴影蒙版的方向光还需要光照贴图坐标。在forward-render中，这些坐标会被包含，因为LIGHTMAP_ON会在需要的时候被定义。然而，LIGHTMAP_ON在additional-pass中永远不会被定义。这意味着多余的方向光没有可用的光照贴图坐标。结果UNITY_LIGHT_ATTENUATION在这种情况下只会使用0，导致错误的光照贴图采样 所以我们不能依靠UNITY_LIGHT_ATTENUATION额外获得使用阴影蒙版的方向光源。用屏幕空间的方向阴影 #if defined(FOG_LINEAR) || defined(FOG_EXP) || defined(FOG_EXP2) #endif #if !defined(LIGHTMAP_ON) &amp;&amp; defined(SHADOWS_SCREEN) #if defined(SHADOWS_SHADOWMASK) &amp;&amp; !defined(UNITY_NO_SCREENSPACE_SHADOWS) #define ADDITIONAL_MASKED_DIRECTIONAL_SHADOWS 1 #endif #endif 接下来，对那些额外有蒙版的定向阴影，我们也要包含光照贴图坐标。 struct Interpolators { #if defined(LIGHTMAP_ON) || ADDITIONAL_MASKED_DIRECTIONAL_SHADOWS float2 lightmapUV : TEXCOORD6; #endif }; Interpolators MyVertexProgram (VertexData v) { #if defined(LIGHTMAP_ON) || ADDITIONAL_MASKED_DIRECTIONAL_SHADOWS i.lightmapUV = v.uv1 * unity_LightmapST.xy + unity_LightmapST.zw; #endif } 当光照贴图坐标可用时，我们可以再次使用FadeShadows函数进行我们自己控制的衰减。 float FadeShadows (Interpolators i, float attenuation) { #if HANDLE_SHADOWS_BLENDING_IN_GI || ADDITIONAL_MASKED_DIRECTIONAL_SHADOWS #endif return attenuation; } 但是，这仍然不正确，因为我们为其输入了错误的衰减数据。我们必须绕开UNITY_LIGHT_ATTENUATION，只得到烘焙后的衰减，在这个情况中我们可以使用SHADOW_ATTENUATION宏。 float FadeShadows (Interpolators i, float attenuation) { #if HANDLE_SHADOWS_BLENDING_IN_GI || ADDITIONAL_MASKED_DIRECTIONAL_SHADOWS //UNITY_LIGHT_ATTENUATION doesn't fade shadows for us. #if ADDITIONAL_MASKED_DIRECTIONAL_SHADOWS attenuation = SHADOW_ATTENUATION(i); #endif #endif return attenuation; } 两个定向光源正确的衰减 消减阴影-Subtractive Shadows 混合光照很好，但是它不像完全烘焙光照那样成本低廉。如果以低性能硬件为目标，那么混合光照不太可行。烘焙光照会管用，但是事实上你也许需要动态物体对静态物体投射阴影。那样的话，你可以使用消减混合光照模式。 消减模式 在切换到消减模式后，场景会亮很多。这是由于静态物体现在同时使用完全烘焙的光照贴图和方向光源。这是因为动态物体仍然会同时使用光照探头和方向光源。 静态物体受到两次光照 消减模式只可用于前向渲染。当使用延迟渲染路径时，相关的物体会回到前向渲染路径，就像透明物体那样。 消减光照 在消减模式中，静态物体通过光照贴图被照亮，同时还将动态阴影考虑在内。这是通过降低光照贴图在阴影区域的强度来实现的。为此，着色器需要使用光照贴图和实时阴影。它还需要使用实时光照来计算出要将光照贴图调暗多少。这就是为什么我们在切换到这个模式后得到了双重光照。 消减光照是一个近似，只在一个单一定向光下起作用，因此它只支持主方向光的阴影。另外，我们必须以某种方式了解在动态着色区域内间接光的环境是什么。由于我们使用的是一个完全烘焙的光照贴图，我们没有这个信息。Unity没有包含一个额外的只有间接光的光 照贴图，而是使用了一个统一的颜色对环境光取近似值。即实时阴影颜色（Realtime Shadow Color），你可以在混合光照选项中调整它。 在着色器中，我们知道_当LIGHTMAP_ON_，SHADOWS_SCREEN，和_LIGHTMAP_SHADOW_MIXING_关键词被定义而_SHADOWS_SHADOWMASK_没有被定义时我们应该使用消减光照。如果这样的话我们定义_SUBTRACTIVE_LIGHTING_，以便更容易使用它。 #if !defined(LIGHTMAP_ON) &amp;&amp; defined(SHADOWS_SCREEN) #if defined(SHADOWS_SHADOWMASK) &amp;&amp; !defined(UNITY_NO_SCREENSPACE_SHADOWS) #define ADDITIONAL_MASKED_DIRECTIONAL_SHADOWS 1 #endif #endif #if defined(LIGHTMAP_ON) &amp;&amp; defined(SHADOWS_SCREEN) #if defined(LIGHTMAP_SHADOW_MIXING) &amp;&amp; !defined(SHADOWS_SHADOWMASK) #define SUBTRACTIVE_LIGHTING 1 #endif #endif 在做其他事情之前，我们必须去除掉双重阴影。为此我们可以关闭动态光照，就像我们对延迟通道所做的那样。 UnityLight CreateLight (Interpolators i) { UnityLight light; #if defined(DEFERRED_PASS) || SUBTRACTIVE_LIGHTING light.dir = float3(0, 1, 0); light.color = 0; #else #endif return light; } 静态物体只有烘焙光 为烘焙光打阴影 为了应用消减阴影，我们创建一个函数以在需要的时候调整间接光。通常它不会做任何事。 void ApplySubtractiveLighting ( Interpolators i, inout UnityIndirect indirectLight ) { } 我们在获取光照贴图数据后要调用该函数。 UnityIndirect CreateIndirectLight (Interpolators i, float3 viewDir) { #if defined(FORWARD_BASE_PASS) || defined(DEFERRED_PASS) #if defined(LIGHTMAP_ON) indirectLight.diffuse = DecodeLightmap(UNITY_SAMPLE_TEX2D(unity_Lightmap, i.lightmapUV)); #if defined(DIRLIGHTMAP_COMBINED) #endif ApplySubtractiveLighting(i, indirectLight); #else indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); #endif #endif return indirectLight; } 如果有消减光照，那么我们必须获取阴影衰减。我们可以简单地从CreateLight中将代码复制过来。 void ApplySubtractiveLighting ( Interpolators i, inout UnityIndirect indirectLight ) { #if SUBTRACTIVE_LIGHTING UNITY_LIGHT_ATTENUATION(attenuation, i, i.worldPos.xyz); attenuation = FadeShadows(i, attenuation); #endif } 下一步，我们要计算出如果使用实时光照的话我们可以接收到多少光。我们假设该信息和烘焙在光照贴图中的信息相吻合。由于光照贴图只包含漫射光，我们只需计算定向光的Lambert。 #if SUBTRACTIVE_LIGHTING UNITY_LIGHT_ATTENUATION(attenuation, i, i.worldPos.xyz); attenuation = FadeShadows(i, attenuation); float ndotl = saturate(dot(i.normal, _WorldSpaceLightPos0.xyz)); #endif 为了达到阴影光照的强度，我们必须将兰伯特项乘以衰减。但是我们已经有了完全不含阴影的烘焙光照。因此我们估算一下有多少光被阴影挡住了。 float ndotl = saturate(dot(i.normal, _WorldSpaceLightPos0.xyz)); float3 shadowedLightEstimate = ndotl * (1 - attenuation) * _LightColor0.rgb; 通过从烘焙光中减去该估值，我们最终得到了调整好的光照。 float3 shadowedLightEstimate = ndotl * (1 - attenuation) * _LightColor0.rgb; float3 subtractedLight = indirectLight.diffuse – shadowedLightEstimate; indirectLight.diffuse = subtractedLight; 减去后得到的光照 无论在什么环境光场景中，这总会产生纯黑色阴影。为了更好地符合场景的需要，我们可以使用我们的消减阴影颜色，可以通过unity_ShadowColor实现。阴影区域不应比这个颜色更暗，不过它们可以更亮些。所以我们取计算出的光照和阴影颜色的最大值。 float3 subtractedLight = indirectLight.diffuse - shadowedLightEstimate; subtractedLight = max(subtractedLight, unity_ShadowColor.rgb); indirectLight.diffuse = subtractedLight; 我们还要考虑到阴影强度被设置为小于1这个情况。为了应用阴影强度，在有阴影和无阴影光照之间基于_LightShadowData的X分量做插值。 subtractedLight = max(subtractedLight, unity_ShadowColor.rgb); subtractedLight = lerp(subtractedLight, indirectLight.diffuse, _LightShadowData.x); indirectLight.diffuse = subtractedLight; 有颜色的阴影 因为我们的场景的环境强度（ambient intensity）被设置为0，所以默认的阴影颜色和场景不太搭配。但是可以很轻松地发现消减阴影，因此我没有调整它。还有一点非常明显，即阴影颜色现在覆盖了所有的烘焙阴影，而实际不应该这样。它应该只影响那些接收动态阴影的区域，不应该使烘焙阴影变亮。为此，使用消减光照和烘焙光照的最小值。 //indirectLight.diffuse = subtractedLight; indirectLight.diffuse = min(subtractedLight, indirectLight.diffuse); 正确的消减阴影 现在只要我们使用适当的阴影颜色，我们就会得到正确的消减阴影。但是记住这只是一个近似，而且它不太适用于多重光照。例如，其它的烘焙光会产生错误的阴影。 多重光照错误的消减" }, { "title": "Unity 光照烘焙(翻译十六)", "url": "/posts/Unity_Static_Lightting/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-19 20:00:00 +0800", "content": "实时光照计算的开销非常昂贵。根据翻译13，延迟渲染允许程序员使用的光源可以多于Forward渲染，但阴影的开销仍然对性能有一个限制。如果我们的场景是动态的，那么没有办法来避免执行这些计算。但是如果光源和几何物体位置都是不变的，那么我们可以只计算一次 光照并重复使用。这使得我们可以在场景中放置许多光源，而不必在运行的时候再渲染它们。这种方法也可以使用那些不能用作实时光源的区域光源(area lighting)。 光照贴图-Lightingmapping 为了尝试光照贴图，我创建了一个简单的测试场景，它具有一个简单的结构，可以提供阴影，还有一些放置在其内部的球体。一切物体都使用默认的Unity材质。 针对光照贴图的一个测试场景 烘焙光源-Baked Lights 要开始使用光照贴图，将唯一的光源对象的模式改为“Baked（烘焙）”而不是“Realtime（实时）”。 使用烘焙模式的主方向光源 将主方向光源变成烘培光源后，就不被纳入动态光照计算。从动态对象的角度来看，光源是不存在的。 唯一仍然不变的是环境光照，它仍然是基于主方向光源的。 没有直接光照的效果 要实际启用光照贴图，请在lighting窗口的“混合光照（Mixed Lighting）”中打开“烘培全局光照(BakedGlobal Illumination)”。 然后将光照模式设置为“烘培间接光照（BakedIndirect）”。 尽管它的名字说的是烘培间接光照，但是它也包括了直接光照。 它通常用于向场景添加间接光照。另外，确保实时全局光照（Realtime Global Illumination）被禁用，因为我们还没有支持到这一点。 烘培间接光照模式 静态几何体 场景的对象都应该是固定的：它们位置永远不会移动。要将这一个信息传达给Unity，请将这些对象标记为静态。你可以通过启用检视器窗口右上角的“静态”切换键来做到这一点。 光源也必须被标记为静态吗？ 不需要。光源只需要设置为适当的模式。 有各种子系统关心物体是否是静态的。“静态（static）”还有一个下拉菜单，你可以使用它来微调哪些系统会将这个对象视为静态的。现在我们只关心光照贴图，但最简单的做法是使一切都完全是静态的。 静态标签设定 一个物体对于光照贴图来说是否是静态的，也可以通过其网格渲染器的检视器来进行查看和编辑。 对于光照贴图来说是静态的物体 现在，所有的物体都是静态的，它们将被包含在光照贴图的处理过程中。 使用烘焙光照的场景 必须注意，使用光照贴图得到的结果不如使用实时照明得到的结果亮度那么高。这是因为缺失了镜面高光，只剩下了漫反射光照。镜面高光取决于视角，因此取决于相机的角度。正是由于相机是移动的，因此它不能包含在光照贴图中。(使用场景推荐)这种限制意味着光照贴图可以用于微弱的光线和暗淡的表面，但不能用于强直射光或有光泽的表面。如果你想要镜面高光，你将不得不使用实时光源。所以你经常会使用烘烤光源和实时光源的混合。 为什么没有立即得到烘焙光源？ 为了确保在需要的时候光照贴图可以实际生成和更新，请在光照窗口的底部启用“自动生成（Auto Generate）”。 否则，你必须手动生成新的光照贴图。 自动烘焙 光照贴图设置-Lightingmapping Setting 光照烘焙窗口包含专门用于光照贴图设置的部分。在这里，你可以在质量、尺寸和烘烤时间之间取得平衡。你还可以在光照贴图烘焙算法引擎：Enlighten和Progressive lightmapper之间进行切换。后者会增量地生成光照贴图，优先考虑场景视图中可见的内容，这在编辑的时候很方便。本教程中使用的是Enlighten光照贴图引擎。 默认的光照贴图设置 在做任何事情之前，请将“DirectionalMode“设置为”Non-Direction“。 稍后我们会处理其他模式。 使用“Non-directional”模式的光照贴图 烘烤的光照存储在纹理中。 你可以通过将光照窗口从“场景（Scene）“切换到”全局地图（Global Maps）“模式来进行查看。 使用默认设置，我的测试场景很容易与一张1024×1024贴图相匹配。 得到光照贴图 Unity自带的Objects物体都有用于光照贴图的UV坐标。对于手动导入的模型，可以自己提供UV坐标，也可以让Unity生成。烘烤后可以在光照贴图中看到展开的纹理。它们需要多少空间取决于场景中物体的大小和光照贴图的分辨率设置。 如果质量要求高分辨率太大，一张贴图涨不下，Unity会创建额外的贴图存储，直至完成。 光照贴图的分辨率的不同会带来很大的差异 对于每个项目来说，最佳设置都是不同。 你必须不断的调整烘焙参数，直到达成很好的效果及平衡。需要注意的是，视觉质量也很大程度上取决于用于光照贴图的纹理展开的质量。不存在纹理接缝可能会产生明显的瑕疵。Unity的默认球体就是一个很好的例子。它不适用于光照贴图。 间接光源 烘焙光照会失去镜面高光，只能获得的是间接光照，它是在到达人眼之前会在多个表面反射的光。烘焙光会在拐角周围区域反射，那些本来会被遮挡的区域仍然会被照亮。我们不能实时计算镜面高光这个信息(本节1.2有说明)，但是我们可以在烘焙的时候包括反射光。 要清楚地看到实时光照和烘培光照之间的差异：将环境光照的强度设置为零，去掉天空盒的影响，所有的光都只是来自方向光。比对 没有环境光照，realtime vs. lightmapped 每次光子反射的时候，它都会失去一些能量，它会被一些需要的材质采样着色。Unity在烘焙间接光照的时候，物体会根据附近的颜色进行着色。 绿色的地面，realtime vs. lightmapped 自发光表面也会影响烘焙光照。它们会成为间接光源。 自发光的地面，realtime vs. lightmapped 间接光照的一个特殊设置是AO环境遮挡：这是指在角落和转折中发生的间接光照造成的阴影。这是一种人为的提升，可以增强深度方面的视觉。 使用环境遮挡的效果 环境遮挡效果完全基于物体表面。它不考虑光线实际来自哪里。烘焙时并不总是正确，举个简单的例子：当与自发光表面组合的时候就会产生一些错误的结果。 显然是错误的环境遮挡效果 透明度-Transparency 光照贴图在一定程度上可以处理半透明表面。 光将通过它们，尽管光的颜色不会被它们所过滤。 半透明的屋顶 镂空材质也可以在光照贴图中正常工作。 镂空的屋顶 但 这仅在使用封闭曲面的时候有效。当使用像是quad这样的单面几何，光线将在不存在的一面损坏。当另外一面没有任何东西的时候，这是很好的，但是当使用单面透明表面的时候会导致问题。 四边形上有一个错误 为了处理这个问题，必须告诉光照贴图系统将这些表面视为透明的。 这可以通过自定义光照贴图设置完成 通过Asset / Create / Lightmap参数来创建这些数据。这些资源允许你自定义每个对象的光照贴图计算。在这种情况下，我们只想表明我们正在处理一个透明的对象。所以启用“它是透明的（Is Transparent）“。 下面它是一个全局作用预计算实时全局光照（Precomputed Realtime GI）部分中的一部分，它会影响所有烘烤光照。 指示这是透明的 单独设置：通过物体的网格渲染器检视器来选择它们。你的资源名字将显示在Lightmap参数的下拉列表中。 为透明四边形使用自定义参数 将物体标记为透明也会改变它对间接光照的贡献。透明物体让间接光通过，而不透明物体则会阻挡间接光。 使用光照贴图 现在我们知道光照贴图是如何工作的，我们可以为Shader着色器添加对光照贴图的支持。第一步是对光照贴图进行采样。调整场景中的球体，以便我们的着色器使用白色材质。 使用我们的白色材质的球体 光照贴图的着色器变体 当一个着色器被认为应该使用光照贴图的时候，Unity会寻找与LIGHTMAP_ON关键字关联的变体。 所以我们必须为这个关键字添加一个多编译指令。 当使用forward-render-path的时候，仅在base-pass中采样光照贴图。 #pragma multi_compile _ SHADOWS_SCREEN #pragma multi_compile _ VERTEXLIGHT_ON #pragma multi_compile _ LIGHTMAP_ON #pragma multi_compile_fog 当使用光照贴图的时候，Unity不会包含顶点光源。他们的关键字是_相互排斥_的。所以我们不需要一个会同时使用_VERTEXLIGHT_ON_和_LIGHTMAP_ON_的变体。（互斥） #pragma multi_compile _ SHADOWS_SCREEN //#pragma multi_compile _ VERTEXLIGHT_ON //#pragma multi_compile _ LIGHTMAP_ON #pragma multi_compile _ LIGHTMAP_ON VERTEXLIGHT_ON #pragma multi_compile_fog 延迟渲染路径中也支持光照贴图，因此也可以将这个关键字添加到延迟渲染通道中。 #pragma multi_compile _ UNITY_HDR_ON #pragma multi_compile _ LIGHTMAP_ON 光照贴图的坐标 用于采样光照贴图的坐标存储在TEXCOORD1。 所以将此通道添加到shader中的VertexData结构体中。Unity给出了uv使用说明表：Shader中是uv0、uv1、uv2、uv3；C#中是UV、UV2、UV3、UV4 struct VertexData { float4 vertex : POSITION; float3 normal : NORMAL; float4 tangent : TANGENT; float2 uv : TEXCOORD0; float2 uv1 : TEXCOORD1; }; 光照贴图坐标也必须进行插值。因为它们与顶点光源互斥，所以都可以使用TEXCOORD6。 struct Interpolators { … #if defined(VERTEXLIGHT_ON) float3 vertexLightColor : TEXCOORD6; #endif #if defined(LIGHTMAP_ON) float2 lightmapUV : TEXCOORD6; #endif }; 来自模型顶点数据的坐标定义了用于光照贴图的纹理展开(第二套uv)。但是它并没有告诉我们这个展开位置在哪里，展开尺寸大小。我们必须缩放和偏移坐标才能得到最终的光照贴图坐标。这种方法类似于常规纹理坐标的转换，除了转换是特定于对象的，而这里的方法是特定于材质的。在_UnityShaderVariables_中将光照贴图的纹理定义为_unity_Lightmap_。 Interpolators MyVertexProgram (VertexData v) { i.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); #if defined(LIGHTMAP_ON) i.lightmapUV = TRANSFORM_TEX(v.uv1, unity_Lightmap); #endif } 不幸的是，我们不能使用方便的_TRANSFORM_TEX_宏，因为它假定光照贴图的变换被被定义为_unity_Lightmap_ST_，而实际上是被定义为_unity_LightmapST_。由于这种不一致，我们必须手动进行这个变换。 i.lightmapUV = v.uv1 *unity_LightmapST.xy + unity_LightmapST.zw; 对光照贴图进行采样-Sampling Lightmap 因为光照贴图的数据被认为是间接光照，我们将在CreateIndirectLight函数中进行采样。当光照贴图可用的时候，必须将它们用作间接光而不是球面谐波。 UnityIndirect CreateIndirectLight (Interpolators i, float3 viewDir) { #if defined(VERTEXLIGHT_ON) indirectLight.diffuse = i.vertexLightColor; #endif #if defined(FORWARD_BASE_PASS) || defined(DEFERRED_PASS) #if defined(LIGHTMAP_ON) indirectLight.diffuse = 0; #else indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); #endif float3 reflectionDir = reflect(-viewDir, i.normal); #endif return indirectLight; } 为什么indirectLight.diffuse的值被赋值而不是加起来？光照贴图从来没有与顶点光源组合起来。 unity_Lightmap的确切形式取决于目标平台。 它被定义为UNITY_DECLARE_TEX2D（unity_Lightmap）。要对它进行采样，我们将使用UNITY_SAMPLE_TEX2D宏而不是tex2D。这是根据不同平台决定。 indirectLight.diffuse = UNITY_SAMPLE_TEX2D(unity_Lightmap, i.lightmapUV); 使用原始光照图数据的效果 我们现在得到了烘焙的间接光照，但效果看起来不对。这是因为光照贴图数据已被编码。颜色以RGBM格式或是半强度格式进行存储，以支持高强度的光。UnityCG的DecodeLightmap函数负责为我们解码。 indirectLight.diffuse = DecodeLightmap ( UNITY_SAMPLE_TEX2D(unity_Lightmap, i.lightmapUV) ); 使用解码后光照图数据的效果 创建光照贴图 目前，光照贴图会将场景对象总是视为不透明和纯白色的物体。我们必须对我们的着色器进行一些调整，添加一个渲染通道来完全支持光照贴图。 从现在开始，对场景中的所有对象使用我们自己的着色器。也不再使用默认的材质。 半透明的阴影-Semitransparent Shadow 光照贴图不使用实时渲染管道，因此现有自写的shader不能支持。 当尝试使用半透明阴影的时候，这是最明显的。通过设置屋顶立方体材质的色调alpha分量小于1来赋予屋顶立方体半透明度。 半透明的屋顶，效果不正确 光照贴图仍然把屋顶看成是实心物体，这是不正确的。它使用材质的渲染类型来确定如何处理表面，这应该告诉光照贴图我们的对象是半透明的。事实上，它确实知道屋顶是半透明的，它只是把它看作是完全不透明的而已。这是因为它采用Unity的命名约定_Color材质属性的alpha组件以及主纹理来设置不透明度。 用_Color替换_Tint。 Properties { // _Tint (\"Tint\", Color) = (1, 1, 1, 1) _Color (\"Tint\", Color) = (1, 1, 1, 1) } 然后，为了保证我们的着色器的功能，我们还必须在shader文件、cg文件替换，而且我们还要调整GUI拓展。 半透明的屋顶，正确的效果 镂空部分的阴影-Cutout Shadow 镂空部分的阴影也有类似的问题。光照贴图程序期望透明度的阈值存储在_Cutoff属性中，但是我们使用的是_AlphaCutoff。 因此，它使用默认阈值1。 镂空的屋顶，效果不正确 解决方案是再次采用Unity的命名约定_Cutoff材质属性。所以替换shader、cg文件、GUI拓展。 镂空的屋顶，正确的效果 添加一个Meta渲染通道-Add Meta Pass 渲染光照贴图正确的表面反照率和自发光。 绿色的地板，效果不正确 要采样物体的表面颜色，光照贴图程序会将它的光照模式设置为Meta来寻找一个着色器渲染通道。这个渲染通道仅由光照贴图程序使用，不使用剔除。所以让我们在我们的着色器上添加一个渲染通道。 Pass { Tags { \"LightMode\" = \"Meta\" } Cull Off CGPROGRAM #pragma vertex MyLightmappingVertexProgram #pragma fragment MyLightmappingFragmentProgram #include \"My Lightmapping.cginc\" ENDCG } 现在我们需要确定反照率、镜面高光颜色、平滑度、自发光。只需要顶点的位置和uv坐标，以及需要vertexProgram中的光照贴图坐标。不使用法线和切线。 #if !defined(MY_LIGHTMAPPING_INCLUDED) #define MY_LIGHTMAPPING_INCLUDED #include \"UnityPBSLighting.cginc\" float4 _Color; sampler2D _MainTex, _DetailTex, _DetailMask; float4 _MainTex_ST, _DetailTex_ST; sampler2D _MetallicMap; float _Metallic; float _Smoothness; sampler2D _EmissionMap; float3 _Emission; struct VertexData { float4 vertex : POSITION; float2 uv : TEXCOORD0; float2 uv1 : TEXCOORD1; }; struct Interpolators { float4 pos : SV_POSITION; float4 uv : TEXCOORD0; }; float GetDetailMask (Interpolators i) { … } float3 GetAlbedo (Interpolators i) { … } float GetMetallic (Interpolators i) { … } float GetSmoothness (Interpolators i) { … } float3 GetEmission (Interpolators i) { … } #endif GetEmission函数去除_FORWARD_BASE_PASS_和_DEFERRED_PASS_限制。 float3 GetEmission (Interpolators i) { // #if defined(FORWARD_BASE_PASS) || defined(DEFERRED_PASS) #if defined(_EMISSION_MAP) return tex2D(_EmissionMap, i.uv.xy) *_Emission; #else return _Emission; #endif // #else // return 0; // #endif } 这些函数只有在定义了适当的关键字时才会起作用，因此可以在渲染通道中为其添加着色功能。 #pragma vertex MyLightmappingVertexProgram #pragma fragment MyLightmappingFragmentProgram #pragma shader_feature _METALLIC_MAP #pragma shader_feature _ _SMOOTHNESS_ALBEDO _SMOOTHNESS_METALLIC #pragma shader_feature _EMISSION_MAP #pragma shader_feature _DETAIL_MASK #pragma shader_feature _DETAIL_ALBEDO_MAP #include \"My Lightmapping.cginc\" 顶点程序-Vertex Program 这个pass的vertex 程序很简单。只是转换位置、转换纹理坐标。 Interpolators MyLightmappingVertexProgram (VertexData v) { Interpolators i; i.pos = UnityObjectToClipPos(v.vertex); i.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); return i; } 计算2.2提到的映射偏移，我们必须使用光照贴图uv坐标而不是顶点位置，然后进行适当的转换把纹理uv坐标作为模型顶点的屏幕位置，模型的UV映射必须要正确：纹理上的每个点必须映射为模型上的唯一点。 Interpolators i; v.vertex.xy = v.uv1 *unity_LightmapST.xy + unity_LightmapST.zw; v.vertex.z = 0; i.pos = UnityObjectToClipPos(v.vertex); v.vertex.z = 0，不是所有机器上都能支持，顶点位置的Z坐标必须以某种方式使用，即使我们不使用它也是如此。Unity的着色器为此使用虚拟值，所以我们将简单地做同样的事情。 Interpolators i; v.vertex.xy = v.uv1 *unity_LightmapST.xy + unity_LightmapST.zw; v.vertex.z = v.vertex.z &gt; 0 ? 0.0001 : 0; i.pos = UnityObjectToClipPos(v.vertex); 片段程序-Fragment Program 在片段程序中，计算输出反照率和自发光颜色。光照贴图程序将通过执行两次渲染来做到这一点，每次执行有一个输出。为了使这个过程更容易，我们可以使用_UnityMetaPass.cginc文_件中定义的_UnityMetaFragment_函数。它使用_UnityMetaInput_结构作为参数，其中包含反照率和自发光颜色。 该函数将决定要输出反照率和自发光颜色中的哪一个以及如何编码输出结果。 UnityMetaInput也包含镜面高光颜色，即使它不存储在光照贴图中。它用于一些编辑器可视化，我们先忽略它。 #include \"UnityPBSLighting.cginc\" #include \"UnityMetaPass.cginc\" … float4 MyLightmappingFragmentProgram (Interpolators i) : SV_TARGET { UnityMetaInput surfaceData; surfaceData.Emission = 0; surfaceData.Albedo = 0; surfaceData.SpecularColor = 0; return UnityMetaFragment(surfaceData); } UnityMetaFragment是什么样子的？ //unity_MetaFragmentControl变量包含一个标记，这个标记会告诉函数是否输出反照率或是自发光颜色。还有一段有关 //编辑器可视化变体的代码，但是我把它删掉了，因为与这里的内容不相关。 half4 UnityMetaFragment (UnityMetaInput IN) { half4 res = 0; if (unity_MetaFragmentControl.x) { res = half4(IN.Albedo,1); // d3d9 shader compiler doesn't like NaNs and infinity. unity_OneOverOutputBoost = saturate(unity_OneOverOutputBoost); // Apply Albedo Boost from LightmapSettings. res.rgb = clamp( pow(res.rgb, unity_OneOverOutputBoost), 0, unity_MaxOutputValue ); } if (unity_MetaFragmentControl.y) { half3 emission; if (unity_UseLinearSpace) emission = IN.Emission; else emission = GammaToLinearSpace (IN.Emission); res = UnityEncodeRGBM(emission, EMISSIVE_RGBM_SCALE); } return res; } 间接光照设置为0的效果 要获得自发光颜色，我们可以简单的使用GetEmission函数。要获得反照率，我们必须再次使用_DiffuseAndSpecularFromMetallic_函数。 该函数具有镜面高光颜色和反射率作为输出参数，即使我们现在不使用它们，我们也必须提供这些参数。我们可以使用surfaceData.SpecularColor来捕获镜面高光颜色。 float4 MyLightmappingFragmentProgram (Interpolators i) : SV_TARGET { UnityMetaInput surfaceData; surfaceData.Emission = GetEmission(i); float oneMinusReflectivity; surfaceData.Albedo = DiffuseAndSpecularFromMetallic ( GetAlbedo(i), GetMetallic(i), surfaceData.SpecularColor, oneMinusReflectivity ); //surfaceData.SpecularColor = 0; return UnityMetaFragment(surfaceData); } 间接光照着色的效果 但自发光光照可能还没有出现在光照贴图中。这是因为光照贴图程序并不总是包含一个自发光光照的渲染通道。材质必须表明它们具有自发光光照属性，以对烘焙过程做出贡献。这是通过Material.globalIlluminationFlags属性完成的。扩展GUI设置：当自发光光 照编辑的时候，它应该被烘焙进光照贴图。 void DoEmission () { … if (EditorGUI.EndChangeCheck()) { if (tex != map.textureValue) { SetKeyword(\"_EMISSION_MAP\", map.textureValue); } foreach (Material m in editor.targets) { m.globalIlluminationFlags = MaterialGlobalIlluminationFlags.BakedEmissive; } } } 粗糙的金属-Rough Metals 我们的shader现在看起来可以正常工作了，但它与标准着色器的结果不完全匹配。 当使用平滑度非常低的有色金属的时候，物体表面不太明亮。 粗糙的绿色金属，standard vs. our 标准着色器通过将反射率的一部分加到镜面高光颜色进行补偿（高亮）。它使用_UnityStandardBRDF.cginc_的_SmoothnessToRoughness函数_来确定基于平滑度的粗糙度值，将其缩小一半，并使用它来缩放镜面高光颜色。 float roughness = SmoothnessToRoughness(GetSmoothness(i)) *0.5; surfaceData.Albedo += surfaceData.SpecularColor *roughness; return UnityMetaFragment(surfaceData); SmoothnessToRoughness计算了什么东西？ //转换：减去平滑度值，然后平方。 从平滑度到粗糙度的平方映射最终会产生比仅仅做线性转换更好的结果。 // Smoothness is the user facing name // it should be perceptualSmoothness // but we don't want the user to have to deal with this name half SmoothnessToRoughness(half smoothness) { return(1 - smoothness) *(1 - smoothness); } 调整反照率后的效果 方向光照贴图-Directinal Lightmap 光照贴图程序只使用物体的顶点数据，不考虑物体的法线贴图。光照贴图的分辨率太低，无法捕获由典型法线贴图提供的细节。这意味着静态光照将是平坦的。当使用具有法线贴图的材质的时候，这变得非常明显。 使用了法线贴图，standard vs. our 当从实时光照切换到烘焙光时，法线贴图的影响几乎完全消失。这是因为它要求环境反射才能看到它们。 方向性-Directionality 通过将“DirectionalMode”改回“Directional”，可以让法线贴图与烘焙光照一起工作。 再次启用定向光照贴图 当使用方向光照贴图的时候，Unity将创建两个贴图。第一张贴图包含通常的光照信息，称为强度图。 第二张贴图被称为方向图。 它包含大部分烘烤光来自的方向。 强度图和方向图 当方向图可用的时候，用它来对烘焙光进行简单的漫反射阴影计算。这使得它可用于法线贴图之上。注意，只有一个光方向是已知的，所以阴影将是一个近似。至少有一个主方向光照的时候，结果就会很好。 对方向进行采样 当方向光照贴图可用的时候，Unity将使用_LIGHTMAP_ON_和_DIRLIGHTMAP_COMBINED_关键字查找着色器变体。我们可以在forward-base-pass通道中使用#pragma multi_compile_fwdbase，而不是为手动添加多编译指令。它会负责解决所有的光照贴图关键字，以及VERTEXLIGHT_ON关键字。 //#pragma multi_compile _ SHADOWS_SCREEN //#pragma multi_compile _ LIGHTMAP_ON VERTEXLIGHT_ON #pragma multi_compile_fwdbase #pragma multi_compile_fog 我们可以为deferred-pass必须使用#pragma multi_compile_prepassfinal指令。 它解决了光照贴图和高动态光照渲染的关键字。 //#pragma multi_compile _ UNITY_HDR_ON //#pragma multi_compile _ LIGHTMAP_ON #pragma multi_compile_prepassfinal prepassfinal是什么东西? Unity 4使用了一种与以后的版本不同的延迟渲染管线。 在Unity 5中，它被称为传统延迟光照。 这种方法有更多的渲染通道。Prepass决定是当时的术语。不需要引入新的指令，#pragma multi_compile_prepassfinal也用于当前的延迟渲染通道。 在CreateIndirectLight函数中，在检索烘焙光源本身后，需要直接获得烘焙光的方向。方向贴图可以通过unity_LightmapInd获得。 #if defined(LIGHTMAP_ON) indirectLight.diffuse = DecodeLightmap(UNITY_SAMPLE_TEX2D(unity_Lightmap, i.lightmapUV)); #if defined(DIRLIGHTMAP_COMBINED) float4 lightmapDirection = UNITY_SAMPLE_TEX2D ( unity_LightmapInd, i.lightmapUV ); #endif #else indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); #endif 但是，这将导致编译错误。这是因为一个纹理变量实际上由两部分组成。 有纹理资源，还有采样器状态。采样器状态决定纹理的采样方式，包括滤波器和截取模式。 通常，每个纹理都定义了这两个部分，但这并不是所有平台都需要的。 也可以将这两个部分分开，这允许 我们为多个纹理定义单个采样器状态。 因为强度和方向贴图总是以相同的方式进行采样，所以在可能的情况下，Unity使用单个采样器状态。 这就是为什么我们在采样强度贴图的时候必须使用UNITY_SAMPLE_TEX2D宏。方向贴图已经定义，没有采样器。 要对其进行采样，我们必须使用UNITY_SAMPLE_TEX2D_SAMPLER宏来明确地告诉它要使用哪个采样器。 float4 lightmapDirection = UNITY_SAMPLE_TEX2D_SAMPLER ( unity_LightmapInd, unity_Lightmap, i.lightmapUV ); 使用方向贴图 要使用方向：1、解码 2、对法向量执行点积，找到漫反射因子并将其应用于颜色。 但是方向贴图并没有包含单位长度的方向，而是比单位长度的方向会大一些。 幸运的是可以使用UnityCG的DecodeDirectionLightmap函数来解码方向数据。 float4 lightmapDirection = UNITY_SAMPLE_TEX2D_SAMPLER ( unity_LightmapInd, unity_Lightmap, i.lightmapUV ); indirectLight.diffuse = DecodeDirectionalLightmap ( indirectLight.diffuse, lightmapDirection, i.normal ); 使用带有方向的光照贴图的效果 DecodeDirectionLightmap内部做了什么？ DecodeDirectionLightmap实际上并不计算正确的漫射照明因子。 相反，它使用的是半Lambert。 这种方法可以有效地将光照射在表面周围，照亮阴影的区域会更多。这么做是有必要的，这是因为烘烤的光照不是来自于单个方向. inline half3 DecodeDirectionalLightmap ( half3 color, fixed4 dirTex, half3 normalWorld ) { // In directional (non-specular) mode Enlighten bakes dominant light // direction in a way, that using it for half Lambert and then dividing // by a \"rebalancing coefficient\" gives a result close to plain diffuse // response lightmaps, but normalmapped. // Note that dir is not unit length on purpose. Its length is // \"directionality\", like for the directional specular lightmaps. half halfLambert = dot(normalWorld, dirTex.xyz - 0.5) + 0.5; return color *halfLambert / max(1e-4h, dirTex.w); } 代码的注释中提到镜面高光。 这些是支持镜面高光的光照贴图，但需要更多的纹理，使用起来也更昂贵，并且在大多数情况下没有产生良好的效果。自Unity 5.6起，它们已被删除了。 光照探针-Light Probes 光照贴图仅适用于静态对象，而不适用于动态对象。 因此，动态对象不适合带有烘烤光照的场景。当没有实时光源的时候，这是非常明显的。 为了更好地混合静态和动态对象，我们必须以某种方式将烘焙的光照应用于动态对象。为了解决这个问题，Unity有光照探针。 光照探针是对空间中的一个点包含该位置的光照信息。 它是用球面谐波来存储这些信息而不是用纹理。 如果可用的话，这些光照探针将用于动态对象，而不是全局环境数据。所以我们要做的就是创建一些探针，等到烘焙的时候，我们的着色器就会自动使用它们。 创建光照探针组 通过GameObject / Light /Light Probe Group将一组光探测器添加到场景中。 这将创建一个新的游戏对象，在立方体的形状中共有八个光探测器。 它们将在渲染动态对象的时候立即使用。 一个新的光探测器组 通过检视器，可以在启用“编辑探针”模式后编辑光探测器组。 放置光照探针 光照探针组将其包围的体积分成四个区域。四个探测器定义了四面体的角。 这些探测器被进行插值以确定用于动态物体的最终球谐函数，这取决于其在四面体内的位置。这意味着动态对象被视为一个单一的点，因此这种方法只对相当小的对象有效。在编辑探测器的时候， 会自动生成四面体。 你不需要知道他们的配置，但它们的可视化信息可以帮助你查看探测器的相对位置。放置光照探针需要你去调整他们的位置，直到你得到一个你可以接受的结果，就像光照贴图的设置一样。首先封装将要包含动态对象的区域。 封装区域 然后根据光照条件如何变化来添加更多的探针。你不必将它们放置在静态几何中。 也不要把它们放在不透明的单面几何体错误的那一面。 放置更多的探测器 继续添加和移动探测器，直到你在所有区域都有了合理的光照条件，并且在它们之间发生的转换是可以接受的。 调整探测器的位置 可以通过移动动态对象来测试探针。当选择一个动态对象的时候，也会显示当前正在发挥作用的探针。探针将显示其光照，而不仅仅是黄色球体。你还可以看到用于动态对象的内插数据。 移动动态对象 通过不同的光照探头，物体的明暗变化明显。" }, { "title": "Unity Deferred Lights-延迟光照(翻译十五)", "url": "/posts/Unity_Deferred_Lights/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-17 20:00:00 +0800", "content": "本篇摘要： 自定义灯光渲染 解码LDR颜色 增加独立Pass渲染光 支持方向光、点光源、聚光灯 手动采样阴影纹理 光照Light Shader 在G-Buffers填充完毕后，然后渲染光。本篇先介绍Unity是如何渲染光，以及实现自己的渲染光的Shader。在Edit / Project Settings / Graphics 去掉默认的Shader。 Using a Custom Shader 每个deferred光都是在一个独立的Pass修改屏幕图像(后处理Image)完成渲染。创建一个Shader然后指定到Built-In shader settings 修改内置的Shader 第二个PassA Second Pass 修改之后，编辑器大量报错. least 2 passes 先简单复制第一个Pass解决错误，结果是屏幕内除了天空盒外所有物体被渲染成黑色了。这是因为使用了stencil-buffer。 报错的原因：为什么需要第二个Pass？ 当HDR禁用时，光照数据会被使用对数编码计算，然后在(第二个)最终的pass解码该数据。所以必须要增加Pass。当禁用HDR时就能调用第二个Pass，但此时天空也变黑了。 Avoiding the Sky 当在LDR（HDR禁用）模式，天空变黑了。这是因为转换过程中没有正确使用stencil-buffer模板掩码。在第二个Pass中配置：应该只渲染不属于背景的片段，可通过_StencilNonBackground提供适当的模板值。 Pass { Stencil { Ref[_StencilNonBackground] ReadMask[_StencilNonBackground] CompBack Equal CompFront Equal } } 颜色转换Converting Colors 在第二个Pass的light-buffer转换光照数据，方法就似Fog shader：用输入源的Image UV坐标采样buffer来绘制一个覆盖全屏的quad struct VertexData { float4 vertex : POSITION; float2 uv : TEXCOORD0; }; struct Interpolators { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; }; Interpolators VertexProgram (VertexData v) { Interpolators i; i.pos = UnityObjectToClipPos(v.vertex); i.uv = v.uv; return i; } 该light buffer通过名为_LightBuffer变量提供给Shader sampler2D _LightBuffer; //... float4 FragmentProgram (Interpolators i) : SV_Target { return tex2D(_LightBuffer, i.uv); } LDR颜色使用指数编码: $2^{-C}$，使用对数解码 $-log2^C$ return -log2(tex2D(_LightBuffer, i.uv)); Directional Lights 新增一个cginc文件，引入第一个pass。要把渲染的光照增加到图像上，必须确保不能擦除已渲染的图像，因此改变混合模式要完全合并源颜色和目标颜色。 Blend One One 也需要所有可能的光照配置shader variants变体，该编译指令：multi_compile_lightpass会创建所有包含的变体。然后再增加一个HDR_ON的指令。 #pragma multi_compile_lightpass #pragma multi_compile _ UNITY_HDR_ON G-Buffer UV Coordinates 需要用UV坐标从G-buffers采样，不幸的是，该light pass通道unity不支持提供该坐标。解决办法：从clip-space传递过来，使用ComputeScreenPos函数计算，返回一个float4的齐次坐标。 v2f VertexProgram(appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = ComputeScreenPos(o.pos); return o; } 然后在fragment就能计算最终的2D坐标。必须在fragment计算。见翻译7 fixed4 FragmentProgram(v2f i) : SV_Target { float2 uv = i.uv.xy / i.uv.w; return 0; } 坐标转换World Position 与上篇deferred fog中相似，需要计算从相机到片元的距离：从相机原点发射射线通过片元(给定方向)到达far-plane，然后再用fragment深度缩放射线。用该方法重建片元的世界坐标。 首先。对于方向光，从quad的四顶点发出的射线作为法向量提供。所以可以通过顶点程序对射线进行插值。 struct VertexData { float4 vertex : POSITION; float3 normal : NORMAL; }; struct Interpolators { float4 pos : SV_POSITION; float4 uv : TEXCOORD0; float3 ray : TEXCOORD1; }; Interpolators VertexProgram (VertexData v) { Interpolators i; i.pos = UnityObjectToClipPos(v.vertex); i.uv = ComputeScreenPos(i.pos); i.ray = v.normal; return i; } 其次。在fragment函数通过采样_CameraDepthTexture纹理和线性化计算可以得到depth值，类似于deferred fog计算 //Unity提供的声明函数，等于 sampler2D _CameraDepthTexture; 定义在UnityCG UNITY_DECLARE_DEPTH_TEXTURE(_CameraDepthTexture); float4 FragmentProgram (Interpolators i) : SV_Target { float2 uv = i.uv.xy / i.uv.w; float depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, uv); depth = Linear01Depth(depth); return 0; } 然后。与deferred fog最大的不同：fog shader需要射线到达far plane；而本shader的射线只能到达near plane。所以必须要缩放射线以便它能达到far-plane：缩放射线使Z坐标变为1，并与远平面距离相乘。 depth = Linear01Depth(depth); float3 rayToFarPlane = i.ray * _ProjectionParams.z / i.ray.z; 再接着。按深度值缩放射线一次得到一个坐标。该射线被定义在视图空间，它是camera的本地空间。因此，射线也以片段在视图空间中的坐标结束。 float3 rayToFarPlane = i.ray * _ProjectionParams.z / i.ray.z; float3 viewPos = rayToFarPlane * depth; 最后。再使用unity_CameraToWorld内置矩阵从view视图空间转换到world世界坐标，该矩阵定义在ShaderVariables.cginc float3 viewPos = rayToFarPlane * depth; float3 worldPos = mul(unity_CameraToWorld, float4(viewPos, 1)).xyz; 读取G-Buff Reading G-Buffer Data 获取World Pos后。通过访问G-buffer检索properties，该buffer可从内置的_CamearGBufferTexture变量获取 sampler2D _CameraGBufferTexture0; sampler2D _CameraGBufferTexture1; sampler2D _CameraGBufferTexture2; 在上一篇Defferred Shading中也手动计算过G-buffer,这次直接读取_CameraGBufferTexture现成的albedo、specular、smoothness、normal float3 worldPos = mul(unity_CameraToWorld, float4(viewPos, 1)).xyz; float3 albedo = tex2D(_CameraGBufferTexture0, uv).rgb; float3 specularTint = tex2D(_CameraGBufferTexture1, uv).rgb;//合并 float3 smoothness = tex2D(_CameraGBufferTexture1, uv).a;//合并 float3 normal = tex2D(_CameraGBufferTexture2, uv).rgb * 2 - 1; 计算BRD Computing BRDF 引入BRDF函数，定义在UnityPBSLighting.cginc中 首先计算视野方向 float3 worldPos = mul(unity_CameraToWorld, float4(viewPos, 1)).xyz; float3 viewDir = normalize(_WorldSpaceCameraPos - worldPos); 其次是表面反射，这可从specular颜色获取，使用SpecularStrength函数提取。 //... float oneMinusReflectivity = 1 - SpecularStrength(specularTint); 然后传递光照数据，初始化直接光和间接光 float oneMinusReflectivity = 1 - SpecularStrength(specularTint); //... UnityLight light; light.color = 0; light.dir = 0; UnityIndirect indirectLight; indirectLight.diffuse = 0; indirectLight.specular = 0; 最后计算最终的颜色 indirectLight.specular = 0; float4 color = UNITY_BRDF_PBS ( albedo, specularTint, oneMinusReflectivity, smoothness, normal, viewDir, light, indirectLight ); return color; 光源配置 Configuring the Light 因为间接光呈现的是黑色的，在这里不适用。但是直接光必须被配置成与当前渲染的光相匹配。对于方向光，需要它的颜色和方向。这两个变量可以通过_LightColor和_LightDir变量获得。 float4 _LightColor, _LightDir; UnityLight CreateLight () { UnityLight light; light.dir = _LightDir; light.color = _LightColor.rgb; return light; } UnityLight light = CreateLight(); // light.color = 0; // light.dir = 0; 光照方向错误 计算得到最终的光照，但光的方向错误了。原因：_LightDir是光到表面的方向。在CreateLight计算中需要表面到光的方向 light.dir = -_LightDir; 正确，没有阴影 阴影 Shadows 在自己的cginc文件中，我们依靠AutoLight中的宏来确定由阴影引起的光衰减。 不幸的是，该文件在编写时并没有考虑到延迟的光线。 现在将自己进行阴影采样，可通过_ShadowMapTexture变量访问阴影贴图。 sampler2D _ShadowMapTexture; 但是，我们不能随意声明此变量。 它已经在UnityShadowLibrary中为点和聚光灯阴影定义了它。 因此，我们不应该自己定义它，除非使用方向光阴影。 #if defined (SHADOWS_SCREEN) sampler2D _ShadowMapTexture; #endif 要应用方向光阴影，需要采样阴影纹理并使用它来减弱光色即可。 在CreateLight中计算就需要把UV坐标参数。 UnityLight CreateLight (float2 uv) { UnityLight light; light.dir = -_LightDir; float shadowAttenuation = tex2D(_ShadowMapTexture, uv).r; light.color = _LightColor.rgb * shadowAttenuation; return light; } UnityLight light = CreateLight(uv); 有阴影的方向光 当然，这仅在定向光启用了阴影时才有效。 如果不是，则阴影衰减始终为1。 float shadowAttenuation = 1; #if defined(SHADOWS_SCREEN) shadowAttenuation = tex2D(_ShadowMapTexture, uv).r; #endif light.color = _LightColor.rgb * shadowAttenuation; Fading Shadows 阴影贴图应该是有限的，它覆盖的面积越大，阴影的分辨率越低。 Unity提供了绘制阴影的最大距离，此距离可以通过_Edit / Project Settings / Quality_进行调整。 阴影距离配置 当阴影几乎快达到了该限定距离就会淡出，Unity内置的shader是这样设定并计算。由于我将手动采样该阴影纹理，当到达纹理的边缘时阴影会被截取，结果是阴影虽然消失了，但有被急剧切割的生硬画面。 长、短距离阴影对比 要渐隐阴影，首先要知道的是阴影完全消失的距离。该距离又依赖于阴影投射方向。在Stable Fit模式下，以map的中心点呈球面形开始渐隐消失阴影；在Close Fit模式它是依赖于视野深度。 UnityComputeShadowFadeDistance函数能计算出正确距离，它需要两个参数：world pos 和 view depth；然后返回距离A。 注意：该距离A是从阴影纹理的中心点位置或者未更改的视野深度开始计算的。 UnityLight CreateLight (float2 uv, float3 worldPos, float viewZ) { UnityLight light; light.dir = -_LightDir; float shadowAttenuation = 1; #if defined(SHADOWS_SCREEN) shadowAttenuation = tex2D(_ShadowMapTexture, uv).r; float shadowFadeDistance = UnityComputeShadowFadeDistance(worldPos, viewZ); #endif light.color = _LightColor.rgb * shadowAttenuation; return light; } 阴影应该是快要接近渐隐距离时开始消失，一旦到达就完全消失。UnityComputeShadowFade函数计算合适的消失因子。 float shadowFadeDistance = UnityComputeShadowFadeDistance(worldPos, viewZ); float shadowFade = UnityComputeShadowFade(shadowFadeDistance); UnityComputeShadowFade 定义在UnityShadowLibrary.cginc，见下： float UnityComputeShadowFadeDistance (float3 wpos, float z) { float sphereDist = distance(wpos, unity_ShadowFadeCenterAndType.xyz); return lerp(z, sphereDist, unity_ShadowFadeCenterAndType.w); } half UnityComputeShadowFade(float fadeDist) { return saturate(fadeDist * _LightShadowData.z + _LightShadowData.w); } 阴影渐隐值范围是[0, 1]，该值决定了阴影要消失多少。实际的消失值可以加到阴影衰减之上并限定在[0, 1]之内 float shadowFade = UnityComputeShadowFade(shadowFadeDistance); shadowAttenuation = saturate(shadowAttenuation + shadowFade); 最后，提供世界坐标和视图深度在片元程序中创建光照。视图深度是片元在视图空间中的位置的Z分量。 UnityLight light = CreateLight(uv, worldPos, viewPos.z); 阴影渐隐 Light Cookies 支持Cookies纹理，使用变量 _LightTexture0 访问；同时还要从world-space转换到light-space，最后采样。转换矩阵使用 _unity_WorldToLight 矩阵变量 sampler2D _LightTexture0; float4x4 unity_WorldToLight; 在 CreateLight，使用上述矩阵变量转换world-space到light-space；然后使用转换后的坐标采样cookie纹理。cookie也要衰减，需要单独定义并使用。 light.dir = -_LightDir; float attenuation = 1; float shadowAttenuation = 1; #if defined(DIRECTIONAL_COOKIE) float2 uvCookie = mul(unity_WorldToLight, float4(worldPos, 1)).xy; attenuation *= tex2D(_LightTexture0, uvCookie).w; #endif //... light.color = _LightColor.rgb * (attenuation * shadowAttenuation); 带有cookie的方向光 整体结果似乎可以，但是观察边缘似乎有硬边 硬边过渡 相邻片元的cookie坐标的巨大差异就会导致该问题出现。在这种情况下，GPU选择的mipmap级别对于最近的表面是low level。解决办法之一就是：在采样mip映射时应用偏移。大v的总结 attenuation *= tex2Dbias(_LightTexture0, float4(uvCookie, 0, -8)).w; 偏移采样 支持LDR Supporting LDR 上述只支持HDR，现在来支持LDR。步骤如下： 首先，编码后的LDR颜色要乘如light-buffer，而不是加法。这可以用：Blend DstColor Zero实现。注意只用该Blend mode会引起HDR的错误。所以需要灵活配置：Blend [_SrcBlend] [_DstBlend] 然后，使用 $2^{-c}$ 函数解码 float4 color = UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, smoothness, normal, viewDir, light, indirectLight ); #if !defined(UNITY_HDR_ON) color = exp2(-color); #endif return color; 聚光源 Spotlights 因为方向光会影响到场景内所有物体，所以被画成全屏quad。相比之下，聚光灯只会影响位于圆锥体内的部分物体。通常不需要计算整个图像的聚光灯光照，将绘制一个与聚光灯的影响范围相匹配的金字塔体。 Drawing a Pyramid 禁用方向灯，改用聚光灯。因为着色器只对方向光正确工作，那么现在的结果会出现错误。但是它仍可以让你看到金字塔的哪些部分被渲染了。 渲染范围 根据上图，金字塔是作为一个普通的3D对象呈现的。它的背面被剔除，所以我们可以看到金字塔的正面。只有当它前面没有东西的时候，它才会被画出来。除此之外，还添加了一个pass，用于设置模板缓冲区，以将绘图限制为位于金字塔卷内的片段。您可以通过frame-debugger来验证。 剔除方式 这意味着我们的着色器的culling和z-test设置被否弃了。 因此将其从着色器中删除。 Blend [_SrcBlend] [_DstBlend] //Cull Off //ZTest Always ZWrite Off 当聚光灯的体积距离相机足够远时，此方法适用。 但是，当聚光灯离摄像机太近时，它会失败。 发生这种情况时，相机可能会进入了该体积内。 甚至有可能将近平面的一部分置于其内部，而将其余部分置于其外部，与近平面相交了。 在这些情况下，模板缓冲区不能用于限制渲染。 仍然渲染光照的技巧是绘制金字塔的内表面，而不是金字塔的外表面。 这是通过渲染其背面而不是其正面来完成的。 而且，仅当这些表面最终位于已渲染的表面之后时才渲染它们。 这种方法还涵盖了聚光灯体积内的所有片段。 但这最终导致渲染了太多的碎片，因为通常金字塔的通常隐藏部分也将被渲染。 因此，仅在必要时执行。 当靠近相机时，要绘制背面才正确 支持多光源 Supporting Multiple Light Types 目前，CreateLight只能用于方向光。让我们确保特定于方向灯的代码只在适当的时候使用。 UnityLight CreateLight (float2 uv, float3 worldPos, float viewZ) { UnityLight light; //light.dir = -_LightDir; float attenuation = 1; float shadowAttenuation = 1; #if defined(DIRECTIONAL) || defined(DIRECTIONAL_COOKIE) light.dir = -_LightDir; #if defined(DIRECTIONAL_COOKIE) float2 uvCookie = mul(unity_WorldToLight, float4(worldPos, 1)).xy; attenuation *= tex2Dbias(_LightTexture0, float4(uvCookie, 0, -8)).w; #endif #if defined(SHADOWS_SCREEN) shadowAttenuation = tex2D(_ShadowMapTexture, uv).r; float shadowFadeDistance = UnityComputeShadowFadeDistance(worldPos, viewZ); float shadowFade = UnityComputeShadowFade(shadowFadeDistance); shadowAttenuation = saturate(shadowAttenuation + shadowFade); #endif #else light.dir = 1; #endif light.color = _LightColor.rgb * (attenuation * shadowAttenuation); return light; } 尽管阴影衰落基于方向阴影贴图，但是其他类型的阴影也应该会被渐隐。 这样可以确保所有阴影都以相同的方式渐隐，而不仅仅是某些阴影。 因此，只要有阴影，阴影淡入淡出代码便适用于所有灯光。 因此，让我们将该代码移到特定于光源的块之外。 我们可以使用布尔值来控制是否使用阴影淡出代码。由于布尔值是一个常数值，如果它仍然为假，代码将被删除。 UnityLight CreateLight (float2 uv, float3 worldPos, float viewZ) { UnityLight light; float attenuation = 1; float shadowAttenuation = 1; bool shadowed = false; #if defined(DIRECTIONAL) || defined(DIRECTIONAL_COOKIE) //省略代码 #if defined(SHADOWS_SCREEN) shadowed = true; shadowAttenuation = tex2D(_ShadowMapTexture, uv).r; // float shadowFadeDistance = UnityComputeShadowFadeDistance(worldPos, viewZ); // float shadowFade = UnityComputeShadowFade(shadowFadeDistance); // shadowAttenuation = saturate(shadowAttenuation + shadowFade); #endif #else light.dir = 1; #endif if (shadowed) { float shadowFadeDistance = UnityComputeShadowFadeDistance(worldPos, viewZ); float shadowFade = UnityComputeShadowFade(shadowFadeDistance); shadowAttenuation = saturate(shadowAttenuation + shadowFade); } light.color = _LightColor.rgb * (attenuation * shadowAttenuation); return light; } 非方向灯光都有一个position变量。它通过内置的 _LightPos提供。 float4 _LightColor, _LightDir, _LightPos; 现在可以确定聚光灯的光向量得出光方向。 #else float3 lightVec = _LightPos.xyz - worldPos; light.dir = normalize(lightVec); #endif World Position Agin 结果为黑色，似乎光线方向不正确。 发生这种情况是因为聚光灯的世界位置计算不正确。 当我们在场景中的某个地方渲染金字塔时，不像方向光那样渲染全屏quad将光线存储在normal通道中。 而必须是经由Vertex-Program从顶点的位置发射射线，通过将顶点的pos转换到view-space完成计算，为此，我们可以使用UnityObjectToViewPos函数。 i.ray = UnityObjectToViewPos(v.vertex); 然而，这会产生方向错误的光线。我们要消去它们的X和Y坐标。 i.ray = UnityObjectToViewPos(v.vertex) * float3(-1, -1, 1); 正确的世界位置 再次看看UnityObjectToViewPos内部实现 inline float3 UnityObjectToViewPos (in float3 pos) { return mul(UNITY_MATRIX_V, mul(unity_ObjectToWorld, float4(pos, 1.0))).xyz; } 当渲染方向光时，应该只使用顶点法线。当渲染非方向灯以外的光几何时，需要把顶点pos转到view-space计算。Unity通过 _LightAsQuad 变量告诉我们正在处理哪种情况。 如果 _LightAsQuad 被设为1，则处理的是方向光quad并且可以使用法线。否则，我们必须使用UnityObjectToViewPos。插值好过if ==&gt; from + (to – from) * t, t为1直接使用法线，为0直接计算到view-space i.ray = lerp ( UnityObjectToViewPos(v.vertex) * float3(-1, -1, 1), v.normal, _LightAsQuad ); 锥形衰减 Cookie Attenuation 聚光灯的锥形衰减是通过cookie纹理创建的，无论是默认的圆形还是定制的cookie。我们可以从复制定向光的cookie代码，仿照着写。也是存储在 _LightTexture0 float3 lightVec = _LightPos.xyz - worldPos; light.dir = normalize(lightVec); float2 uvCookie = mul(unity_WorldToLight, float4(worldPos, 1)).xy; attenuation *= tex2Dbias(_LightTexture0, float4(uvCookie, 0, -8)).w; 但是，聚光灯Cookie越远离灯光位置，它就会变得越大。 这是由于通过透视变换造成的。 因此，矩阵乘法会产生4D齐次坐标。 为了得到规则的2D坐标，我们必须将X and Y除以W。 float4 uvCookie = mul(unity_WorldToLight, float4(worldPos, 1)); uvCookie.xy /= uvCookie.w; attenuation *= tex2Dbias(_LightTexture0, float4(uvCookie.xy, 0, -8)).w; cookie衰减 上图实际上产生了两个光锥，一个向前一个向后。 后向圆锥通常在渲染区域之外结束，但这并不能保证。我们只需要前向锥，它对应于负的W坐标。 attenuation *= tex2Dbias(_LightTexture0, float4(uvCookie.xy, 0, -8)).w; attenuation *= uvCookie.w &lt; 0; 距离衰减 Distance Attenuation 聚光灯发出的光也会根据距离衰减。此衰减存储在查找纹理中，可通过 _LightTextureB0 使用该纹理。 sampler2D _LightTexture0, _LightTextureB0; 纹理被设计成必须使用光的距离的平方，并按光的范围进行缩放，作为UV进行采样。范围存储在 _LightPos的第四个分量中。采样得到的纹理应该使用哪个通道在不同的平台，由 _UNITY_ATTEN_CHANNEL 宏定义。 light.dir = normalize(lightVec); attenuation *= tex2D ( _LightTextureB0, (dot(lightVec, lightVec) * _LightPos.w).rr ).UNITY_ATTEN_CHANNEL; float4 uvCookie = mul(unity_WorldToLight, float4(worldPos, 1)); cookie 和 distance衰减 Shadows 当聚光灯有阴影时，定义SHADOWS_DEPTH关键字。 //在CreateLight中 float4 uvCookie = mul(unity_WorldToLight, float4(worldPos, 1)); uvCookie.xy /= uvCookie.w; attenuation *= tex2Dbias(_LightTexture0, float4(uvCookie.xy, 0, -8)).w; #if defined(SHADOWS_DEPTH) shadowed = true; #endif 聚光灯和方向灯使用相同的变量来采样阴影贴图。在聚光灯的情况下，可以使用内置UnitySampleShadowmap来处理采样硬阴影或软阴影的细节。参数：阴影空间中的片元位置。unity_WorldToShadow_(4x4)_矩阵中第一个数组可以用来将世界空间转换为阴影空间。 shadowed = true; shadowAttenuation = UnitySampleShadowmap( mul(unity_WorldToShadow[0], float4(worldPos, 1)) ); 点光源 Point Lights 点光源使用与聚光灯相同的光向量、方向和距离衰减。这样他们就可以共享代码.应该只在定义SPOT关键字时使用spotlight代码的其余部分。 #if defined(DIRECTIONAL) || defined(DIRECTIONAL_COOKIE) //... #else float3 lightVec = _LightPos.xyz - worldPos; light.dir = normalize(lightVec); attenuation *= tex2D( _LightTextureB0, (dot(lightVec, lightVec) * _LightPos.w).rr ).UNITY_ATTEN_CHANNEL; #if defined(SPOT) float4 uvCookie = mul(unity_WorldToLight, float4(worldPos, 1)); uvCookie.xy /= uvCookie.w; attenuation *= tex2Dbias(_LightTexture0, float4(uvCookie.xy, 0, -8)).w; attenuation *= uvCookie.w &lt; 0; #if defined(SHADOWS_DEPTH) shadowed = true; shadowAttenuation = UnitySampleShadowmap( mul(unity_WorldToShadow[0], float4(worldPos, 1)) ); #endif #endif #endif 这已经足够让点光源工作了。它们被渲染成和聚光灯一样的效果，除了渲染范围使用的是球形而不是锥形。 高亮 Shadows 点光源的阴影存储在一个CubeMap。内置UnitySampleShadowmap可采样。参数：光的方向。一个从光到表面的向量。它是光的相反方向。 #if defined(SPOT) //... #else #if defined(SHADOWS_CUBE) shadowed = true; shadowAttenuation = UnitySampleShadowmap(-lightVec); #endif #endif 点光源阴影 Cookies Point light cookie也可以通过 _LightTexture0获得。需要的是一个cubeMap映射，而不是常规的纹理。 //sampler2D _LightTexture0, _LightTextureB0; #if defined(POINT_COOKIE) samplerCUBE _LightTexture0; #else sampler2D _LightTexture0; #endif sampler2D _LightTextureB0; float4x4 unity_WorldToLight; 要对cookie进行采样，请将片段的world-space转换为light-space，并使用光照空间对立方体映射进行采样。 #else #if defined(POINT_COOKIE) float3 uvCookie = mul(unity_WorldToLight, float4(worldPos, 1)).xyz; attenuation *= texCUBEbias(_LightTexture0, float4(uvCookie, -8)).w; #endif #if defined(SHADOWS_CUBE) shadowed = true; shadowAttenuation = UnitySampleShadowmap(-lightVec); #endif #endif 点光源cookie Skipping Shadows 现在，我们可以使用自己的着色器渲染所有动态光源。 尽管我们目前并未对优化进行太多关注，但仍有一项潜在的大型优化值得考虑：最终超出阴影渐隐距离的片元将不会被阴影化。 但是现在仍在采样它们的阴影，这可能很昂贵。 我们可以通过基于阴影衰落因 子进行 _UNITY_BRANCH_分支来避免这种情况。 它接近1，那么我们可以完全跳过阴影衰减。 if (shadowed) { float shadowFadeDistance = UnityComputeShadowFadeDistance(worldPos, viewZ); float shadowFade = UnityComputeShadowFade(shadowFadeDistance); shadowAttenuation = saturate(shadowAttenuation + shadowFade); UNITY_BRANCH if (shadowFade &gt; 0.99) { shadowAttenuation = 1; } } 但是，即使用了 _UNITY_BRANCH_分支它本身也很昂贵。除了靠近阴影区域的边缘，所有碎片都落在阴影区域的内部或外部。 但这仅在GPU可以利用这一点的情况下才重要。 在这种情况下，使用HLSLSupport.cginc定义UNITY_FAST_COHERENT_DYNAMIC_BRANCHING宏。 #if defined(UNITY_FAST_COHERENT_DYNAMIC_BRANCHING) UNITY_BRANCH if (shadowFade &gt; 0.99) { shadowAttenuation = 1; } #endif 即使这样，仅当阴影需要多个纹理样本时才值得使用。 对于柔和的聚光灯和点光源阴影，进一步使用用SHADOWS_SOFT关键字指示。 而方向光阴影始终只需要单个纹理，因此它性能很便宜。 [射线对应的Z值 ={射线击中物体时的长度\\over射线总长度}] #if defined(UNITY_FAST_COHERENT_DYNAMIC_BRANCHING) &amp;&amp; defined(SHADOWS_SOFT) UNITY_BRANCH if (shadowFade &gt; 0.99) { shadowAttenuation = 1; } #endif" }, { "title": "Unity Shader Fog(翻译十四)", "url": "/posts/UNity_Shader_Fog/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-15 20:00:00 +0800", "content": "本篇摘要： 应用雾到游戏对象 基于距离或深度的雾 支持deferred fog 前向渲染雾-Forward Fog 在14之前，一直假定着光线在真空中传播，在真空中可能是精确的。但是当光线穿过大气或水就不一样了，光线在击中物体表面时会发生被吸收、散射和反射。 一个精确的大气干扰光线渲染将需要及其昂贵的体积测量方法，那是大多数现代GPU负担不起的。相反，勉强采用一些常量雾参数近似模拟。 标准雾-Standard Fog Unity光照设置包含了场景雾设置选项，默认是不启用。启用后，默认是灰色雾。Unity自带的雾只适用于使用了Forward渲染路径的物体。若激活Deferred path，提示： deferred 提示 不明显的雾 线性雾-Linear Fog 图2不明显，是因为Fog color灰色雾将散射和反射更多的光线，吸收较少。把Fog Color改为纯黑色试试 linear fog 雾的浓度是随视距线性增长的，在视距开头正常显示，超过这个距离就只有雾的颜色可见。 线性雾公式： \\(f = { {E-c} \\over {E-s} }\\) c 是雾坐标； s 是视距起始距离； E 是视距终止距离； f值 被限定在[0, 1]范围，被用在雾和物体着色之间插值。 最终计算在fragment color着色到物体对象上，雾不会影响到skybox 指数雾-Exponential Fog 更接近真实感的雾 指数雾 指数雾公式： \\(f = {1 \\over {2^{cd}}}\\) d 是fog的密度因子； c 是距离因子。 指数平方雾-Exponential Squared Fog 指数平方雾 指数平方雾公式： \\(f = {1 \\over {2^{(cd)^2}}}\\) 渲染雾-Adding Fog 增加Fog到自己的shader中，增加Fog需要使用内置关键字：multi_compile_fog_指令。该指令的变体会额外增加：_FOG_LINEAR、FOG_EXP、FOG_EXP2 #pragma multi_compile_fog 新增ApplyFog()函数，用于在Fragment计算最终着色：获取当前颜色和插值数据作为参数，返回最终颜色。 计算步骤： 任何雾公式都是基于视距的，首先计算出视距值备用； 然后使用UnityCG.cginc宏_UNITY_CALC_FOG_FACTOR_RAW_根据具体雾公式计算出雾因子。 最后根据雾因子，在fog_color和当前color取插值返回。 float4 ApplyFOG(float4 color, Interpolators i) { float viewDistance = length(_WorldSpaceCameraPos - i.worldPos); UNITY_CALC_FOG_FACTOR_RAW(viewDistance); return learp(unity_FogColor, color, unityFogFactor); } //宏UNITY_CALC_FOG_FACTOR_RAW #if defined(FOG_LINEAR) // factor = (end-z)/(end-start) = z * (-1/(end-start)) + (end/(end-start)) #define UNITY_CALC_FOG_FACTOR_RAW(coord) float unityFogFactor = (coord) * unity_FogParams.z + unity_FogParams.w #elif defined(FOG_EXP) // factor = exp(-density*z) #define UNITY_CALC_FOG_FACTOR_RAW(coord) float unityFogFactor = unity_FogParams.y * (coord); unityFogFactor = exp2(-unityFogFactor) #elif defined(FOG_EXP2) // factor = exp(-(density*z)^2) #define UNITY_CALC_FOG_FACTOR_RAW(coord) float unityFogFactor = unity_FogParams.x * (coord); unityFogFactor = exp2(-unityFogFactor*unityFogFactor) #else #define UNITY_CALC_FOG_FACTOR_RAW(coord) float unityFogFactor = 0.0 #endif //宏UNITY_CALC_FOG_FACTOR #define UNITY_CALC_FOG_FACTOR(coord) UNITY_CALC_FOG_FACTOR_RAW(UNITY_Z_0_FAR_FROM_CLIPSPACE(coord)) //unity_FogParams 定义在ShaderVariables // x = density / sqrt(ln(2)), useful for Exp2 mode // y = density / ln(2), useful for Exp mode // z = -1/(end-start), useful for Linear mode // w = end/(end-start), useful for Linear mode float4 unity_FogParams; 注意雾因子必须限定在[0,1] return learp(unity_FogColor, color, saturate(unityFogFactor)); 同时雾也不能影响Alpha值 color.rgb = learp(unity_FogColor.rgb, color.rgb, saturate(unityFogFactor)); linear：standard vs. mine exp：standard vs. mine exp2：standard vs. mine 深度雾-Depth-Based Fog 增加深度雾支持。与Standard Shader不同的原因是计算fog坐标方法不同。虽然使用world-space视图距离是有意义的，但标准着色器使用裁剪空间深度值。因此视角不影响雾坐标。此外，在某些情况下，距离是受相机的近裁切面距离的影响，这将把雾推开一点。 深度 (三角) vs. 距离(园) 基于深度代替距离的优点是：不必计算平方根，计算速度更快，适用于非真实渲染。缺点是：忽略视角，也即相机以原点旋转会影响雾密度，因为旋转时密度会改变。 红到蓝旋转，深度改变密度 支持depth-based深度雾 ，必须把clip-pass裁剪空间深度值传递到片元函数。定义一个关键字：FOG_DEPTH. #if defined(FOG_LINEAR) || defined(FOG_EXP) || defined(FOG_EXP2) #define FOG_DEPTH 1 #endif 由于需要多存储一个z值，但又不能新增一个独立的变量，就把worldPos改为float4 #if defined(FOG_DEPTH) float4 worldPos : TEXCOORD4; #else float3 worldPos : TEXCOORD4; #endif 然后要替换i.worldPos的所有用法为i.worldPos.xyz。将剪贴空间深度值赋给i.worldPos.w，在fragment传递给viewDistance。它只是齐次剪贴空间位置的Z坐标，所以在它被转换为0-1范围内的值之前。 错误 正确 不正确的原因：可能会有反向裁剪空间Z的情况，需要转换。 #if defined(UNITY_REVERSED_Z) //D3d with reversed Z =&gt; //z clip range is [near, 0] -&gt; remapping to [0, far] //max is required to protect ourselves from near plane not being //correct/meaningfull in case of oblique matrices. #define UNITY_Z_0_FAR_FROM_CLIPSPACE(coord) max(((1.0-(coord)/_ProjectionParams.y)*_ProjectionParams.z),0) #elif UNITY_UV_STARTS_AT_TOP //D3d without reversed z =&gt; z clip range is [0, far] -&gt; nothing to do #define UNITY_Z_0_FAR_FROM_CLIPSPACE(coord) (coord) #else //Opengl =&gt; z clip range is [-near, far] -&gt; should remap in theory //but dont do it in practice to save some perf (range is close enought) #define UNITY_Z_0_FAR_FROM_CLIPSPACE(coord) (coord) #endif #define UNITY_CALC_FOG_FACTOR(coord) UNITY_CALC_FOG_FACTOR_RAW(UNITY_Z_0_FAR_FROM_CLIPSPACE(coord)) 裁剪空间与世界空间-Clip-Space Depth vs World-Space Distance 增加双支持！FOG_DISTANCE 和 FOG_DEPTH。用宏代替feature指令，仿照_BINORMAL_PER_FRAGMENT_定义_FOG_DISTANCE，_默认就是它。 CGINCLUDE #define BINORMAL_PER_FRAGMENT #define FOG_DISTANCE ENDCG //在shader中，要切换到基于距离的雾，如果FOG_DISTANCE已经被定义,我们要做的就是去掉FOG_DEPTH的定义。 #if defined(FOG_LINEAR) || defined(FOG_EXP) || defined(FOG_EXP2) #if !defined(FOG_DISTANCE) #define FOG_DEPTH 1 #endif #endif Disabling Fog 增加支持禁用。只在需要时使用雾，增加FOG_ON宏 #if defined(FOG_LINEAR) || defined(FOG_EXP) || defined(FOG_EXP2) #if !defined(FOG_DISTANCE) #define FOG_DEPTH 1 #endif #define FOG_ON 1 #endif float4 ApplyFog (float4 color, Interpolators i) { #if FOG_ON float viewDistance = length(_WorldSpaceCameraPos - i.worldPos.xyz); #if FOG_DEPTH viewDistance = UNITY_Z_0_FAR_FROM_CLIPSPACE(i.worldPos.w); #endif UNITY_CALC_FOG_FACTOR_RAW(viewDistance); color.rgb = lerp(unity_FogColor.rgb, color.rgb, saturate(unityFogFactor)); #endif return color; } 多光源-Multiple Lights 增加支持多光源。但是变得更亮了，这是因为每个光的颜色都叠加到了雾色之上，所以黑色雾是没问题。 太亮 解决办法就是：对additive pass使用黑色雾，这样就会淡化一部分颜色。 float3 fogColor = 0; #if defined(FORWARD_BASE_PASS) fogColor = unity_FogColor.rgb; #endif color.rgb = lerp(fogColor, color.rgb, saturate(unityFogFactor)); 太亮 延迟雾渲染-Deferred Fog deferred路径没有雾，这是因为所有的光照计算完成后，才会计算雾。为了能够在deferred渲染雾，见2.1 后处理效果-Image Effects 要增加雾渲染，需要等所有光照计算直到它们完成后，在其他pass再次渲染雾。该pass不在shader内部，属于屏幕ImageEffects(后处理)阶段。 [ExecuteInEditMode] public class DeferredFogRender : MonoBehaviour { private void OnRenderImage(RenderTexture source, RenderTexture destination) { } } 这是增加了一个全屏后处理pass。如果有多个这样实现了OnRenderImage脚本，将会按顺序依次执行。 OnRenderImage(RenderTexture source, RenderTexture destination) 两个参数： source 是已计算好最终颜色 destination 输出雾。若为空直接进入帧缓冲区 方法内部必须调用Graphics.Blit函数，它会画一个全屏面片输出到Destination void OnRenderImage (RenderTexture source, RenderTexture destination) { Graphics.Blit(source, destination); } 后处理pass Fog Shader 2.1只是做了简单的拷贝，没什么用。必须要新建一个处理sourceTexture的shader来渲染雾。基本框架： Shader \"Custom/MyDeferredFog\" { Properties { _MainTex (\"Texture\", 2D) = \"white\" {} } SubShader { // No culling or depth Cull Off ZWrite Off ZTest Always Pass { } } } 然后用后处理脚本需要引用该Shader Pass { CGPROGRAM #pragma vertex VertexProgram #pragma fragment FragmentProgram #pragma multi_compile_fog #include \"UnityCG.cginc\" sampler2D _MainTex; struct modelData { float4 vertex : POSITION; float2 uv : TEXCOORD0; }; struct Interpolarters { float4 position : SV_POSITION; float2 uv : TEXCOORD0; }; Interpolarters VertexProgram(modelData m) { Interpolarters i; i.position = UnityObjectToClipPos(m.vertex); i.uv = m.uv; return i; } float4 FragmentProgram(Interpolarters i) :SV_Target { float3 sourceColor = tex2D(_MainTex, i.uv).rgb; return float4(sourceColor, 1); } ENDCG } 深度雾采样-Depth-Based Fog 增加深度雾。Unity自带深度buffer变量__CameraDepthTexture_, 然后使用指令_SAMPLE_DEPTH_TEXTURE_采样深度： UNITY_DECLARE_DEPTH_TEXTURE(_CameraDepthTexture); float4 FragmentProgram(Interpolarters i) :SV_Target { float depth = SAMPLE_DEPTHE_TEXTURE(_CameraDepthTexture, i.uv); float3 sourceColor = tex2D(_MainTex, i.uv).rgb; return float4(sourceColor, 1); } ！首先。可以使用在UnityCG中定义的Linear01Depth函数将其转换为一个线性范围。这是因为从深度缓冲区得到原始数据后，需要从齐次坐标转换为[0,1]范围的clip-space坐标。我们必须转换这个值，使它成为世界空间中的一个线性深度值。 float depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv); depth = Linear01Depth(depth); Linear01Depth内部实现： // Z buffer to linear 0..1 depth inline float Linear01Depth( float z ) { return 1.0 / (_ZBufferParams.x * z + _ZBufferParams.y); } // Values used to linearize the Z buffer // (http://www.humus.name/temp/Linearize%20depth.txt) // x = 1-far/near // y = far/near // z = x/far // w = y/far float4 _ZBufferParams; ！然后。需要使用far_clip平面距离缩放该depth值，得到真实的深度视距。clip_space裁剪空间可通过float4 ProjectionParams变量获得, 定义在_UnityShaderVariables.cginc中。其中Z分量就是远平面far_clip距离。。 depth = Linear01Depth(depth); float distance = depth * _ProjectionParams.z; ！最后，计算实际的fog。 float viewDistance = depth * _ProjectionParams.z; UNITY_CALC_FOG_FACTOR_RAW(viewDistance); unityFogFactor = saturate(unityFogFactor); float3 sourceColor = tex2D(_MainTex, i.uv).rgb; float3 color = lerp(unity_FogColor.rgb, sourceColor, unityFogFactor); return float4(color, 1); 不太明显的雾 雾混合-Fixing the Fog 对比图15，就像把雾蒙在了物体上方。解决办法就是在绘制物体之前，绘制雾。使用_ImageEffectOpaque_属性绘制 [ImageEffectOpaque] void OnRenderImage (RenderTexture source, RenderTexture destination) { Graphics.Blit(source, destination, fogMate); } 不太明显的雾 处理近平面（非精确处理），near plane存储在Y值中 float viewDistance = depth * _ProjectionParams.z -_ProjectionParams.y; 距离场雾-Distance-Based Fog deferred灯光的着色，从depth-buffer中重建世界空间位置，以便计算灯光。我们也可以这样仿照这样计算雾。 透视相机的clip-space空间定义了一个梯形区域，如果忽略near-plane就得到的是一个以相机world-pos为顶点的三角形区域。它的高是far-plaen距离，那么线性化后的depth范围：顶点为0，底边为1。 金字塔区域 对于渲染的后处理图形Image的每个像素，都能通过从顶点到底边发射一条射线(从屏幕射向3D空间)，检测是否击中任何物体，击中渲染，未击中不渲染。 Image每个像素发射一条射线 若击中某个物体，那么对应像素的深度就要小于1. 如果，该射线在半道击中了物体，射线对应的像素深度值就是1/2.这就意味着 $射线对应的Z值 ={射线击中物体时的长度\\over射线总长度}$。范围[0, 1]。又由于射线的方向都是一致的，X和Y坐标也应该减半。 射线的缩放 一旦得到该射线，就能从相机的位置出发，寻找可能会被渲染的物体表面的世界坐标(若击中)。同时，也要得到该射线的长度。 要使用上述方法，必须知道从相机到平面的每一个像素的射线。但实际上，只需要4条射线，金字塔的每个角都需要一条射线。用插值可给出中间所有像素的光线。 距离检测-Calculating Rays 基于相机远平面和视角计算光线，同时相机的方向和位置与距离无关，所以可以忽略变换。Camera提供了一个函数：CalculateFrustumCorners，四个参数 矩形面积(image rect) 光线投射距离(相机far-plane) 立体渲染(相机自带) 4个元素的3D向量组 deferredCamera.CalculateFrustumCorners ( rectArea, deferredCamera.farClipPlane, deferredCamera.stereoActiveEye, corners ); 下一步传递该数据至Shader，同时也得改变索引顺序。相机提供的是：左下、左上、右上、右下。shader需要：左下、右下、左上、右上 //corners vectex index: b-l, u-l, u-r, b-r //shader vectex index : b-l, b-r, u-l, u-r frustumCorners[0] = corners[0]; frustumCorners[1] = corners[3]; frustumCorners[2] = corners[1]; frustumCorners[3] = corners[2]; fogMate.SetVectorArray(\"__FustumCorners\", frustumCorners); 计算距离-Deriving Distances Shader需要一个接收变量，同时定义一个FOG_DISTANCE宏，当需要使用距离时再计算光线。 #define FOG_DISTANCE struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; #if FOG_DISTANCE float3 ray : TEXCOORD1; #endif }; 根据UV坐标计算获取数组中对应的光线，传进shader的数组排列：(0,0) (1,0) (0,1) (1,1)，使用U+2V可得 #if FOG_DISTANCE i.ray = _FustumCorners[i.uv.x + 2 * i.uv.y]; #endif 最后在Fragment函数替换基于深度计算的雾，使用基于距离计算 float viewDistance = 0; #if defined(FOG_DISTANCE) viewDistance = length(i.ray * depth); #else viewDistance = depth * _ProjectionParams.z - _ProjectionParams.y; #endif 基于距离的雾 standard vs deferrd 天空盒-Fogged Skybox 解放天空盒。两个不同渲染路径渲染的雾会有显著差异。延迟雾也会影响天空盒。它的作用就像far-plane是一个固体屏障，受到雾的影响。当深度值接近1时，表明已经到达了远平面。如果不想给天空盒蒙上雾，可以通过将雾因子设置为1来防止。 if (depth &gt; 0.999) { unityFogFactor = 1; } 基于距离的雾 standard vs deferrd No Fog 最后考虑如何停止渲染雾。解决方案是当没有设置任何雾关键字，通过设置雾因子为1即可。 #if !defined(FOG_LINEAR) || !defined(FOG_EXP) || !defined(FOG_EXP2) unityFogFactor = 1; #endif" }, { "title": "Unity Shader 延迟渲染(翻译十三)", "url": "/posts/Unity_Deferred_Shading/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-10 20:00:00 +0800", "content": "本篇摘要: G-Buffer HDR与LDR Deffered反射 Deferred Rendering Path 到目前为止一直使用了Unity的Forward Render Path，现在开始学习Deferred Path，以及对比这两者间的差异 准备工作 通过Edit/Project Setting/Graphic切换Render Path； 关闭环境光、反射光； Quality设置阴影质量为最高，方便观察； 启用dynamic batching 开始对比Draw Calls 一共有64个Object可见物体组成一个Prefab。 通过对比这个prefab有和没有阴影，分别计算处于ForwardPath和DeferredPath下的Draw Call数。 $\\downarrow Use Forward Path \\downarrow$ 1. No Shadows 没有阴影下，128次几何绘制加1次Clear；1次天空盒绘制；2次屏幕处理绘制，总共132次Draw Call。(如果是使用一个方向光，动态批处理就会生效，就可以少于64个批次绘制)。然而由于有一个额外的方向光，dynamicBatching就不会生效，所以总共绘制两遍。 Forward，no shadow 2. Enable Shadows enable shadow 启用阴影后，需要更多的Draw Calls去生成阴影纹理，分析如下： 首先，填充depth-buffer，需要47次Draw Call，47少于64得益于dynamicBatching； 其次，创建Cascading阴影纹理。第一个光提交了110次Drall Call，同时第二个光提交了115次Drall Call，这些纹理渲染在屏幕空间screen-space buffer，执行过滤。 最后，每个光绘制一次几何物体，用了128=$(64*2)$次DC。 总共408次Draw Calls。 Forward，enable shadow $\\uparrow Use Forward Path \\uparrow$ 上面是分析前向渲染，下面是分析延迟渲染。 $\\downarrow Use Deferred Path \\downarrow$ 1. No Shadows Deferred，no shadows 首先，45次Draw Call渲染GBuffer，这得益于dynamic Batching； 其次，1次Draw Call复制深度纹理； 接着，1次绘制反射和1次自发光反射； 最后，2次光照着色(两个方向光)。 总共52次 = 49次几何绘制； 1次天空盒绘制；2次屏幕处理绘制 2. Enable Shadows 与上图的lighting着色不同，用231次Draw Calls绘制。但是其阴影绘制方法与Forward模式是一样的。 Deffered，enable shadows Deferred不支持MSAA，如果启用Camera组件会有Warning 延迟着色依赖于每个片段存储的数据，这是通过纹理完成的。 这与MSAA不兼容，因为该抗锯 齿技术依赖于子像素数据。 尽管 三角形边缘仍然可以从MSAA中受益，但延迟的数据仍会混叠。 您必须依靠一个后处理过滤器来进行抗锯齿。 $\\uparrow Use Deferred Path \\uparrow$ 分析1.2的对比数据 结论 当渲染多个light光时，Deffered着色模式比Forward着色模式的渲染效率更高！ 相同 两个模式的阴影绘制方法一样 差异 forward模式要求每个光每个物体有一个额外的additive pass；deferred不需要 deferred不必花费额外Draw Calls绘制深度纹理，Copy完成。缘由？ 缘由以及deferred着色解释： 要渲染物体，需要抓取它的Mesh数据；转换到正确的空间；插值传递数据；检索properties数据；计算光照。对于Forward shaders：对要着色的物体的每个像素重复上述步骤；additivePass要比basePass节省，是因为depth-buffer已经提前准备好，同时它不需要关心间接 光；但它任然会重复在basePase已经完成的有大量工作。 重复流程 既然每次计算的几何性质都是一样的，可以让basePass计算后将它们存储在一个缓冲区中。然后，additivePass可以重用数据，消除重复的工作。要存储这些片段的数据，就需要一个适合的缓冲区，就像深度和帧缓冲区一样。 缓存，及部分数据须再次计算 现在，缓冲区中提供了照明所需的所有几何数据。 唯一缺少的是灯光本身。 但这意味着我们不再需要渲染几何体，仅需渲染光就足够了。 此外，basePass只需要填充缓冲区，然后推迟所有直接光照着色计算。这就是延迟着色。 deferred shading deferred下多光源如何使用 当使用一个光源，deferred着色本身没有多大好处。但是当使用很多光源时，每额外增加一个光只会少量增加一点工作，前提是该光源不投射阴影。 因此，当几何物体和光源分开渲染，光的数量对物体的影响是没有限制的，所有的光对它们范围内的物体都是逐像素着色，这个_Pixel Light Count_也就没用了。 many lights, deferred vs. forward 多个实时光源，也可以用bake替代。 光源如何渲染分析 光本身是如何渲染的？ 1、directional方向光，它被渲染成一个面片(Quad)覆盖整个屏幕，使用_Internal-DeferredShading_ shader完成渲染. directional lights use a quad 该shader使用了_UnityDeferredLibrary.cginc_的UnityDeferredCalculateLightParams函数计算光照。 **对于SpotLight、PointLight类似，不同在于它有自身的照明范围。** 2、SpotLight首先要渲染成一个类似金字塔体Mesh。 首先，使用Internal-StencilWrite shader渲染该金字塔Mesh并写入模板缓冲区； 然后，用该缓冲区与稍后将渲染的片元比对，是否要屏蔽体积范围外的片元光照计算。 目的：处于体积范围内的片元将被计算光照、阴影，体积范围外的不需要计算，如果这个金字塔内一个片元被渲染，它会执行光照计算。防止那些不必要的光照计算降低开销。 原因：光线无法到达那里。 注意，当光的体积与相机的近平面相交则该方法失效。 3、PointLight使用类似方法，区别在于它被渲染成球体Mesh。 分析什么是G-Buffers 缓存数据的缺点就是要存储。deferred渲染使用了multiple render texture实现(MRT)，这些纹理就是G-buffers。 deferred要求4个G-Buffers，合并后每个像素总位数：LDR160位，HDR192位。这比起32位大多了，所以对于移动GPU有限制，桌面则没问题。 是哪四个纹理？ 打开frame Debugger或点击Scene/top left下拉菜单选择Deferred选项 Albedo-对应RT0；典型代表diffuse color Specular–对应RT1；典型代表specular + roughness Normal-对应RT2；典型代表normals Emission–对应RT3；典型代表emission + reflection 混合使用Deferred与Forward渲染模型 分析freamDebugger 首先，执行deferred渲染G-buffer，数据存到G-buffer缓冲区。在这渲染期间，forward模式下的物体不参与渲染，不可见。 接着，渲染forward depth，同样存到G-buffer缓冲区。由于是forward所以输出黑色轮廓，覆盖了之前的片元。 开始写Deferred Shader 增加一个Pass，指定光照标签Light Mode = “Deferred”，pass的顺序不重要。 Pass { Tags { \"LightMode\" = \"Deferred\" } } 完整版的着色 Pass { Tags { \"LightMode\" = \"Deferred\" } CGPROGRAM #pragma target 3.0 #pragma exclude_renderers nomrt//（对于不支持MRT设备禁用该pass） #pragma shader_feature _ _RENDERING_CUTOUT #pragma shader_feature _METALLIC_MAP #pragma shader_feature _ _SMOOTHNESS_ALBEDO _SMOOTHNESS_METALLIC #pragma shader_feature _NORMAL_MAP #pragma shader_feature _OCCLUSION_MAP #pragma shader_feature _EMISSION_MAP #pragma shader_feature _DETAIL_MASK #pragma shader_feature _DETAIL_ALBEDO_MAP #pragma shader_feature _DETAIL_NORMAL_MAP #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #define DEFERRED_PASS #include \"MyLighting_SemitransparencyShadow.cginc\" ENDCG } 4个g-buffer输出 要填充4个缓冲区需要为Fragment支持两种输出格式；然后修改Fragment函数返回值。 struct FragmentOutput { #ifdef DEFERRED_PASS float4 gBuffer0 : SV_TARGET0; float4 gBuffer1 : SV_TARGET1; float4 gBuffer2 : SV_TARGET2; float4 gBuffer3 : SV_TARGET3; #else float4 color : SV_TARGET; #endif }; FragmentOutput MyFragmentProgram(Interpolators i){ FragmentOutput output; UNITY_INITIALIZE_OUTPUT(FragmentOutput, output);//不加这句会有warning #ifdef DEFERRED_PASS #else output.color = color; #endif return output; } 计算Buffer 0 该g-Buffer一般用来计算diffuse albedo和surface occlusion，这是ARGB32格式纹理。 diffuse存在RGB通道，occlusion存在A通道。 #if defined(DEFERRED_PASS) output.gBuffer0.rgb = albedo; output.gBuffer0.a = GetOcclusion(i); #else Albedo and occlusion 计算Buffer 1 该g-Buffer一般用来计算specular albedo和smoothness，也是ARGB32格式纹理。 specular存在RGB通道，smoothness值存在A通道。 output.gBuffer1.rgb = specularTint; output.gBuffer1.a = GetSmoothness(i); Specular color and smoothness 计算Buffer 2 该g-Buffer包含了世界空间法线向量，存在RGB通道，是一个ARGB2101010格式纹理。Alpha占2位，RGB各占10位。这意味着该向量的每个标量占10bits而不是之前的8位，也就有更精确。alpha通道不用，默认位1. output.gBuffer2 = float4(i.normal * 0.5 + 0.5, 1); Normals 计算Buffer 3 该g-Buffer被用来计算场景光照，纹理格式依赖于相机设置的HDR或LDR。LDR是ARGB2101010格式。HDR是ARGBHalf格式，每个通道存储16位单精度浮点数，共64位。因此HDR版本比其他buffers大两倍内存。只用到了RGB，Alpha默认为1。 output.gBuffer3 = color; 错误的emission 上面使用的color颜色已计算过阴影，需要使用DEFRRED_PASS重新计算；同时关闭light.ndotl计算；在GetEmission也要使用DEFRRED_PASS标签 output.gBuffer3 = color; UnityLight CreateLight (Interpolators i) { UnityLight light; #if defined(DEFERRED_PASS) light.dir = float3(0, 1, 0); light.color = 0; #else //... #endif ~light.ndotl = DotClamped(i.normal, light.dir);~ return light; } float3 GetEmission (Interpolators i) { #if defined(FORWARD_BASE_PASS) || defined(DEFERRED_PASS) … #else return 0; #endif } 错误的emission Ambient Light four buffers shading 输出四个Buffers后结果看起来较好，但是还不够完整，还缺少环境光。环境光与自发光都没有单独的pass计算，都需要在g-buffers填充后附加到最终颜色值上。环境光是间接光，在间接光函数计算DEFERRED_PASS four buffers shading 光源范围-LDR与HDR分析 对于Internal-DeferredShading shader，Camera组件启用或关闭HDR： 启用HDR，则不执行pass 1，pass 0 fragment返回half4精度，纹理格式ARGBHalf。每个通道存储16位单精度浮点数，总共64位。 关闭HDR后，使用LDR计算策略。在pass 0 fragment会返回fixed精度，使用exp2(x)_[exp2(x)=2-x]_对数编码lightBuffer，获取到更大的颜色范围；pass 1则使用log对数解码buffer数据到主纹理上，纹理格式为ARGB2101010。Alpha占2位，RGB各占10位。 HDR 与 LDR 开启HDR模式下，Deferred与Forward着色效果几乎相似。但是在LDR模式下，Deferred着色就是错误的。 incorrect shading in LDR Mode 在上图描述中，Unity会使用exp2(x)编码LDR数据，所以对自发光和环境光也要使用exp2(x)编码。 incorrect shading in LDR Mode #pragma multi_compile _ UNITY_HDR_ON #if defined(DEFERRED_PASS) #if !defined(UNITY_HDR_ON) color.rgb = exp2(-color.rgb); #endif … #else output.color = color; #endif use exp2(x) and log in LDR Deferred Reflections 展示不同渲染模式下的，探针照射区域过渡对比。下图显示了每个反射可混合多个反射探针。 forward vs. deferred mode 逐像素照射 延迟模式的不同之处在于：探测不会针对每个对象进行混合。而是按像素混合的。这是由internal-deferred shader完成计算。图3右很明显，大地板镜子，延伸到结构之外后，它的渲染范围很小。 Forward模式下： 地板被迫在整个表面使用reflection探头。结果，boxProjection探射到的投影在box外面也显示。也能看见它和其他探针混在一起。图3左。 forward reflection probes Deferred模式下： reflection只渲染box size范围，探射的范围被投射到与boxProjection相交的面积。所以reflection的反射不会超出给定BoxSize范围。实际上，边缘消失的时候也会延伸一点。其他两个探头也是如此。 deferred reflection probes 渲染reflection所用方法，与渲染lights类似： 首先，使用Internal-StencilWrite shader渲染box size大小的Mesh并写入模板缓冲区； 然后，用该缓冲区与稍后将渲染的片元比对，是否要屏蔽体积范围外的片元。 Draw deferred reflections Importance Importance决定各reflection的渲染先后顺序；Intensity决定了反射强度：默认为1，最小为0，大于1过曝。Forward下是整个接收面积；Deferred下是相交面积。下面谈Blend Distance。 Blend Distance Deferred下探针体积边缘有过渡混合，由Blend Distance决定。该值默认为1，只有Deferred模式可调。 多个probes相交时，边缘过渡混合非常有效。也可以用来增大探射体积范围。 blend distance gif Deferred Pass下的反射 虽然deferred很有效，并且每个物体可以混合两个以上的探针。 也有缺点：不能使用锚重写来强制对象使用特定的反射探测。有时这是确保得到正确反射的唯一方法。可以先禁用内置Deferred Refelection， 打开frame Debugger查看G-Buffers RT3，包含了Emission和Refelection Importance 采样黑色探针是浪费。要确保deferred pass只在有需要时采样，用UNITY_ENABLE_REFLECTION_BUFFERS来检查。 UnityIndirect CreateIndirectLight (Interpolators i, float3 viewDir) { #if defined(FORWARD_BASE_PASS) || defined(DEFERRED_PASS) #if defined(DEFERRED_PASS) &amp;&amp; UNITY_ENABLE_REFLECTION_BUFFERS indirectLight.specular = 0; #endif #endif return indirectLight; }" }, { "title": "Unity 透明渲染(翻译十二)", "url": "/posts/Unity_Shader_Transparent_2/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-09 20:00:00 +0800", "content": "本篇摘要: 支持镂空阴影 噪声 粗略的半透明阴影 镂空阴影和半透明阴影之间切换 镂空阴影 翻译11介绍了镂空渲染，可能也注意到了，物体的投射的阴影是物体本身的形状，跟镂空形状完全不一致。这是因为我们之前的Shader投射阴影只是简单的采样了光的方向和到达物体表面的距离，没有区分表面形状。 不符合现实的半透明阴影 重写阴影 为了将透明度考虑在内，需要在阴影投射pass通道访问alpha值。这意味着我们要采样albedo纹理。然而，当仅用opaque渲染模式时就不需要采样，为此需要适配Shader变体。 现在的Shader已经集成了两个变体阴影，一是针对PointLight立方体阴影，二是针对其他类型灯光。现在增加第三个变体。改造一下之前的Shadow.cginc-略 裁剪阴影片段 就像渲染镂空效果时，阴影的镂空也根据alpha值来决定是否丢弃片段。因此算法上大致相似，必要变量也相似。拷贝tint、albedo、renderingsettings。 #include \"UnityCG.cginc\" float4 _Tint; sampler2D _MainTex; float4 _MainTex_ST, _AlphaCutOff; 当使用Cutout渲染模式就需要采样albedo纹理，但是只有当不使用albedo纹理alpha值作为平滑度时才能这样做(之前是把albedo`s alpha作为了平滑值，避免冲突)。如果满足上述条件，就可把uv坐标传递给片元程序。 写一个宏_SHADOW_NEED_UV #if defined(_RENDERING_CUTOUT) &amp;&amp; !defined(_SMOOTHNESS_ALBEDO) #define _SHADOW_NEED_UV 1 #endif struct Interplotars { float4 position : SV_POSITION; #if defined(_SHADOW_NEED_UV) float2 uv : TEXCOORD0; #endif #if defined(SHADOWS_CUBE) float3 lightVec : TEXCOORD1; #endif }; 实现宏_SHADOW_NEED_UV ，需要传递uv坐标时就采样 #if defined(_SHADOW_NEED_UV) i.uv = TRANSFORM_TEX(v.uv, _MainTex); #endif 拷贝GetAlpha函数，把厉害的宏替换为_SHADOW_NEED_UV //采样alpha float GetAlpha(Interpolators i) { float alpha = _Tint.a; #if defined(_SHADOW_NEED_UV) alpha *= tex2D(_MainTex, i.uv.xy).a; #endif return alpha; } 现在可以在fragment程序中获取alpha值，然后在Cutout渲染模式下使用clip裁切。 float alpha = GetAlpha(i); #if defined(_RENDERING_CUTOUT) clip(alpha - _AlphaCutOff); #endif 镂空阴影 局部阴影 为了同时支持Fade和Transparent渲染模式的阴影，必须将其关键字添加到shadowCaster通道的着色器钟并定义ShaderFeature。 #pragma shader_feature _ _RENDERING_CUTOUT _RENDERING_FADE _RENDERING_TRANSPARENT 在Fade和Transparent模式下阴影也应该是半透明的，定义宏_SHADOWS_SEMITRANSPARENT_ #if defined(_RENDERING_FADE) || defined(_RENDERING_TRANSPARENT) #define SHADOWS_SEMITRANSPARENT 1 #endif 再次调整Shadeow_need_uv #if SHADOWS_SEMITRANSPARENT || defined(_RENDERING_CUTOUT) #if !defined(_SMOOTHNESS_ALBEDO) #define SHADOWS_NEED_UV 1 #endif #endif 这个时候由于是使用了Clip，还不能正确投射半透明阴影。继续！ 抖动阴影 阴影贴图包含了光线到表面的距离。光线阻挡结果信息存在阴影贴图，存储的结果是：0或1。因此是没有办法指定光被半透明表面部分阻挡。 目前能做到的就是将阴影表面的一部分剪掉(镂空)。这就是为镂空阴影所做的。但是，除了基于阈值进行裁剪外，我们还可以裁剪片段。例如，如果一个表面让一半的光通过，并通过使用特定抖动纹理裁剪所有其他片段。就可以把生成的阴影显示为完整阴影的一半。 依靠纹理的alpha值，我们可以使用带有更多或更少孔的图案，就不会出现重复的模式。而且，如果我们混合使用这些模式，则可以对阴影的密度进行平滑过渡。基本上，我们仅使用两种状态来近似渐变。这种技术被称为抖动。 Unity包含一个抖动模式图集，我们可以使用。它包含16种4×4像素的不同图案。它从一个完全空的模式开始。每个连续的模式填充一个额外的像素，直到有七个像素被填充。然后模式被反转和反转，直到所有像素都被填充。 VPOS 要对我们的阴影应用抖动模式，我们必须对其进行采样。 不能使用网格的UV坐标，因为它们在阴影空间中不一致。 相反，我们需要使用片段的屏幕空间坐标。 阴影是从光的视角角度渲染贴图的，这会使图案与阴影贴图对齐。 片段的屏幕空间坐标可以在片元程序中访问，方法是添加一个带有VPOS语义的参数。这些坐标不是由顶点程序显式输出的，但GPU可以程序用使用它们。 然而不幸的是，VPOS和SV_POSITION语义，在一些平台上，它们最终可能会映射到相同的语义位置。所以我们不能同时在我们的结构体中使用它们。幸运的是，我们只需要在顶点程序中使用SV_POSITION，而VPOS只需要在fragment程序中使用。所以分别为Vertex和Fragment程序定义一个单独的结构体。 抖动处理 要访问Unity的抖动模式纹理，需要增加变量__DitherMaskLOD_，不同模式存储在3D纹理的不同layer中，必须使用sampler3D声明。 sampler3D _DitherMaskLOD; 如果需要半透明阴影，就在片元程序采样该纹理。通过tex3D采样，参数是__DitherMaskLOD_和一个3D坐标。由于该纹理有16种模式，模式选择由Z坐标决定，Z的范围是0-1。以0.0625增量步进。 #if SHADOWS_SEMITRANSPERANT tex3D(_DitherMaskLOD, float3(i.vpos.xy, 0.0625)); #endif 采样该纹理的目的是取得改纹理的alpha通道，再使用它减去一个较小值，然后用clip裁剪掉 #if SHADOWS_SEMITRANSPERANT float dither = tex3D(_DitherMaskLOD, float3(i.vpos.xy, 0.0625)).a; clip(dither – 0.01); #endif 要真正看到该模式纹理显示，需要调整显示密度大小，这可以通过将纹理的坐标位置乘以0.01来实现的。聚光灯的阴影下观察它。Z值从0-1，纹理将从不渲染到完全渲染。 tex3D(_DitherMaskLOD, float3(i.vpos.xy * 0.1, 0.0625)) Dither抖动纹理 0.0625 vs. 0.9375 近似半透明 根据tex3D采样函数，可知Z坐标值决定是否要渲染片元。那么只需将该片元的alpha值与最高LOD层级(15/16=0.9375)相乘，得出该片元alpha所在的LOD层级。 tex3D(_DitherMaskLOD, float3(i.vpos.xy * 0.1, alpha * 0.9375)) 近似模拟半透 抖动纹理密度大小可以缩放VPOS指定的纹理坐标实现，Unity使用了0.25值来缩放。 tex3D(_DitherMaskLOD, float3(i.vpos.xy * 0.25, alpha * 0.9375)) 缩放后 缩放后效果还行但不完美，这取决于抖动纹理的分辨率。越高的分辨率填充的越密集，效果越好。同时，抖动纹理在Hard和Soft阴影模式下的效果也不相同 soft vs. hard 这个阴影模式在物体移动时有一个视觉错误，整个阴影都在动。密集恐惧。 抖动 组合剪影与淡出效果 可能Dither模式太难堪了，我们可能既想要投射阴影，也要半透明的效果。这就可以组合Cutout与Fade渲染。在阴影pass中增加一个新的关键字： _SEMITRANSPARENT_SHADOWS。若没有启用淡出半透阴影，回退到裁剪阴影。 #pragma shader_feature _ _SEMITRANSPARENT_SHADOWS #if defined(_RENDERING_FADE) || defined(_RENDERING_TRANSPARENT) //#define SHADOWS_SEMITRANSPARENT 1 #if _SEMITRANSPARENT_SHADOWS #define SHADOWS_SEMITRANSPARENT 1 #elif #define _RENDERING_CUTOUT #endif #endif 上面这段代码加入后，会自动切换为裁剪阴影。 3.2 合并显示 void DoSimetransparentShadow(RenderMode mode) { if (mode == RenderMode.Fade || mode == RenderMode.Transparent) { EditorGUI.BeginChangeCheck(); bool enable = EditorGUILayout.Toggle ( new GUIContent(\"Semitransparent Shadow\"), IsKeyEnable(\"_SEMITRANSPARENT_SHADOWS\") ); if(!enable) { isShowCutoffAlpha = true; } if (EditorGUI.EndChangeCheck()) { SetKeyword(\"_SEMITRANSPARENT_SHADOWS\", enable); } } }" }, { "title": "Unity 透明渲染(翻译十一)", "url": "/posts/Unity_Shader_Transparent_1/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-08 20:00:00 +0800", "content": "本篇摘要: 剪纸镂空shader 渲染队列 半透明材质 合并反射和透明 Cutout-镂空渲染 要创建透明的材质，首先要了解每个片元的透明度。透明度信息存储在颜色的alpha通道，在我们的shader里是主纹理的alpha通道和调色值的alpha通道。 确定Alpha值 获取alpha值 //采样alpha float GetAlpha(Interpolators i){ return _Tint.a * tex2D(_MainTex, i.uv.xy).a; } 但是前面讲了SMOOTHNESS_ALBEDO值可能被用来确定平滑度，所以要在该值不被使用时才能采样该纹理的alpha值，避免错误 //采样alpha float GetAlpha(Interpolators i){ float alpha = _Tint.a; #if !defined(_SMOOTHNESS_ALBEDO) alpha *= tex2D(_MainTex, i.uv.xy).a; #endif return alpha; } 剪切镂空 在不透明的材质，每个通过深度测试的片元都会被渲染，所有的片元都是不透明的并且写入深度缓冲。所以实现透明最简单快捷的方法是：在深度测试时，要么它完全不透明，要么它完全透明。如果它是透明就不渲染。 CG提供了一个Clip函数，若参数&lt;0，那么该像素所在的片元整个就会被丢弃(粗暴处理)，同时不会写入深度缓冲。可以将得到的Alpha减去一个给定值(视效果)来决定是否渲染。 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float alpha = GetAlpha(i); clip(alpha - 0.5); //... } Transparency 自定义裁剪范围 给定0.5不太灵活，我们需要一个任意值来代替。 Properties { _AlphaCutoff (\"Alpha Cutoff\", Range(0, 1)) = 0.5 } float _AlphaCutoff; float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float alpha = GetAlpha(i); clip(alpha - _AlphaCutoff); //... } DX11汇编 0: sample r0.xyzw, v1.xyxx, t0.xyzw, s2//采样 1: mul r0.xyz, r0.xyzx, cb0[6].xyzx//r0 * cb0[6] 2: mad r0.w, cb0[6].w, r0.w, -cb0[8].y//cb0[6] * r0 + (-cb0[8]) 3: lt r0.w, r0.w, l(0.000000)//浮点比较：r0.w &lt; 0 ? 0xFFFFFFFF : 0x0000000 4: discard_nz r0.w 渲染模式 Clip性能问题：桌面GPU尚可，但是对移动GPU来讲性能就很糟糕。而且对于不透明物体不需要执行该函数。增加预定义的关键字 void SetRenderMode() { RenderMode mode = RenderMode.Opaque; if (IsKeyEnable(\"_RENDER_CUTOUT\")) { mode = RenderMode.Cutout; } EditorGUI.BeginChangeCheck(); GUIContent gc = new GUIContent(\"RenderMode\"); mode = (RenderMode)EditorGUILayout.EnumPopup(gc, mode); if (EditorGUI.EndChangeCheck()) { SetKeyword(\"_RENDER_CUTOUT\", mode == RenderMode.Cutout); } } float alpha = GetAlpha(i); #if defined(_RENDERING_CUTOUT) clip(alpha - _AlphaCutoff); #endif 渲染队列 UnityShader提供了队列Queue标签 //Determine in which order objects are renderered. public enum RenderQueue { //This render queue is rendered before any others. Background = 1000, //Opaque geometry uses this queue. Geometry = 2000, //Alpha tested geometry uses this queue. AlphaTest = 2450, //Last render queue that is considered \"opaque\". GeometryLast = 2500, //This render queue is rendered after Geometry and AlphaTest, in back-to-front //order. Transparent = 3000, //This render queue is meant for overlay effects. Overlay = 4000 } 我们可以用自定义UI来决定渲染队列 if (EditorGUI.EndChangeCheck()) { SetKeyword(\"_RENDER_CUTOUT\", mode == RenderMode.Cutout); RenderQueue queue = mode == RenderMode.Opaque? RenderQueue.Geometry:RenderQueue.AlphaTest; targetMaterial.renderQueue = (int)queue; } 渲染类型标签 标签在用摄像机的replacement shaders时非常有用，可以自定义渲染效果。 string renderType = mode == RenderMode.Opaque ? \"\" : \"TransparentCutout\"; targetMaterial.SetOverrideTag(\"RenderType\", renderType); 半透明-淡入淡出渲染 - Cutout渲染局限性： 是对每一个像素计算，不满足alpha值的片元整个会被丢弃 是渲染结果要么全不透明要么全透明 是在不透明和透明区间没有过渡，硬边严重 是镂空边缘锯齿严重 为了解决上述问题，Unity自带的StandardShader使用了一种Fade模式。增加关键字__RENDERING_FADE_略。 渲染设置 Fade模式有它自己的渲染队列和渲染类型。Queue始值是3000，标识透明物体。RenderType是Transparent. 替换SetRenderMode函数，定义一个通用的渲染设置 class RenderingSettings { public RenderQueue Queue; public string RenderType; public static RenderingSettings[] modes = { new RenderingSettings(){Queue = RenderQueue.Geometry, RenderType = \"\"}, new RenderingSettings() { Queue = RenderQueue.AlphaTest, RenderType = \"TransparentCutout\"}, new RenderingSettings() { Queue = RenderQueue.Transparent, RenderType = \"Transparent\"} }; } void SetRenderMode() { RenderMode mode = RenderMode.Opaque; if (IsKeyEnable(\"_RENDERING_CUTOUT\")) { mode = RenderMode.Cutout; AlphaCutOffShow(); }else if (IsKeyEnable(\"_RENDERING_FADE\")) { mode = RenderMode.Fade; } EditorGUI.BeginChangeCheck(); GUIContent gc = new GUIContent(\"RenderMode\"); mode = (RenderMode)EditorGUILayout.EnumPopup(gc, mode); if (EditorGUI.EndChangeCheck()) { SetKeyword(\"_RENDERING_CUTOUT\", mode == RenderMode.Cutout); SetKeyword(\"_RENDERING_FADE\", mode == RenderMode.Fade); //RenderQueue queue = mode == RenderMode.Opaque? RenderQueue.Geometry:RenderQueue.AlphaTest; //string renderType = mode == RenderMode.Opaque ? \"\" : \"TransparentCutout\"; RenderingSettings settings = RenderingSettings.modes[(int)mode]; targetMaterial.renderQueue = (int)settings.Queue; targetMaterial.SetOverrideTag(\"RenderType\", settings.RenderType); } } 渲染透明物体 打开FrameDebugger观察，选择Fade模式，然后对比前后的变化： 当使用Opaque或Cutout模式，使用该材质的对象通过Render.OpaqueGeometry渲染。 当使用Fade模式，对象通过Render.TransparentGeometry渲染。 Opaque vs Transparent 注意右侧图片，也调用了Render.OpaqueGeometry方法。注意这个调用先后顺序，Opaque和Cutout先被渲染，Transparent后渲染。因此该渲染顺序，确保了半透明物体不会在它们之后渲染。 Blending Fragments-融合 公式：Blend src dst 举例：源：透明度为a，颜色值Ac；目标颜色为Bc. 混合后颜色 \\(Ac*a + (1-a)*Bc\\) 实现Fade渲染模式。用两个关键字在base和additive pass支持三种渲染模式。 #pragma shader_feature _ _RENDERING_CUTOUT _RENDERING_FADE 在Fade模式下，我们需要对当前片元的颜色与colorBuffer已存在的颜色融合。这一步虽在GPU完成，但需要现在Fragment程序预先计算提供。 为了创建一个透明效果，我们需要使用不同于Opaque和Cutout的融合模式。例如在additive Pass，把新颜色加到现有的颜色。然而不能简单的相加在一起，这个多通道融合需要取决于alpha值： 当alpha为1，物体将会渲染成完全不透明。对于上述举例，通常在basePass使用Blend One Zero，在additivePass使用Blend One One； 当alpha为0，物体将会渲染成完全透明。这两个pass的融合只能使用Blend Zero One； 当alpha为0.25，使用Blend 0.25 0.75 和 Blend 0.25 One; 以此类推… 为了达成上述效果，可以使用SrcAlpha和OneMinusSrcAlpha关键字 半透明效果 这两个关键字只是对Fade渲染半透明效果的近似估计，我们也可以自定义属性变量代替。 _SrcBlend(\"SrcBlend\", float) = 1 _DstBlend(\"DstBlend\", float) = 0 由于这两个属性依赖渲染模式，因此不能直接显示在Inspector面板。 [HideInInspector] _SrcBlend(\"SrcBlend\", float) = 1 [HideInInspector] _DstBlend(\"DstBlend\", float) = 0 使用自定义的属性，必须放入中括号，这是旧Shader语法要求。 Blend [_SrcBlend] [_DstBlend] 为了能控制这些变量，在RenderingSettings增加两个变量 public static RenderingSettings[] modes = { new RenderingSettings() { queue = RenderQueue.Geometry, renderType = \"\", srcBlend = BlendMode.One, dstBlend = BlendMode.Zero }, new RenderingSettings() { queue = RenderQueue.AlphaTest, renderType = \"TransparentCutout\", srcBlend = BlendMode.One, dstBlend = BlendMode.Zero }, new RenderingSettings() { queue = RenderQueue.Transparent, renderType = \"Transparent\", srcBlend = BlendMode.SrcAlpha, dstBlend = BlendMode.OneMinusSrcAlpha } }; 通过Material.SetInt函数直接为__SrcBlend_ 和 __DstBlend_指定值 targetMaterial.SetInt(\"_SrcBlend\", (int)settings.SrcBlend); targetMaterial.SetInt(\"_DstBlend\", (int)settings.DstBlend); 深度冲突问题 把两个半透明物体一高一低放置在一起(不是重叠但很接近)，互相重合区域会出现一个遮挡问题。不能透过透明区域看见另一个物体。 奇怪现象 Unity首先试图绘制距离摄像机最近的物体，这是对重叠几何绘制最有效的方法。不幸的是它对半透明物体失效了。解决方法就是反过来绘制，最远的物体先绘制写入深度缓冲，最近的物体后绘制与深度缓冲比较。这也是绘制半透明物体代价更昂贵的原因。 为了确定几何图形的绘制顺序，Unity使用了物体的中心位置。这对于相距很远的小物体来说很有效。但是对于较大物体，或者是紧靠在一起的平面物体，它就不好使了。在这种情况下，当你改变视角时，绘制顺序会突然变化。这可能会导致重叠的半透明物体外观的突然变 化。 无法绕过此限制，尤其是在绘制相交几何时。 但是在我们的例子中，某些绘制顺序产生了明显错误结果。 发生这种错误是因为我们的着色器仍会写入深度缓冲区。 深度缓冲区是二进制数，并不关心透明度。 如果片段没有被裁剪，其深度最终将写入缓冲区。 由于半透明对象的绘制顺序并不完美，若开启深度写入，不可见物体的深度值最终可能会覆盖可见的物体的深度值。 因此在使用Fade渲染模式时，我们必须禁用对深度缓冲区的写入。 ZWrite 控制 类似融合自定义属性，ZWrite模式也可自定义 [HideInInspector] _SrcBlend (\"_SrcBlend\", Float) = 1 [HideInInspector] _DstBlend (\"_DstBlend\", Float) = 0 [HideInInspector] _ZWrite (\"_ZWrite\", Float) = 1 Blend [_SrcBlend] [_DstBlend] ZWrite [_ZWrite] GUI拓展略 重合正确 全透明渲染-逼真的材质 Fade缺点： 我们创建的半透明渲染模式会根据物体alpha值淡出显示。 请注意，物体颜色的显示呈褪色。 它的漫反射和镜面反射以及高光都会被淡化了。 这就是为什么它被称为淡入淡出模式。 Fade模式适用于大多数效果，但它不能正确渲染一个半透明的固体物体。例如玻璃。 Transparent优点： 玻璃材质实际上是完全透明的，并且也有清晰的高光和反射光，反射光也会加入到其他任何经过的光的路径钟参与着色。 红色过渡趋势 enum RenderingMode { Opaque, Cutout, Fade, Transparent } 设置Transparent模式与Fade相同，不管Alpha值是多少我们必须添加反射。因此源混合模式必须为1而不能依赖Alpha值。然后增加关键字_RENDERING_TRANSPARENT. 预乘Alpha 预乘Alpha：纹理的RGB通道分别预乘Alpha得到新的颜色，然后就可以不需要Alpha通道。 优点： 它们可存储与RGB通道相关联的不同Alpha值，这样可以使用相同数据来实现多种组合效果。 少计算一次乘法：混合后颜色 \\(Ac*a + (1-a)*Bc =&gt; Ac^` + (1-a)*Bc\\).提交效率 在有透明像素的边缘进行正确插值：如果RGB不预乘Alpha，插值时的权重过大，边缘会产生奇怪的颜色。例如出现黑边问题 缺点: RGB预乘后，RGB每个通道值会变小，颜色变暗 纹理精度下降。每个通道都乘以了alpha因子 不能直观知道原RGB颜色值 调节Alpha 如果一个物体既透明又反光，我们就能看到它背后的东西，也能看到反射回来的东西。这对物体的两面观察都成立。但是同样的光既不能被反射也不能穿过物体。这又是一个能量守恒的问题。所以一个物体的反射性越强，穿过它的光就越少，不管它固有的透明度如何。 表面没有反射，它的alpha值是0。当反射所有的光时，它的alpha实际上变成了1。当我们在Fragment Program中确定反射率时，我们可以使用它来调整alpha值。给定初始alpha和反射率： 调节\\(alpha = 1 – (1 - α)(1 - r)\\), \\((1 – r)\\)在Shader定义为oneMinusReflectivity 调节\\(alpha = 1 – oneMinusReflectivity + α * oneMinusReflectivity\\) #if defined(_RENDERING_TRANSPARENT) albedo *= alpha; alpha = 1 - oneMinusReflectivity + alpha * oneMinusReflectivity; #endif 效果变得更暗了" }, { "title": "Unity Shader GUI 扩展二(翻译十)", "url": "/posts/Unity_ShaderGUI_Extension_2/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-07 22:00:00 +0800", "content": "本篇摘要: 把自身阴影烘焙进材质 增加细节纹理部分 支持更丰富的shader变体 一次编辑多个材质球 遮挡区域的Self-Shading 美术能够创作非常复杂丰富的表面纹理，它只是一个视错觉。为了增强表面纹理视觉真实感，引入Self-Shading。 如何增强呢？通常我们使用了法线来增强模型表面的凹凸层次感，法线带来的视觉增强是第一步，但是法线只适用于采样直接光照下。现在开始第二步增强，给凹凸表面引入阴影：凸向凹投射阴影 Occlusion Map 使用遮挡纹理增加self-shadings，也是灰度图，凹趋近黑色0。在Properties声明和拓展gui: [NoScaleOffset]_OcclusionMap(\"OcclusionMap\", 2D) = \"white\"{} _Occlusion(\"Occlusion\", Range(0,1)) = 0 #pragma shader_feature _ _OCCLUSION_MAP 这两种是等效的 shader_feature _ _OCCLUSION_MAP _shader_feature _OCCLUSION_MAP //但是这两种不等效!!! #pragma shader_feature _SMOOTHNESS_ALBEDO _SMOOTHNESS_METALLIC #pragma shader_feature _ _SMOOTHNESS_ALBEDO _SMOOTHNESS_METALLIC 总结一下shader_feature，只有一个关键字默认生成&lt;no keywords defined&gt;，若有多个关键字第一个关键字会替代&lt;no keywords defined&gt;，在使用手动收集ShaderVariantCollection要多加注意。而multi_compile有单个关键字时必须加_。 Occlusion GUI_Extension void OcclusionShow() { EditorGUI.BeginChangeCheck(); MaterialProperty mp = MakerMapWithScaleShow(\"_OcclusionMap\", \"_Occlusion\", false, \"遮挡纹理\"); if (EditorGUI.EndChangeCheck()) { SetKeyword(\"_OCCLUSION_MAP\", mp.textureValue); } } 直接光阴影 创建采样函数 float GetOcclusion(Interpolators i) { #ifdef _OCCLUSION_MAP return tex2D(_OcclusionMap, i.uv).g; #endif return 1; } 但是由于阴影的强度有可调需求，需要动态改变。结合_OcclusionStrength做线性插值 float GetOcclusion(Interpolators i) { #ifdef _OCCLUSION_MAP float g = tex2D(_OcclusionMap, i.uv).g return lerp(1, g, _OcclusionStrength); #endif return 1; } 然后将采样得到的值作用于光照颜色内，包括直接光和间接光，这里是直接光 UnityLight CreateLight(Interpolators i) { //。。。 UNITY_LIGHT_ATTENUATION(attenuation, i, i.worldPos); attenuation *= GetOcclusion(i); light.color = _LightColor0.rgb * attenuation; light.ndotl = DotClamped(i.normal, light.dir); return light; } without and with occlusion map 上图带有遮挡纹理的凹陷阴影过渡，gif显示： 阴影过渡 间接光阴影 直接光采样下凹陷越深阴影越重，但不是那么明显。这是因为除了直接光以外还有间接光，幸好OcclusionMap并不是针对特定光线的纹理，现在来给他增加间接光采样，然后看看效果如何。 UnityIndirect CreateIndirectLight(Interpolators i, float3 viewDir) { #if defined(VERTEXLIGHT_ON) //... float occlusion = GetOcclusion(i); indirectLight.diffuse *= occlusion; indirectLight.specular *= occlusion; #endif return indirectLight; } wihtout and with occlusion 把图1.2与图1.1对比，可以明显感觉到Occlusion纹理好似专门针对间接光而制作，它随着凹陷越深阴影越明显，甚至有点过头了。那么我们何不把直接光采样这步去掉，看看它的效果如何。 UnityLight CreateLight(Interpolators i) { //。。。 UNITY_LIGHT_ATTENUATION(attenuation, i, i.worldPos); ~attenuation *= GetOcclusion(i);~ light.color = _LightColor0.rgb * attenuation; light.ndotl = DotClamped(i.normal, light.dir); return light; } without and with occlusion 把上图逐一对比，感觉最后一图的效果适中，看起来舒服。就Occlusion而言，它具有相当大真实感。虽然如此，我们通常会发现游戏里的遮挡图也被用在直接光上。Unity老旧的shader就是这样做的，虽然它不太真实，但是对灯光效果的控制提供了相当大的灵活性。 SSAO：screen-space-ambient-occlusion:它是屏幕后处理效果，使用深度缓冲来动态创建整个帧的遮挡映射。它被用来增强屏幕的深度感，因为它是后处理效果，它在所有的灯光渲染之后被使用。这意味着那个阴影即使用了间接光也使用了直接光。因此它也是不真实的。 合并纹理 我们只用了遮挡纹理的G通道，而metallic金属纹理是存储在R通道，SmoothNess纹理存储在alpha通道。这意味着我们可以把三个纹理合并为一个纹理。 合并后的纹理 优势 单一纹理降低了内存和存储压力； 弊端 这个Shader中它会采样两次；(可以手动优化为从同一纹理采样)； 使用DXT5压缩后，纹理大小变小了但是它的质量也降低了。所幸这些纹理不要求太高的细节和精度。 细节纹理 增加细节纹理和法线，把细节纹理和法线设置为fade-out mipmap。 细节纹理效果 细节遮罩纹理 根据图2.1效果，细节纹理覆盖整个表面后，看起来的效果不是太好。最好的效果是它不覆盖金属区域部分。所以，我们可以用细节遮罩纹理来控制这部分显示，这就好像蒙版测试。不同之处在于0表示没有细节，1表示完整的细节。 Unity的Standard Shader使用了遮罩纹理的alpha通道，这张纹理四个通道存储了同样的值。 Albedo Details 调整Albedo纹理采样，必须基于detal mask纹理的采样值，在未修改和修改后的albedo之间插值。 float3 GetAlbedo (Interpolators i) { float3 albedo = tex2D(_MainTex, i.uv.xy).rgb * _Tint.rgb; float3 details = tex2D(_DetailTex, i.uv.zw) * unity_ColorSpaceDouble; albedo = lerp(albedo, albedo * details, GetDetailMask(i)); return albedo; } float4 MyFragmentProgram (Interpolators i) : SV_TARGET { // float3 albedo = tex2D(_MainTex, i.uv.xy).rgb * _Tint.rgb; // albedo *= tex2D(_DetailTex, i.uv.zw) * unity_ColorSpaceDouble; float3 specularTint; float oneMinusReflectivity; float3 albedo = DiffuseAndSpecularFromMetallic( GetAlbedo(i), GetMetallic(i), specularTint, oneMinusReflectivity ); } Normal Details 对于法线，同样需要相同的调整。但是，这里的细节不符合与未修改的切线空间法向量相对应。因为原作者给的这张法线纹理不是切线空间.需要手动匹配一次。 void InitializeFragmentNormal(inout Interpolators i) { float3 mainNormal = UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale); float3 detailNormal = UnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale); detailNormal = lerp(float3(0, 0, 1), detailNormal, GetDetailMask(i)); float3 tangentSpaceNormal = BlendNormals(mainNormal, detailNormal); } Mask Details" }, { "title": "Unity Shader GUI 扩展一(翻译九)", "url": "/posts/Unity_ShaderGUI_Extension_1/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-07 20:00:00 +0800", "content": "本篇摘要: 自定义Shader GUI面板拓展 混合金属与非金属效果 非均匀平滑 表面自发光 自定义界面 自定义Shader界面功能，与自定义GUI面板类似，区别在于重实现函数不同。 myLightingShader vs standard ShaderGUI 拓展 创建一脚本继承UnityEditor.ShaderGUI，脚本放入Editor文件夹下 using UnityEngine; namespace GUIExtension { public class MyCustomShaderGUI : UnityEditor.ShaderGUI { } } 同时在需要修改面板界面的Shader文件内，引用该类文件。注意命名空间也要带上 Shader \"Custom/Custom/Shader_GUIExtension\" { SubShader{} SubShader{} SubShader{} //只能放在所有SubShader最后调用 CustomEditor \"GUIExtension.MyCustomShaderGUI\" } 同时再次查看ShaderGUI有哪些虚函数可供重写。 using UnityEngine; namespace UnityEditor { public abstract class ShaderGUI { protected ShaderGUI(); // 参数: // propertyName:Name of the material property. // properties: OnGUI函数引用地址传递. // 返回结果: 没找到返回null. protected static MaterialProperty FindProperty(string propertyName, MaterialProperty[] properties); // 参数: // propertyName:Name of the material property. // properties:The array of available properties. // propertyIsMandatory:值true且没有找到对应property就抛出异常 // 返回结果:同上 protected static MaterialProperty FindProperty(string propertyName, MaterialProperty[] properties, bool propertyIsMandatory); // 摘要:给这个材质选一个新shader时的回调 public virtual void AssignNewShaderToMaterial(Material material, Shader oldShader, Shader newShader); // 参数: // materialEditor:当前材质面板 // properties:当前选中的shader所有properties. public virtual void OnGUI(MaterialEditor materialEditor, MaterialProperty[] properties); public virtual void OnMaterialInteractivePreviewGUI(MaterialEditor materialEditor, Rect r, GUIStyle background); //预览 public virtual void OnMaterialPreviewGUI(MaterialEditor materialEditor, Rect r, GUIStyle background); //预览 public virtual void OnMaterialPreviewSettingsGUI(MaterialEditor materialEditor); } } 第一个重实现函数就是：OnGUI 创建文本 创建文本与GUI拓展创建文本类似，统一使用GUILayout public override void OnGUI(MaterialEditor materialEditor, MaterialProperty[] properties) { DoMain(); } void DoMain() { GUILayout.Label(\"Main Map\"); } label 演示 GUILayout.Label有多个重载函数，可以为文字指定各种颜色、字体、格式等效果。 文字效果 显示Albedo纹理属性 选中当前材质后，若材质使用的Shader调用了GUI拓展，则会自动读取该Shader的所有属性。通过重实现OnGUI函数后，获取其参数地址就能读取。 MaterialEditor MaterialEditor; MaterialProperty[] materialProperties; //MaterialEditor是面板实例 //该shader所有properties public override void OnGUI(MaterialEditor materialEditor, MaterialProperty[] properties) { this.MaterialEditor = materialEditor; this.materialProperties = properties; } 查找Albedo属性，并将其显示出来。Albedo是一个Texture纹理属性，对应一张纹理和名称描述，可使用FindProperty和GUIContent容器 void AlbedoPropertyShow() { MaterialProperty albedo = FindProperty(\"_MainTex\", materialProperties, true); //displayName是shader内手写好的名字 GUIContent content = new GUIContent(albedo.displayName, albedo.textureValue, \"this is a main texture\"); MaterialEditor.TexturePropertySingleLine(content, albedo); } Albedo纹理，tooltip 然后给该纹理增加色调Tint显示 void AlbedoPropertyShow() { MaterialProperty albedo = FindProperty(\"_MainTex\", MaterialProperties, true); MaterialProperty tint = FindProperty(\"_Tint\", MaterialProperties, true); //displayName是shader内手写好的名字 GUIContent content = new GUIContent(albedo.displayName, albedo.textureValue, \"this is a main texture\"); //重载函数 MaterialEditor.TexturePropertySingleLine(content, albedo, tint); } 色调和偏移 显示Normal纹理属性 同理，有一个点在于bumpScale属性显示，假如没有指定纹理时就不想显示BumbScale属性。 void NormalShow() { MaterialProperty normal = FindProperty(\"_NormalMap\", MaterialProperties, true); //没有纹理时不想显示bumpscale MaterialProperty bumpScale = null; if(normal.textureValue != null) bumpScale = FindProperty(\"_BumpScale\", MaterialProperties, true); MaterialEditor.TexturePropertySingleLine(MakeGUIContent(normal), normal, bumpScale); } 隐藏属性 显示金属和平滑值属性 这两个值主要作用于MainTex纹理，应该放在其之后位置显示。 void SpecialShaderPropertyShow(string propertyName,string tooltip = null) { MaterialProperty metallic = FindProperty(propertyName, MaterialProperties, true); MaterialEditor.ShaderProperty(metallic, MakeLabelGUIContent(metallic, tooltip)); } void MetallicShow() { SpecialShaderPropertyShow(\"_Metallic\"); } void SmoothnessShow() { SpecialShaderPropertyShow(\"_Smoothness\"); } property show //必须包围使用， 先缩进后恢复，不然会影响后面的显示 EditorGUI.indentLevel += 2; MaterialEditor.ShaderProperty(metallic, MakeLabelGUIContent(metallic, tooltip)); EditorGUI.indentLevel -= 2; 缩进 显示第二细节纹理 同理，不贴代码了。 secondary map show 金属纹理 如何混合金属与非金属纹理，两个不同光泽的纹理如何混合？ 没有金属纹理，质感稍显油腻 使用Metallic 纹理 一般可以用灰度图标记金属色、凹凸(视差)色。金属标记为1白色，非金属向0趋近黑色。因此采样灰度图、高度图alpha与diffuse混合。接着扩展GUI-略 //... [NoScaleOffset]_MetallicMap(\"MetallicMap\", 2D) = \"white\"{} float GetMetallic(Interpolators i) { return tex2D(_MetallicMap, i.uv.xy).r * _Metallic; } 油腻感降低 定义Metallic的Shader关键字 当使用metallicMap时就不能再使用metallicValue滑条，如不就会导致双倍叠加。所以可以用Shader关键字来决定使用二者之一。MaterialEditor.target是当前inspector面板material实例，增加关键字可以使用Material.EnableKeyword,禁用Material.DisableKeyword。 void SetKeyword(string keyword, bool enable) { if(enable) { targetMaterial.EnableKeyword(keyword); } else { targetMaterial.DisableKeyword(keyword); } } 自定义命名约定:_XXX_XX_...。而#pragma multi_compile指令会自动纳入已定义的关键字生成shader变体。 2.5 打开Debug模式查看 enable keyword 使用自定义Keyword(Features) #pragma multi_compile _ _MATALLIC_MAP 使用multi_compile指令然后分别查看变体编译： 有没有使用Matallic贴图都会编译出如下排列组合 8 keyword variants used in scene: &lt;no keywords defined&gt; _MATALLIC_MAP VERTEXLIGHT_ON VERTEXLIGHT_ON _MATALLIC_MAP SHADOWS_SCREEN SHADOWS_SCREEN _MATALLIC_MAP SHADOWS_SCREEN VERTEXLIGHT_ON SHADOWS_SCREEN VERTEXLIGHT_ON _MATALLIC_MAP 可是，multi_compile指令会生成所有可能的排列组合变体，这些要花费大量时间编译，而且有些keywords确实没使用。对于自定义的shader keywords可以使用shader_feature编译指令优化哪些没有使用的关键字，不生成变体，同时在构建时也会检查关键字是否被使用。 shader_feature没有使用Matallic贴图，变体数量降低： 4 keyword variants used in scene: &lt;no keywords defined&gt; VERTEXLIGHT_ON SHADOWS_SCREEN SHADOWS_SCREEN VERTEXLIGHT_ON 那么multi_compile与shader_feature何时使用？ 1、对于shader_feature最佳食用方法就是在编辑器模式，人工配置material面板属性自动收集变体。 2、如果要在runtime使用shader_feature，就要确保所有的shader变体都被构建进应用内。当然这也是很完美的方案，但是一定要确保能够手动收集所有变体 3、对于第二点的稍次解决方案就是使用muti_compile指令 根据关键字启用，自动获取 float GetMetallic(Interpolators i) { #if defined(_METALLIC_MAP) return tex2D(_MetallicMap, i.uv.xy).r; #else return _Metallic; #endif } ChangeCheck检查 现在每帧调用OnGUI，会重复执行所有方法。理论上只有当material面板属性被改变了，调用执行内部方法。Unity提供了EditorGUI.BeginChaneCheck和EditorGUI.EndChangeCheck方法。这两个方法需要匹配使用，begin在要检查之前的位置调用，end在结束时检查：若有修改返回true void MetallicMapShow() { EditorGUI.BeginChangeCheck(); MaterialProperty mp = MakerMapWithScaleShow(\"_MetallicMap\", \"_Metallic\", true); if(EditorGUI.EndChangeCheck()) { SetKeyword(\"_METALLIC_MAP\", mp.textureValue); } } 光滑纹理 类似金属纹理，光滑纹理也是一张灰度图。金属部分很光滑，其他部分很粗糙。Unity提供的standardShader是采样了纹理的alpha通道，而事实上它要求金属和光滑值合并在同一张金属贴图的不同通道(这也给了一个很好的工作流提示)。好处：一是不用采样两次；二是DXT5压缩会分离RGB和A通道。当然这在两者都需要之下。 float GetSmoothness(Interpolators i) { #if defined(_METALLIC_MAP) return tex2D(_MetallicMap, i.uv.xy).a  * _Smoothness; #else return _Smoothness; #endif } 金属_光滑纹理 效果缺点：DXT5nm纹理压缩法线贴图会造成伪影。尖锐的对角边没有与UV轴对齐，不能正确地近似。电路中是这种压缩最糟糕的情况。这种缺点在金属和非常光滑的表面上变得清晰可见。 边缘伪影 Albedo与smoothness结合 第一个工作流：对于金属质感材质总是需要Metallic，同时也肯定需要smoothness增强平滑感。 第二个工作流：不要金属质感而要平滑，可以把smoothness放进Albedo纹理alpha通道。这种情况适用不需要金属的不透明材质。 关键字选择 对于多个工作流，能提供一个下拉列表项匹配就很方便。 enum SmoothnessSource { Uniform, Albedo, Metallic } 当使用Uniform代表没有关键字写入。当Albedo代表包含光滑度的albedo纹理；当metallic代表包含光滑度的metallic纹理。Material实例提供了IsKeywordEnabled函数检测关键字启用。 void SmoothnessShow() { Switchkeyword source = Switchkeyword.UNIFORM; if (IsKeyEnable(keyword_smoothness_albedo)) { source = Switchkeyword.SMOOTHNESS; } if(IsKeyEnable(keyword_smoothness_metallic)) { source = Switchkeyword.METALLIC; } //必须包围使用， 先缩进后恢复，不会影响后面的显示 EditorGUI.indentLevel += 2; MakeShaderSpecialPropertyShow(\"_Smoothness\"); EditorGUI.indentLevel += 1; GUIContent gc = new GUIContent(\"Source\"); EditorGUI.BeginChangeCheck(); EditorGUI.indentLevel += 1; GUIContent gc = new GUIContent(\"Source\"); //在这里开始检查是否手动修改了下拉单元，然后设置对应的关键字 EditorGUI.BeginChangeCheck(); source = (Switchkeyword)EditorGUILayout.EnumPopup(gc, source); if (EditorGUI.EndChangeCheck()) { //RecordAction(\"123124\");//取消 SetKeyword(keyword_smoothness_metallic, source == Switchkeyword.METALLIC); SetKeyword(keyword_smoothness_albedo, source == Switchkeyword.SMOOTHNESS); } EditorGUI.indentLevel -= 3; } Unity提供了MaterialEditor.RegisterPropertyChangeUndo函数取消 Smoothness变体使用 #pragma shader_feature _ _SMOOTHNESS_ALBEDO _SMOOTHNESS_METALLIC 先检查是否使用albedo关键字作为平滑，然后检查smoothness和metallic作为平滑(当开启金属肯定会有平滑需求)。然后与_Smoothness叠加。叠加：1不变，拉动滑条能二次调节。叠加这是一种效果算法，是经验公式，写的多、积累的多了，自然就能写出自己的经验公式。 float GetSmoothness(Interpolators i) { float smoothness = 1; #if defined(_SMOOTHNESS_ALBEDO) smoothness = tex2D(_MainTex, i.uv.xy).a; #elif defined(_SMOOTHNESS_METALLIC) &amp;&amp; defined(_METALLIC_MAP) smoothness = tex2D(_MetallicMap, i.uv.xy).a ; #endif return smoothness * _Smoothness; } 查看Smoothness_Albedo uniform vs albedo 自发光 通过模拟材质表面光源效果，只需要在basePase采样一次即可。 [NoScaleOffset]_EmissionMap(\"EmissionMap\", 2D) = \"white\"{} _Emission(\"Emission\", Color) = (0,0,0)//默认黑色，叠加 //base pass #pragma shader_feature _ _EMISSION_MAP //自发光采样 float3 GetEmission(Interpolators i){ #ifdef FORWARD_BASE_PASS #ifdef _EMISSION_MAP return tex2D(_EmissionMap, i.uv.zw).rgb; #else return _Emission; #endif #endif return 0; } //片元函数 final.rgb += GetEmission(i); emission HDR Emission HDR：颜色的RGB分量可以大于1，创建bloom效果。 Unity中_MaterialEditor_正好提供了_TexturePropertyWithHDRColor_函数，需要参数一是颜色范围；参数二是否需要alpha通道。 颜色范围有ColorPickerHDRConfig对象声明，亮度范围和曝光范围。先取StandardShader数值(0,99, 1/99, 3) Rect r = MaterialEditor.TexturePropertyWithHDRColor ( MakeMapGUIContent(mapinfo, null), mapinfo, bumpScale, config, false ); $ $" }, { "title": "Unity Reflection 反射(翻译八)", "url": "/posts/Unity_Reflection/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-06 20:00:00 +0800", "content": "本篇摘要: 采样坏境 使用reflection probes探针 创建粗糙或光滑的镜面 完成box投影与立方体采样 混合两个探针 环境映射-Environment Mapping 一块完美的镜子是不会发生漫反射，但现在我们自己的Shader包含的光照：环境光、漫反射、高光反射、纹理、阴影，结果看起来蛮好。但是当把Metallic设为1，Smoothness设位0.95，看起来很亮就很不自然了。从下图看尽管颜色是白色但整个表面都是黑色，只有一个很小的高亮点。这个亮点形成1是光源的入射，2朝向观察者的反射。 金属感高亮点 间接镜面反射 之前对于间接光光照计算时，只计算了漫反射没有计算镜面反射，默认值给的0。这就是为什么球面是黑色。现在我们简单的在_CreateIndirectLight_函数给specular变量赋值，看球的表面有什么变化： indirectLight.specular = float3(1,0,1); Pink! 这就给出突破点，计算间接光specular的正确值，就可以反射周围环境！ 改变smoothness值，注意边缘过渡 关于边缘反射，最著名的就是菲涅耳反射。我们先使用UNITY_BRDF_PBS的版本来计算。 环境采样-Sampling Enviroment 为了反射周围环境，我们需要采样天空盒。场景内天空盒对应的内置变量是在_UnityShaderVariables_文件的_unity_SpecCube0_的，该变量类型取决于目标平台，又由_HLSLSupport_文件决定。而采样函数UNITY_SAMPLE_TEXCUBE宏需要两个参数，1是天空盒，2是uv。先用法线替代uv。同时天空盒支持HDR高动态范围颜色，所以还需要对HDR解码到RGB：UnityCG包含DecodeHDR函数 half4 envSample = UNITY_SAMPLE_TEXCUBE(unity_SpecCube0, i.normal); indirectLight.specular = DecodeHDR(envSample, unity_SpecCube0_HDR); 环境采样 UNITY_SAMPLE_TEXCUBE 函数是根据平台自动切换对应的CG函数,DecodeHDR转RGB：RGBM包含解码后的RGB和M系数。最终的RGB要与\\(xM^y\\)结果相乘。 // UNITY_SAMPLE_TEXCUBE内部实现 // Decodes HDR textures // handles dLDR, RGBM formats inline half3 DecodeHDR (half4 data, half4 decodeInstructions) { // Take into account texture alpha // if decodeInstructions.w is true(the alpha value affects the RGB channels) half alpha = decodeInstructions.w * (data.a - 1.0) + 1.0; // If Linear mode is not supported we can skip exponent part #if defined(UNITY_COLORSPACE_GAMMA) return (decodeInstructions.x * alpha) * data.rgb; #else #if defined(UNITY_USE_NATIVE_HDR) // Multiplier for future HDRI relative to absolute conversion. return decodeInstructions.x * data.rgb; #else return (decodeInstructions.x * pow(alpha, decodeInstructions.y)) * data.rgb; #endif #endif } 反射追踪-Tracing Reflections 虽然得到正确的颜色，但没有看见正确的反射结果。因为上面使用了球体的法线采样环境，且投影不依赖视图方向，因此就好像在球体上画了环境。为了得到正确的结果，我们需要得到从相机到表面的方向，然后用表面法线再反射该方向。 UnityIndirect CreateIndirectLight(Interpolators i, float3 viewDir) { //。。。 #if defined(FORWARD_BASE_PASS) float3 reflectDir = reflect(-viewDir, i.normal); half4 envSample = UNITY_SAMPLE_TEXCUBE(unity_SpecCube0, reflectDir); indirectLight.specular = DecodeHDR(envSample, unity_SpecCube0_HDR); #endif return indirectLight; } 正确的反射 反射探针 Unity自带反射探针组件。通过chuangjia你_GameObject/Light/Reflection Probe。_参数如下图的的 Reflection Probe探针参数 Type可以设置位bake或realtime，不管那种模式都会渲染6次。其中realtime模式下可以让程序通过代码配置：采样频率、在满足某种情况下激活采样，这能适当地节省运行时计算量。而烘焙模式下，需要把物体设置为静态模式。 有瑕疵的反射 只有完美的光滑表面才能产生完美的反射，越粗糙的表面它的漫反射越多。如何模拟暗淡的模糊镜面反射？ 开启MipMaps. bake模式下，烘焙后得到的cubeMap 粗糙镜面 我们可使用UNITY_SAMPLE_TEXCUBE_LOD宏指定采样cubeMap的mipmap等级。由于烘焙得到的环境cubeMap使用三线性过滤，所以能混合相邻mipMapLevel.这可使得根据smoothness大小确定mipmap等级。材质越粗糙，mipMap等级就越高。粗糙值范围也是[0,1]，也就是1-smoothess。Unity提供了UNITY_SPECCUBE_LOD_STEPS宏来计算这个范围。 /*2Lod采样*/ float3 reflectDir = reflect(-viewDir, i.normal); float roughness = 1 - _Smoothness; half4 envSample = UNITY_SAMPLE_TEXCUBE_LOD(unity_SpecCube0, reflectDir, roughness * UNITY_SPECCUBE_LOD_STEPS); indirectLight.specular = DecodeHDR(envSample, unity_SpecCube0_HDR); smoothness衰减 事实上，粗糙度和mipmap等级不是线性的，Unity使用了\\(1.7r – 0.7r^2\\)公式换算。r是原始的粗糙度 float3 reflectDir = reflect(-viewDir, i.normal); float roughness = 1 - _Smoothness; roughness *= 1.7 - 0.7 * roughness; half4 envSample = UNITY_SAMPLE_TEXCUBE_LOD(unity_SpecCube0, reflectDir, roughness * UNITY_SPECCUBE_LOD_STEPS); indirectLight.specular = DecodeHDR(envSample, unity_SpecCube0_HDR); 更早地粗糙 UnityStandardBRDF文件提供了Unity_GlossyEnvironment函数计算roughness、采样cubeMap、转换HDR代码，以及Unity_GlossyEnvironmentData结构体包含了roughness、反射方向。 /*3Unity宏*/ float3 reflectDir = reflect(-viewDir, i.normal); Unity_GlossyEnvironmentData envData; envData.roughness = 1 - _Smoothness; envData.reflUVW = reflectDir; indirectLight.specular = Unity_GlossyEnvironment ( \tUNITY_PASS_TEXCUBE (unity_SpecCube0), \tunity_SpecCube0_HDR, \tenvData ); 这是Unity_GlossyEnvironment函数，内部计算细节与我们计算大致相同。 // - half3 Unity_GlossyEnvironment (UNITY_ARGS_TEXCUBE(tex), half4 hdr, Unity_GlossyEnvironmentData glossIn) { half perceptualRoughness = glossIn.roughness /* perceptualRoughness */ ; // TODO: CAUTION: remap from Morten may work only with offline convolution, see impact with runtime convolution! // For now disabled #if 0 float m = PerceptualRoughnessToRoughness(perceptualRoughness); // m is the real roughness parameter // smallest such that 1.0+FLT_EPSILON != 1.0 (+1e-4h is NOT good here. is visibly very wrong) const float fEps = 1.192092896e-07F; // remap to spec power. See eq. 21 in https://dl.dropboxusercontent.com/u/55891920/papers/mm_brdf.pdf float n = (2.0/max(fEps, m*m))-2.0; n /= 4; // remap from n_dot_h formulatino to n_dot_r. See section \"Pre-convolved Cube Maps vs Path Tracers\" // in https://s3.amazonaws.com/docs.knaldtech.com/knald/1.0.0/lys_power_drops.html // remap back to square root of real roughness // (0.25 include both the sqrt root of the conversion and sqrt for going from roughness to perceptualRoughness) perceptualRoughness = pow( 2/(n+2), 0.25); #else // MM: came up with a surprisingly close approximation to what the #if 0'ed out code above does. perceptualRoughness = perceptualRoughness*(1.7 - 0.7*perceptualRoughness); #endif half mip = perceptualRoughnessToMipmapLevel(perceptualRoughness); half3 R = glossIn.reflUVW; half4 rgbm = UNITY_SAMPLE_TEXCUBE_LOD(tex, R, mip); return DecodeHDR(rgbm, hdr); } #define UNITY_PASS_TEXCUBE(tex) tex, sampler##tex 模拟凹凸镜 给材质球指定一个法线纹理。廉价的水面扰动模拟。 模拟水面扰动 金属vs非金属 非金属，0.5、0.75、0.9 金属反射上色；非金属反射不上色(方块还是白色) 镜面和阴影 间接光反射依赖于物体表面的直接光照，最明显的就是阴影区域。对非金属而言，阴影区域反倒很亮。 阴影区域更亮 对于金属而言，smoothness越光滑阴影越暗 smoothnees由大到小 Box投影 一个probes，所有球体反射一样 我们不想每个球体都配一个probe。但是只有一个Probe时为了能得到周围的反射，我们要计算probe反射方向与每个立方体的交点，然后构造中心probe到此交点的向量，得到最终的反射。 一个probes，所有球体反射一样 反射探针box box边界 特性 box尺寸和原点确定了其位置在世界空间的立方体区域 始终与轴对齐，忽略所有旋转和缩放 Unity使用这些区域决定在渲染时使用哪个探针 box指定了立方区域大小，以该大小进行投影 调整采样方向-BoxProjection 增加BoxProjection函数，目的是初始化反射方向。从cubeMap坐标和box边界采样得到。 第一步是偏移box到相对该表面顶点为中心 /*初始化box投影方向*/ float3 BoxProjection(float3 direction, float3 position, float3 cubeMapPosition, float3 boxMin, float3 boxMax) { boxMin -= position; boxMax -= position; return direction; } 第二步缩放方向向量，以便从该位置移动到交点位置。 x维度，如果方向向量x分量是正数，它就指向box的max边界；否则指向box的min边界。然后用正确边界值再除以方向向量的x分量，得到适当的标量。 y、z维度同理。 取得三个标量，再从中拿到一个最小值。表示哪个边界面最接近表面。 计算最近边界面 float3 BoxProjection(float3 direction, float3 position, float3 cubeMapPosition, float3 boxMin, float3 boxMax) { boxMin -= position; boxMax -= position; float x = (direction.x &gt; 0 ? boxMax.x : boxMin.x) / direction.x; float y = (direction.y &gt; 0 ? boxMax.y : boxMin.y) / direction.y; float z = (direction.z &gt; 0 ? boxMax.z : boxMin.z) / direction.z; float scalar = min(min(x, y), z); return direction; } 第三步使用最小标量缩放方向向量找到交点。通过减去cubeMap位置得到新的反射方向。 return direction * scalar + (position - cubeMapPosition); 可以使用任何非零向量对cubemap进行采样。cubemap采样基本上和我们刚才做的是一样的。它指出向量指向哪个面，然后执行除法以找到与cubemap面相交的点。它使用这个点的适当坐标来采样纹理。 简化上面三步的代码如下： float3 BoxProjection(float3 direction, float3 position, float3 cubeMapPosition, float3 boxMin, float3 boxMax) { boxMin -= position; boxMax -= position; /*\tfloat x = (direction.x &gt; 0 ? boxMax.x : boxMin.x) / direction.x; float y = (direction.y &gt; 0 ? boxMax.y : boxMin.y) / direction.y; float z = (direction.z &gt; 0 ? boxMax.z : boxMin.z) / direction.z; float scalar = min(min(x, y), z);*/ float3 scalarVec = (direction &gt; 0 ? boxMax : boxMin) / direction; float scalar = min(min(scalarVec.x, scalarVec.y), scalarVec.z); return direction * scalar + (position - cubeMapPosition); } 正确的box投影 这样的盒型投影探针能够很好的解决多个probe带来的性能问题。 可选的投影 组件有个Project toggle开关，用以控制是否使用盒型投影探针。Unity把这个开关值存在cubeMap坐标的第四个分量，如果w值大于0表示开启盒型投影探针。 /*初始化box投影方向*/ float3 BoxProjection(float3 direction, float3 position, float4 cubeMapPosition, float3 boxMin, float3 boxMax) { boxMin -= position; boxMax -= position; /*float x = (direction.x &gt; 0 ? boxMax.x : boxMin.x) / direction.x; float y = (direction.y &gt; 0 ? boxMax.y : boxMin.y) / direction.y; float z = (direction.z &gt; 0 ? boxMax.z : boxMin.z) / direction.z; float scalar = min(min(x, y), z);*/ if (cubeMapPosition.w &gt; 0) { float3 scalarVec = (direction &gt; 0 ? boxMax : boxMin) / direction; float scalar = min(min(scalarVec.x, scalarVec.y), scalarVec.z); direction = direction * scalar + (position - cubeMapPosition); } return direction; } Unity有一个UNITY_BRANCH宏来要求编译器提供和和实际编写时一样的分支而不是条件赋值语句。不太赞同在shader使用大量分支语句 UNITY_BRANCH if (cubemapPosition.w &gt; 0) { //... } Unity也提供了计算采样boxProjection反射方向的函数：BoxProjectedCubemapDirection定义在_UnityStandardUtils文件中。不使用的原因是对方向做了归一化，前面讲了任何非零向量都可采样。 //- inline half3 BoxProjectedCubemapDirection (half3 worldRefl, float3 worldPos, float4 cubemapCenter, float4 boxMin, float4 boxMax) { // Do we have a valid reflection probe? UNITY_BRANCH if (cubemapCenter.w &gt; 0.0) { half3 nrdir = normalize(worldRefl); #if 1 half3 rbmax = (boxMax.xyz - worldPos) / nrdir; half3 rbmin = (boxMin.xyz - worldPos) / nrdir; half3 rbminmax = (nrdir &gt; 0.0f) ? rbmax : rbmin; #else // Optimized version half3 rbmax = (boxMax.xyz - worldPos); half3 rbmin = (boxMin.xyz - worldPos); half3 select = step (half3(0,0,0), nrdir); half3 rbminmax = lerp (rbmax, rbmin, select); rbminmax /= nrdir; #endif half fa = min(min(rbminmax.x, rbminmax.y), rbminmax.z); worldPos -= cubemapCenter.xyz; worldRefl = worldPos + nrdir * fa; } return worldRefl; } 混合反射探针 在探针组件定义的立方体区域边界切换时，如何做到自然过渡？ 边界过渡僵硬 两个探针插值计算 shader支持了两个探针数据，第二个探针内置变量名是unity_SpecCube1。我们要采样两次环境贴图并依据哪个更优进行插值。Unity已经做了计算把插值值存在了，unity_SpecCube0_BoxMin的第四个分量w。w为1只是第一个探针，值小于1开始混合。 float3 reflectDir = reflect(-viewDir, i.normal); Unity_GlossyEnvironmentData envData; envData.roughness = 1 - _Smoothness; envData.reflUVW = BoxProjection(reflectDir, i.worldPos, unity_SpecCube0_ProbePosition unity_SpecCube0_BoxMin, unity_SpecCube0_BoxMax);//reflectDir; float3 probe0 = Unity_GlossyEnvironment( UNITY_PASS_TEXCUBE(unity_SpecCube0), unity_SpecCube0_HDR, envData ); envData.reflUVW = BoxProjection(reflectDir, i.worldPos, unity_SpecCube1_ProbePosition, unity_SpecCube1_BoxMin, unity_SpecCube1_BoxMax); float3 probe1 = Unity_GlossyEnvironment( //UNITY_PASS_TEXCUBE(unity_SpecCube1),//注意：这里由于需要两个探针过渡，但是场景内只有一个探针。所以用下面代码消除错误。 UNITY_PASS_TEXCUBE_SAMPLER(unity_SpecCube1,unity_SpecCube0), unity_SpecCube1_HDR, envData ); indirectLight.specular = lerp(probe1 ,probe0, unity_SpecCube0_BoxMin.w); 正确过渡 探针盒重叠 多个探针重叠有一个权重值 weight 也可以混合探针和天空盒，其中off是关闭探针只用天空盒 reflection probes 优化 由于计算两个探针的计算量太大，增加一个分支 #if UNITY_SPECCUBE_BLENDING UNITY_BRANCH if (unity_SpecCube0_BoxMin.w &lt; 0.9999) { envData.reflUVW = BoxProjection(reflectDir, i.worldPos, unity_SpecCube1_ProbePosition, unity_SpecCube1_BoxMin, unity_SpecCube1_BoxMax); float3 probe1 = Unity_GlossyEnvironment( //UNITY_PASS_TEXCUBE(unity_SpecCube1), UNITY_PASS_TEXCUBE_SAMPLER(unity_SpecCube1, unity_SpecCube0), unity_SpecCube1_HDR, envData ); indirectLight.specular = lerp(probe1, probe0, unity_SpecCube0_BoxMin.w); } else { indirectLight.specular = probe0; } #else indirectLight.specular = probe0; #endif 对于BoxProjection函数的优化指令：UNITY_SPECCUBE_BOX_PROJECTION #if UNITY_SPECCUBE_BOX_PROJECTION UNITY_BRANCH if (cubeMapPosition.w &gt; 0) { float3 scalarVec = (direction &gt; 0 ? boxMax : boxMin) / direction; float scalar = min(min(scalarVec.x, scalarVec.y), scalarVec.z); direction = direction * scalar + (position - cubeMapPosition); } #endif 模拟反射的反弹 这有个光的反弹系数，最高5次。计算量很大。$ $" }, { "title": "Unity Shadow 阴影(翻译七)", "url": "/posts/Unity_Shadows/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-05 20:00:00 +0800", "content": "本篇摘要: 探索Unity中的阴影渲染 投射一个方向光阴影 接收一个方向光阴影 支持对聚光源和点光源阴影 方向光阴影-Direction 前面写的光照shader产生了相当真实的效果，可它假设着来自每个光源的光线最终都会击中它的片元，但是这只有在那些光线没有被遮挡才成立。 方向光投射阴影的草图 当一个物体位于光源和另一个物体之间时，它可能会阻止部分或全部光线到达另一个物体。这些光线照亮了第一个物体就不再可能照亮第二个物体。因此，第二个物体有一部不发光，而不发光的区域位于第一个物体的阴影下。我们通常是这样描述：第一个物体投射了一个阴影到第二个物体。 实际上，在全光照和全阴影的存在过渡区，被称为半阴影。这是因为所有光源都有一个体积，因此，这些区域只有部分光源是可见的，意味着它是部分阴影。光源远大，表面距离阴影投射器越远，半影区域也就越大。 但是Unity不支持半影，只支持软阴影soft shadow，但它是阴影过滤算法。 半阴影或是soft shadow 启用阴影-Enable Shadow 先关闭环境光，这样会更容易看见阴影。 没有投射阴影 没有阴影，物体间的空间视觉感不太强。在QualitySetting可以打开或关闭阴影。 阴影参数 同时确保光源开启投射阴影，分辨率依赖于上面的quality设置 阴影类型 阴影投射 阴影贴图-Shadow map Unity是如何把阴影添加到屏幕？上面所有物体使用的standard着色器，有一些方法确定光线是否被阻挡。 要搞清楚一个点是否在阴影中，可以通过在场景中从光线到表面片元投射光线，如果光线在到达表面之前击中某些东西，说明它就被阻挡了。这些事是物理引擎做的，但是要计算每个片元与每个光是不实际的，而且还要把结果传递给GPU。 现在有许多支持实时阴影的技术，它们各有优劣。而Unity采用了最常用的技术：Shadow Mapping。这意味着unity把阴影数据存储至纹理中。现在来看看它是如何工作的。 打开frame Debugger，Window/Frame Debugger。点击Enable，按顺序查看面板信息。注意看看每帧在gameScene视图中的不同，以及阴影的开启。 frame debugger调试 当启用阴影绘制时，这个绘制过程变得非常复杂：有更多的渲染阶段，和更多的draw call。阴影绘制非常昂贵！ 渲染深度纹理-Rendering to the Depth Texture 当方向阴影激活后，Unity在渲染过程开启一个depth pass通道计算。结果存储在与屏幕分辨率相匹配的纹理，这个pass通道会渲染整个屏幕，但是只收集每个片元的深度信息。这些信息与GPU用于确定一个片段渲染结束时在先前渲染的片段之上(前)还是之下(后)的信息相同。 这个数据对应在裁剪空间(clip space)坐标的z分量值。而裁剪空间是定义摄像机能看见的区域，深度信息最终存储为0-1范围内的值。在debugger查看该纹理时，近裁切面附近的纹理显示趋近为(白)浅色，远裁切面附近的纹素texel，颜色趋近黑(暗)色。 depth texture, 摄像机近裁切面为5 与屏幕分辨率一致 这些信息实际上与阴影没有太多直接关系，但Unity在后面的pass通道使用了它。 渲染阴影贴图-Rendering to Shadow Maps 这步主要工作：先渲染第一个光源的阴影贴图，然后就会渲染第二个光源的阴影贴图。 再一次渲染整个屏幕，并再次把深度信息存储在纹理中。但是，这此的屏幕渲染是从光源位置角度进行的，实际上是把光源作为摄像机。这意味着用深度值告诉了我们光线击中物体之前走了多远距离，这可以用来确定什么东西被遮蔽了! 阴影贴图记录了实际的几何图形的深度信息。而法线贴图是为了添加粗糙表面的一种错觉， 阴影贴图会忽略它们。因此，阴影不受法线贴图的影响。 由于我们使用方向光，这些光模拟的摄像机是正交投影，没有透视投影。因此它们模拟的相机的位置精确性就不那么重要。Unity将定位常规相机使其能够看见视野内所有物体。 左第一个光源，右第二个光源 事实上，原来Unity渲染整个场景不是每个光只渲染一次，而是每个光要渲染四次！ 这个阴影纹理被分成四个象限，每个象限从不同的角度呈现。这是因为我们选择使用 Four Cascades(QualitySetting)。如果我们设置为Two Cascades，就是每个光渲染两次； 如果设置没有，只会渲染一次。我们接下来要探查阴影质量与该项设置的关系。Unity为什么渲染这么 多次。 收集阴影-Collecting Shadows 我们已经从摄像机的角度得到场景的深度信息，也有了从每个光模拟的相机视角得到的深度信息，这些数据存在不同的裁剪空间。但是我们知道这些空间的相对位置和方向，因此能从一个空间转换到另一个空间。这允许我们从两个视角比较深度测量。理论上讲，我们有两个向量应该在同一点交会结束，这样相机和光源都能看见该点，说明它被点亮了。如果光的向量在到达该点之前结束，则光被挡住，这意味着该点被阴影化。 当摄像机看不到一个点时？ 看不到的这些点被隐藏在距离相机更近的其他点后面。 场景深度纹理仅包含最接近的点。 因此没有时间浪费在评估隐藏点。 每个光的屏幕空间阴影 Unity通过渲染一个单独的覆盖整个视野的面片来创建这些纹理，它使用了Hidden/Internal-ScreenSpaceShadows shader的通道，每个片元从场景和光源的深度纹理采样，进行比较，渲染最终阴影值到屏幕空间的阴影纹理。亮的纹素值设为1，阴影纹素值设为0。此时Unity能执行过滤，创建柔和的阴影。 shader 通道0 为什么Unity在渲染和收集间交替？ 每个光需要它自己的屏幕空间阴影贴图，然而从光源位置视野渲染的阴影贴图能被重复使用。 采样阴影贴图-Sampling the Shadow Maps 最后，Unity完成了阴影渲染。现在屏幕是常规渲染，只有一个更改：光照颜色与它的阴影贴图的值相乘。这就消除了被遮挡的光线。渲染的每个片元都要采样阴影贴图，每个最终隐藏在其他对象之后的片元会最后绘制。因此这些片元最后能接收到最终能遮挡它们的对象的阴影。当在frame debugger步进调试观察时，您还可以看到阴影在实际投射它的对象之前出现。当然这些错误只在渲染帧时很明显，一旦完成渲染就是正确的了 部分渲染帧 阴影质量-Shadow Quality 虽然场景是从光源的方向进行渲染，但是该方向与场景内摄像机视野方向不匹配。因此阴影贴图的纹素与最终呈现图像的纹素是没有对齐的，会出现锯齿。阴影贴图的分辨率也会不同，最终图像的分辨率是由显示设置决定的，而阴影贴图的分辨率由阴影质量设置决定。 当阴影贴图的纹素最后渲染的比最终图像大时，将很明显：阴影的边缘出现叠加，在使用硬阴影时非常明显。 硬阴影 vs 软阴影 在质量设置面板修改使用hard shadow、lowest resolution、no cascades。就会看见满屏的锯齿。 低质量阴影 “阴影是一张纹理” 现在就非常明显了。但是上图有些阴影出现在了不该出现的地方。 距离摄像机越近的阴影，它们的纹素变得越大。这是由于阴影贴图当前覆盖了场景相机的整个可视区域。在QualitySetting面板通过降低阴影覆盖的区域，来提升靠近相机区域的阴影质量。 Shadow Distance降至25，其他参数一致 通过限制靠近屏幕相机的阴影区域，我们能使用相同的阴影纹理去覆盖更多小区域。结果是能得到更好的阴影。但是会丢失更远区域的阴影细节，因为当阴影接近最大距离时会逐渐消失。 理想情况是，既要获得近距离高质量阴影，同时也要保留远处的阴影。因为远处的阴影最后渲染在较小的屏幕区域，就可以用作低分辨率阴影纹理。这就是Shadow Cascades的工作。当启用该选项，多个阴影贴图渲染进同一张纹理，每张贴图对应某些距离来使用。 fourCascades,100Distance,hardShodw,LowResolution 当使用FourCascades，上图结果看起来比之前的要好，尽管我们使用了同一张纹理分辨率，我们更有效的使用了纹理。不过缺点就是我们现在至少要渲染场景3次以上。当渲染屏幕空间阴影纹理时，Unity关注从正确Cascade采样，如下图CascadeSplits：一个cascade结束是下一个的开始。 Cascade Splits 可以控制cascade的范围，作为阴影距离的一部分。也能通过改变_Shading Mode/Miscellaneous/Shadow Cascades_观察scene视图的变化。 Cascade范围：StableFit 上图显示的cascade形状(覆盖区域)是可以通过_Shadow Projection_调整，默认是_Stable Fit_：这个模式cascade条带选择的区域基于距离摄像机位置的远近。其他模式是_Close Fit_：使用相机的深度信息替代，在相机可视方向产生一条规则的条带。 Close Fit Close Fit模式可以更高效的利用阴影纹理，绘制更高质量的阴影。然而，该阴影投射模式(ShadowProjection)取决于阴影产生后位置和方向以及相机参数。结果是，当移动或旋转相机，阴影贴图也会跟着移动。这就是著名的阴影抖动。所以Stale Fit是引擎默认的选项。 Close Fit: swimming Stable Fit模式下，在相机位置改变时Unity能够对齐纹理，纹素看起来好像没动。实际上cascade移动了，只是在cascade相互过渡时阴影会发生改变。如果没有注意到cascade改变，就不容易察觉到。 Stable Fit: edge transition 阴影“痤疮”(0!什么鬼) 当我们使用低质量的硬阴影时，我们看见一些阴影出现在不正确的地方。不幸的是，不管如何设置_Quality Setting_都会发生。 Shadow Acne 阴影贴图中每个纹素表示光线击中表面的点。然而，这些纹素不是单独点。它们最后要覆盖很大的区域并且与光的方向对 齐，而不是与表面一致。结果时，它们会像黑色瓦片最终黏在、穿过、伸出表面；当阴影纹理的一部分从投射出阴影的表面 伸出时，表面看起来也会产生阴影。 凸起 阴影凸起的另一个来源是数字精度的限制，当使用非常小的距离时这些限制会导致不正确的结果。默认是0.05. light组件中设置没有biases 避免该问题的一个方法是：当渲染阴影纹理时增加深度偏移。这个_偏差系数_目的是增加‘光投射到表面距离’，把阴影‘推进’表面内。 Biases系数控制粉刺 较低的Bias系数会产生粉刺，而较高的偏差系数就会有另一个问题：当投射阴影的对象逐渐远离光源时，阴影也会逐渐飘离原对象。使用较小的值问题还可接受，但太大的值会导致物体与该物体的阴影不再相连接了，好像飞起来了。 太大的Bias导致阴影飘移 除了距离bias偏差，还有法线偏差。该系数辅助调整阴影投射：沿着法线，将投射的阴影顶点向内‘推’。该值也会改善“阴影粉刺”，但是越大的值越会使阴影变得更小并且有可能使阴影中间出现洞。 best bias settings？没有最优的默认值，必须不停的实验调整 。 抗锯齿 Anti-Aliasing:图形边缘锯齿缓和。在Unity开启了4倍抗锯齿，感觉并没有达到想要的抗锯齿效果。 Unity采用的多重采样抗锯齿方案：MSAA，通过沿三角形边缘执行超级采样以消除边缘锯齿，更重要的是Unity渲染屏幕空间阴影时，它使用了一个单独四方面片覆盖整个可视区域。结果是，这就没有了三角形边缘，因此MSAA对屏幕空间阴影纹理采样就没有效果了。MSAA对最终图像有效，但阴影值是取之屏幕空间阴影纹理，当亮表面紧挨着暗表面被阴影覆盖时就非常明显。明暗之间的边缘是反锯齿的，而阴影边缘则不是。 no AA 4倍MSAA 当然也有FXAA，是屏幕后处理抗锯齿，效果挺好！ 投射阴影 通过上面我们知道了Unity如何创建方向光阴影，是时候写自己的Shader来支持阴影了。当前光照shader既不支持投射阴影也不支持接收阴影。 首先来处理投射阴影：我们知道对于方向光阴影Unity会渲染多次屏幕。对每个阴影纹理一次是深度pass渲染，一次是每个光源渲染。而屏幕空间阴影纹理是屏幕效果暂时与我们无关。阴影渲染Pass标签是_ShadowCaster。_因为我们只对深度值感兴趣，它与别的Pass相比应该会简单。增加一个pass Pass{ Tags{\"LightMode\" = \"ShadowCaster\"} CGPROGRAM #pragma target 3.0 #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"MyShadow.cginc\" ENDCG } 创建一个_MyShadow.cginc_文件 #if !defined(MY_SHADOW_INCLUDE) #define MY_SHADOW_INCLUDE #include “UnityCG.cginc” struct InputData { float4 position : POSITION; }; float4 MyVertexProgram(InputData i) : SV_POSITION{ return UnityObjectToClipPos(i.position); } half4 MyFragmentProgram() : SV_TARGET{ return 0; } #endif 上面写完就嫩产生方向光阴影了。下面开始用代码调优阴影质量。 偏差-Bias 我们要支持阴影的偏移。在渲染深度Pass时该值是0，但当渲染阴影纹理时，偏差值取光照组件设置。我们要做的就是：在顶点函数中在裁切空间下，对顶点坐标应用深度偏差。UnityCG函数_UnityApplyLinerShadowBias：_ float4 MyVertexProgram(InputData i) : SV_POSITION{ float4 position = UnityObjectToClipPos(i.position); return\tUnityApplyLinearShadowBias(position); } 在裁剪空间增加Z分量，复杂的是在其次坐标空间下，必须补偿透视投影，这样偏移不会随着与相机距离改变而改变，也必须确保结果不会越界。 float4 UnityApplyLinearShadowBias(float4 clipPos) { #if defined(UNITY_REVERSED_Z) // We use max/min instead of clamp to ensure proper handling of the rare case // where both numerator and denominator are zero and the fraction becomes NaN. clipPos.z += max(-1, min(unity_LightShadowBias.x / clipPos.w, 0)); float clamped = min(clipPos.z, clipPos.w*UNITY_NEAR_CLIP_VALUE); #else clipPos.z += saturate(unity_LightShadowBias.x / clipPos.w); float clamped = max(clipPos.z, clipPos.w*UNITY_NEAR_CLIP_VALUE); #endif clipPos.z = lerp(clipPos.z, clamped, unity_LightShadowBias.y); return clipPos; } 同时支持Normal Bias，必须根据法向量移动顶点坐标。因此，添加一个normal变量。然后可以使用UnityCG定义的UnityClipSpaceShadowCasterPos函数 float4 MyVertexProgram(InputData i) : SV_POSITION{ //float4 position = UnityObjectToClipPos(i.position); float4 position = UnityClipSpaceShadowCasterPos(i.position, i.normal); return UnityApplyLinearShadowBias(position); } 先将顶点坐标转换到世界空间，然后转换到裁剪空间。计算光的方向，计算法线和光的角度，取正弦值，最后转与观察投影矩阵相乘转到裁剪空间。 float4 UnityClipSpaceShadowCasterPos(float4 vertex, float3 normal) { float4 wPos = mul(unity_ObjectToWorld, vertex); if (unity_LightShadowBias.z != 0.0) { float3 wNormal = UnityObjectToWorldNormal(normal); float3 wLight = normalize(UnityWorldSpaceLightDir(wPos.xyz)); // apply normal offset bias (inset position along the normal) // bias needs to be scaled by sine between normal and light direction // (http://the-witness.net/news/2013/09/shadow-mapping-summary-part-1/) // // unity_LightShadowBias.z contains user-specified normal offset amount // scaled by world space texel size. float shadowCos = dot(wNormal, wLight); float shadowSine = sqrt(1-shadowCos*shadowCos); float normalBias = unity_LightShadowBias.z * shadowSine; wPos.xyz -= wNormal * normalBias; } return mul(UNITY_MATRIX_VP, wPos); } 写完就具备了完全的阴影投射 接收阴影 First,我们先关注主方向光的阴影，因为该光源属于BasePass，必须要先适配。当主方向光投射阴影，Unity会找一个启用了SHADOWS_SCREEN关键字的shader变体。所以我们要在Base Pass创建两个变体，同之前使用顶点光关键字类似：一个无，一个是该关键字。 #pragma multi_compile _ VERTEXLIGHT_ON #pragma multi_compile _ SHADOWS_SCREEN 该basePass有两个multi_compile指令，每个都是单关键字。因此编译后这里会有4个变体： // Total snippets: 3 // -- // Snippet #0 platforms ffffffff: SHADOWS_SCREEN VERTEXLIGHT_ON 4 keyword variants used in scene: &lt;no keywords defined&gt; SHADOWS_SCREEN VERTEXLIGHT_ON SHADOWS_SCREEN VERTEXLIGHT_ON (老版本Unity有可能出现)当增加了multi_compile指令后，shader编译器会提示关于__ShadowCoord_不存在。这是因为_UNITY_LIGHT_ATTENUATION_宏在使用阴影时的行为不同导致。在MyLighting_shadow.cginc顶点函数快速修复 #if defined(UNITY_SCREEN) float attenuation = 1; #else UNITY_LIGHT_ATTENUATION(attenuation, 0, i.worldPos); #endif 采样阴影 Secend,采样屏幕空间阴影纹理。 Third,需要获取屏幕空间纹理坐标，从顶点函数传递给片元函数。在插值器Interpolator添加一个float4 变量以支持传递阴影纹理坐标。从裁剪空间开始(裁剪空间顶点坐标)。 struct Interpolator{ #if defined(SHADOWS_SCREEN) float4 shadowCoordinate : TEXCOORD6; #endif } Interpolators MyVertexProgram(VertexData v) { //。。。 #if defined(SHADOWS_SCREE) i.shadowCoordinate = i.position; #endif //。。。 } 错误的纹理坐标映射 AutoLignt.cginc定义了Sampler2D _ShadowMapTexture，可以通过它访问屏幕阴影纹理。但是要覆盖整个屏幕，就需要屏幕空间坐标。在裁剪空间，XY坐标范围是[-1, 1]，而屏幕空间下是[0,1]；然后偏移坐标与屏幕左小脚等于0对齐。因为我们处理的使透视变换，偏移坐标值取决于距离，这里的偏移值等于加上齐次坐标的w分量之后的一半。 #if defined(SHADOWS_SCREEN) i.shadowCoordinate.xy = (i.position.xy + i.position.w) * 0.5; i.shadowCoordinate.zw = i.position.zw; #endif 错误的左下角映射 上图的投影错误，还需要通过x和y除以齐次坐标进一步转换 错误投影 上图结果仍然是错误的，影子被拉伸了。这是由于在顶点函数计算导致，不应该在传递给片元函数时提前修改原始数据，需要保持它们的独立性。在片元函数再次除以w. 颠倒的投影 此时，影子是上下颠倒的。如果它们被翻转，这意味着你的图形Direct3D屏幕空间Y坐标从0向下到1，而不是向上。要与此同步，翻转顶点的Y坐标。 #if defined(SHADOWS_SCREEN) i.shadowCoordinate.xy = (float2(i.pos.x, -i.pos.y) + i.pos.w);// (i.pos.xy + i.pos.w) * 0.5; i.shadowCoordinate.zw = i.pos.zw; #endif 继续错误 内置函数使用 SHADOW_COORDS宏定义纹理坐标 TRANSFRE_SHADOW宏获取阴影纹理坐标(转换) #define TRANSFER_SHADOW(a) a._ShadowCoord = ComputeScreenPos(a.pos); SHADOW_ATTENUATION宏阴影纹理明暗衰减 #define SHADOW_COORDS (idx1) unityShadowCoord4 _ShadowCoord : TEXCOORD##idx1; #define SHADOW_ATTENUATION(a) unitySampleShadow(a._ShadowCoord) UNITY_LIGHT_ATTENUATION宏包含了SHADOW_ATTENUATION宏使用，可替换之 当启用SHADOWS_SCREEN指令时，会自动计算，不启用不计算，没有任何损失。 struct Interpolators { //... // #if defined(SHADOWS_SCREEN) // float4 shadowCoordinates : TEXCOORD5; // #endif SHADOW_COORDS(5) //... }; Interpolators MyVertexProgram (VertexData v) { //... // #if defined(SHADOWS_SCREEN) // i.shadowCoordinates = i.position; // #endif TRANSFER_SHADOW(i); //... } UnityLight CreateLight (Interpolators i) { //... #if defined(SHADOWS_SCREEN) float attenuation = SHADOW_ATTENUATION(i); #else UNITY_LIGHT_ATTENUATION(attenuation, 0, i.worldPos); #endif UNITY_LIGHT_ATTENUATION(attenuation, i, i.worldPos); //... } 正确了 ComputeScreenPos函数 inline float4 ComputeNonStereoScreenPos(float4 pos) { float4 o = pos * 0.5f; o.xy = float2(o.x, o.y*_ProjectionParams.x) + o.w; o.zw = pos.zw; return o; } inline float4 ComputeScreenPos(float4 pos) { float4 o = ComputeNonStereoScreenPos(pos); #if defined(UNITY_SINGLE_PASS_STEREO) o.xy = TransformStereoScreenSpaceTex(o.xy, pos.w); #endif return o; } 聚光灯阴影 关闭方向光，增加聚光灯后，竟然直接有阴影了。这是Unity宏带来的便利。 点光源阴影 再看帧调试器 SpotLight Debugger 上图对于聚光灯源阴影的渲染工作量很少 不同之处： 没有方向光独立的深度pass和屏幕空间阴影pass，而是直接渲染阴影纹理； 与方向光渲染阴影还有很大的差别之处：聚光灯光线不是平行的，因此用光的位置模拟相机视角会得到一个透视视角，结果就是不支持阴影分段渲染(cascades)； normal bias(法线偏差)只支持方向光阴影，对于其他光源类型简单的置为0； 采样代码不同。 相同之处： 投射阴影的这段代码通用。 采样阴影纹理 由于聚光灯不使用屏幕空间的阴影，这段采样纹理代码就有点不一样。因此，如果我们想要使用软阴影，我们必须在fragment程序中进行过滤。而Unity宏已经做了过滤计算UnitySampleShadowmap。 //阴影坐标把顶点坐标从模型空间转到世界空间再转到光的阴影空间得到。 // - Spot light shadows #if defined (SHADOWS_DEPTH) &amp;&amp; defined (SPOT) #define SHADOW_COORDS(idx1) unityShadowCoord4 _ShadowCoord : TEXCOORD##idx1; #define TRANSFER_SHADOW(a) a._ShadowCoord = mul(unity_WorldToShadow[0], mul(unity_ObjectToWorld,v.vertex)); #define SHADOW_ATTENUATION(a) UnitySampleShadowmap(a._ShadowCoord) #endif 然后SHADOW_ATTENUATION宏使用UnitySampleShadowmap函数采样阴影映射。这个函数定义在_UnityShadowLibrary_，_AutoLight_文件引用了它。当使用硬阴影时，该函数对阴影纹理采样一次。当使用软阴影时，它对纹理采样四次并对结果取平均值。这个结果没有用于屏幕空间阴影的过滤效果好，但是速度快得多。 hard vs. soft SpotLight Shadow // Spot light shadows inline fixed UnitySampleShadowmap (float4 shadowCoord) { // DX11 feature level 9.x shader compiler (d3dcompiler_47 at least) // has a bug where trying to do more than one shadowmap sample fails compilation // with \"inconsistent sampler usage\". Until that is fixed, just never compile // multi-tap shadow variant on d3d11_9x. #if defined (SHADOWS_SOFT) &amp;&amp; !defined (SHADER_API_D3D11_9X) // 4-tap shadows #if defined (SHADOWS_NATIVE) #if defined (SHADER_API_D3D9) // HLSL for D3D9, when modifying the shadow UV coordinate, really wants to do // some funky swizzles, assuming that Z coordinate is unused in texture sampling. // So force it to do projective texture reads here, with .w being one. float4 coord = shadowCoord / shadowCoord.w; half4 shadows; shadows.x = UNITY_SAMPLE_SHADOW_PROJ(_ShadowMapTexture, coord + _ShadowOffsets[0]); shadows.y = UNITY_SAMPLE_SHADOW_PROJ(_ShadowMapTexture, coord + _ShadowOffsets[1]); shadows.z = UNITY_SAMPLE_SHADOW_PROJ(_ShadowMapTexture, coord + _ShadowOffsets[2]); shadows.w = UNITY_SAMPLE_SHADOW_PROJ(_ShadowMapTexture, coord + _ShadowOffsets[3]); shadows = _LightShadowData.rrrr + shadows * (1-_LightShadowData.rrrr); #else // On other platforms, no need to do projective texture reads. float3 coord = shadowCoord.xyz / shadowCoord.w; half4 shadows; shadows.x = UNITY_SAMPLE_SHADOW(_ShadowMapTexture, coord + _ShadowOffsets[0]); shadows.y = UNITY_SAMPLE_SHADOW(_ShadowMapTexture, coord + _ShadowOffsets[1]); shadows.z = UNITY_SAMPLE_SHADOW(_ShadowMapTexture, coord + _ShadowOffsets[2]); shadows.w = UNITY_SAMPLE_SHADOW(_ShadowMapTexture, coord + _ShadowOffsets[3]); shadows = _LightShadowData.rrrr + shadows * (1-_LightShadowData.rrrr); #endif #else float3 coord = shadowCoord.xyz / shadowCoord.w; float4 shadowVals; shadowVals.x = SAMPLE_DEPTH_TEXTURE (_ShadowMapTexture, coord + _ShadowOffsets[0].xy); shadowVals.y = SAMPLE_DEPTH_TEXTURE (_ShadowMapTexture, coord + _ShadowOffsets[1].xy); shadowVals.z = SAMPLE_DEPTH_TEXTURE (_ShadowMapTexture, coord + _ShadowOffsets[2].xy); shadowVals.w = SAMPLE_DEPTH_TEXTURE (_ShadowMapTexture, coord + _ShadowOffsets[3].xy); half4 shadows = (shadowVals &lt; coord.zzzz) ? _LightShadowData.rrrr : 1.0f; #endif // average-4 PCF half shadow = dot (shadows, 0.25f); #else // 1-tap shadows #if defined (SHADOWS_NATIVE) half shadow = UNITY_SAMPLE_SHADOW_PROJ(_ShadowMapTexture, shadowCoord); shadow = _LightShadowData.r + shadow * (1-_LightShadowData.r); #else half shadow = SAMPLE_DEPTH_TEXTURE_PROJ(_ShadowMapTexture, UNITY_PROJ_COORD(shadowCoord)) &lt; (shadowCoord.z / shadowCoord.w) ? _LightShadowData.r : 1.0; #endif #endif return shadow; } 点光源阴影 如果直接使用点光源，会有编译报错：undeclared identifier ‘UnityDecodeCubeShadowDepth’。该函数在_UnityCG.cginc_文件。 UnityPBSLighting文件引用；AutoLight文件引用 所以根据引用结构，需要把UnityPBSLighing文件放在第一位引用。就不会报错了。 左：render six times per light 投射阴影 从帧调试器查看，左边一个光要渲染6次，两盏光就是12次了。有很多个RenderJobPoint渲染了。结果是，点光源的阴影纹理是一个立方体贴图，而立方体贴图是通过相机在6个不同方向观察场景，每个方向渲染一面组成六面体，前面1.4讲过把光源模拟相机对屏幕渲染。所以点光源阴影计算很费，尤其是实时点光源阴影。 错误的阴影纹理 当渲染点光源阴影纹理时，Unity引擎会找shader变体关键字SHADOWS_CUBE，而SHADOWS_DEPTH关键字只适用于方向光和聚光灯。为了支持点光源阴影，Unity提供了一个特殊编译指令 #pragma multi_compile_shadowcaster // -- // Snippet #2 platforms ffffffff: SHADOWS_CUBE SHADOWS_DEPTH 2 keyword variants used in scene: SHADOWS_DEPTH SHADOWS_CUBE 所以，需要创建一个独立的处理程序。这里首先要计算光到表面的距离，但得知道光到表面的方向。在顶点函数先转换顶点坐标所在世界空间，再计算光的方向。然后在片元函数计算该方向向量长度再与bias偏差相加。然后再除以点光源的范围映射到[0.1]再与长度相乘，最后解码。而_LightPositionRange.w = 1/range已经计算好了隐射范围，直接用。 #if defined(SHADOWS_CUBE) struct Interplotars { float4 position : SV_POSITION; float3 lightVec : TEXCOORD0; }; Interplotars MyVertexProgram(InputData v){ Interplotars i; i.position = UnityObjectToClipPos(v.position); i.lightVec = mul(unity_ObjectToWorld, v.position).xyz - _LightPositionRange.xyz; //float4 position = UnityClipSpaceShadowCasterPos(i.position, i.normal);//方向光源：简单的裁剪空间顶点坐标 return\ti; } half4 MyFragmentProgram(Interplotars i) : SV_TARGET{ float depth = length(i.lightVec) + unity_LightShadowBias.x; depth *= _LightPositionRange.w; return UnityEncodeCubeShadowDepth(depth); } #else float4 MyVertexProgram(InputData i) : SV_POSITION{ //float4 position = UnityObjectToClipPos(i.position); float4 position = UnityClipSpaceShadowCasterPos(i.position, i.normal); return\tUnityApplyLinearShadowBias(position); } half4 MyFragmentProgram() : SV_TARGET{ return 0; } #endif 正确的阴影纹理 UnityEncodeCubeShadowDepth函数： // Shadow caster pass helpers float4 UnityEncodeCubeShadowDepth (float z) { #ifdef UNITY_USE_RGBA_FOR_POINT_SHADOWS return EncodeFloatRGBA (min(z, 0.999)); #else return z; #endif } // 使用浮点类型cube——map,存储再8位RGBA纹理 inline float4 EncodeFloatRGBA( float v ) { float4 kEncodeMul = float4(1.0, 255.0, 65025.0, 16581375.0); float kEncodeBit = 1.0/255.0; float4 enc = kEncodeMul * v; enc = frac (enc);//返回小数部分 enc -= enc.yzww * kEncodeBit; return enc; } 采样阴影纹理 在additional pass的编译指令，Unity宏已经做了。 //同样计算光的方向，然后采样cubeMap。区别是float3类型而不是float4，不需要齐次坐标。 // - Point light shadows #if defined (SHADOWS_CUBE) #define SHADOW_COORDS(idx1) unityShadowCoord3 _ShadowCoord : TEXCOORD##idx1; #define TRANSFER_SHADOW(a) a._ShadowCoord = mul(unity_ObjectToWorld, v.vertex).xyz - _LightPositionRange.xyz; #define SHADOW_ATTENUATION(a) UnitySampleShadowmap(a._ShadowCoord) #endif // // Point light shadows //在这种情况下，UnitySampleShadowmap采样一个立方体地图，而不是2D纹理。 #if defined (SHADOWS_CUBE) samplerCUBE_float _ShadowMapTexture; inline float SampleCubeDistance (float3 vec) { #ifdef UNITY_FAST_COHERENT_DYNAMIC_BRANCHING return UnityDecodeCubeShadowDepth(texCUBElod(_ShadowMapTexture, float4(vec, 0))); #else return UnityDecodeCubeShadowDepth(texCUBE(_ShadowMapTexture, vec)); #endif } inline half UnitySampleShadowmap (float3 vec) { float mydist = length(vec) * _LightPositionRange.w; mydist *= 0.97; // bias #if defined (SHADOWS_SOFT) float z = 1.0/128.0; float4 shadowVals; shadowVals.x = SampleCubeDistance (vec+float3( z, z, z)); shadowVals.y = SampleCubeDistance (vec+float3(-z,-z, z)); shadowVals.z = SampleCubeDistance (vec+float3(-z, z,-z)); shadowVals.w = SampleCubeDistance (vec+float3( z,-z,-z)); half4 shadows = (shadowVals &lt; mydist.xxxx) ? _LightShadowData.rrrr : 1.0f; return dot(shadows,0.25); #else float dist = SampleCubeDistance (vec); return dist &lt; mydist ? _LightShadowData.r : 1.0; #endif } #endif // #if defined (SHADOWS_CUBE) 同样，如果使用软阴影会采样四次并取平均值，硬阴影采样一次。同时没有进行过滤计算，计算昂贵且效果很粗糙！ hard vs soft pointLight Shadows 对于点光源阴影实在不能用于手机平台，替代方式就是用无阴影点光+cookie投射，模拟阴影。或者用较少的聚光灯阴影代替。" }, { "title": "Unity 纹理高级用法(翻译六)", "url": "/posts/Unity_Advance_Texture/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-04 20:00:00 +0800", "content": "本篇摘要: 扰动法线以模拟凹凸视觉 从高度场计算法线 采样和混合法线贴图 从切线空间转换到世界空间 Bump Map(凹凸贴图): Bump Map Type Describe NormalMap 法线图，映射公式：normal=pixel*2-1,反映射：pixel=(normal+1)/2. 法线存储既可以在模型空间，也可以在切线空间。//unity顶点输入结构带切线变量，一般存在切线空间更佳 HeightMap 灰度图(黑白纹理-强度值)，颜色越浅该表面越向外凸起，颜色深越凹。视差映射技术，与Occlusion Map搭配使用体验更佳，计算昂贵 Occlusion Map 灰度图，表面细节更丰富。颜色值白色表示应该接收完全间接照明的区域，黑色表示没有间接照明。如裂缝或褶皱，实际上不会接收到太多的间接光，可与高度图一起使用 Detail Maps 参考StandardShader:第二细节纹理，应用第二反照率图和第二法线图，在近距离观察时有清晰的细节，比如毛孔、细小的裂缝等。计算昂贵 常用纹理 NormalMap 法线纹理：比较常用 HeightMap 高度纹理(视差映射)：手机平台不常用，使用法线纹理替代 Occlusion Map：细节纹理 Secondary Maps (Detail Maps) &amp; Detail Mask：细节纹理 高度图-HeightMap 高度图为了模拟平面的凹凸程度，将高度(黑白色)数据存储在纹理中，由于纹理数据是二维的，即u轴和v轴，那为了得到这些数据为每个片段生成法向量，可分别在u轴和v轴上采样。先从U轴计算$ f(u)=h $ ,如果知道了斜率就能求得u轴上所有点的法向量，但斜率由h的变化程度高低决定。为了近似得到从一个点到下一个点的高度差： 斜率采样示意图，从$f(0) \\rightarrow f(1)$ 这是对切向量的一个粗略的估算，它把整张纹理作为线性的斜率。那为了避免这种粗略计算，可以采样两个靠的更近点的。例如，从$ 0 \\rightarrow {1\\over2} $，那这两点的斜率$ f=f({1 \\over 2})-f(0) $，同时f因子被缩小，需要乘以相应的倍数。$ 2f({1 \\over 2})-f(0) $。扩展开来，可以得到如下的函数：δ值越小越精确，必须大于0小于1. 差分函数: [f^`(u) = {f(u+δ) - f(u) \\over δ}] 有限差分函数: [f^`(u)= lim_{(δ\\rightarrow 0)} {f(u+δ) - f(u) \\over δ}] 那么切向量就是 \\(\\begin{bmatrix} 1\\\\ f`(u)\\\\ 0 \\end{bmatrix}^\\mathrm{T}\\) ，从切向量计算法向量 \\(\\begin{bmatrix} f’(u)\\\\ 1\\\\ 0 \\end{bmatrix}^\\mathrm{T}\\) sampler2D _HeightMap; float4 _HeightMap_TexelSize;//xy是纹素坐标(uv)，zw是整张纹理宽高 float2 delta = float2(_HeightMap_TexelSize.x, 0);//u轴 float h1 = tex2D(_HeightMap, i.uv);//模型uv在高度图采样 float h2 = tex2D(_HeightMap, i.uv + delta);//二次采样 //第一步套用公式 //i.normal = float3(1, (h2 - h1) / delta.x, 0); //第二步优化，缩放向量并不改变方向，消除了除法操作 //i.normal = float3( delta.x, (h2 - h1), 0); //第三步改变垂直方向，需要得到法向量正向垂直于表面，那么逆时针旋转90度以翻转x分量符号.//Y是扰动法向量的高低变化因子 i.normal = float3( h1 - h2, 1 , 0); i.normal = normalize(i.normal); 有限差分只在一个方向近似求值，为了更好近似可以在两个方向线性逼近. 中心差分: [f^`(u)= lim_{(δ\\rightarrow 0)} {f(u+{δ\\over 2}) - f(u - {δ\\over 2}) \\over δ}] float2 delta = float2(_HeightMap_TexelSize.x * 0.5, 0); float h1 = tex2D(_HeightMap, i.uv - delta); float h2 = tex2D(_HeightMap, i.uv + delta); i.normal = float3(h1 - h2, 1, 0); 那么$f’(u, v)$计算$f’(v)$同理,切向量 \\(\\begin{bmatrix} 0\\\\ f’(v)\\\\ 1 \\end{bmatrix}^\\mathrm{T}\\) 法向量是 \\(\\begin{bmatrix} 0\\\\ 1\\\\ f’(v) \\end{bmatrix}^\\mathrm{T}\\) float2 du = float2(_HeightMap_TexelSize.x * 0.5, 0); float u1 = tex2D(_HeightMap, i.uv - du); float u2 = tex2D(_HeightMap, i.uv + du); //float3 tu = float3(1, u2 - u1, 0); float2 dv = float2(0, _HeightMap_TexelSize.y * 0.5); float v1 = tex2D(_HeightMap, i.uv - dv); float v2 = tex2D(_HeightMap, i.uv + dv); //float3 tv = float3(0, v2 - v1, 1); //i.normal = cross(tv, tu);//直接使用叉积求出垂直于u和v轴的法向量=&gt;(0*(v2-v1)-(u2-u1)*1, 1*1-0*0, (u2-u1)*0-1*(v2-v1))=(u1-u2, 1, v1-v2) i.normal = float3(u1 - u2, 1, v1 - v2); i.normal = normalize(i.normal); \\(\\begin{bmatrix} 0\\\\ f’(v)\\\\ 1 \\end{bmatrix}\\) x \\(\\begin{bmatrix} 1\\\\ f’(u)\\\\ 0 \\end{bmatrix}\\) = \\(\\begin{bmatrix} -f’(u)\\\\ 1\\\\ -f’(v) \\end{bmatrix}\\) 法线-Normal Map 高度图是每帧采样实时计算法线，为了避免高额计算量，采用预制法线纹理代替。 Unity中使用高度图 导入高度图作为法线贴图预先计算法线纹理必须勾选Create from Grayscale，白色表示相对更高，黑色表示相对更低。 像素分量范围是[0,1]，而法线分量范围[-1,1]。相互映射转换公式为： $ pixel = {(normal+1)\\over 2} $ $ normal = pixel \\cdot 2 – 1 $ 法线纹理呈现淡蓝色，这是因为法向映射最常见的约定是将向上的方向存储在Z分量中(垂直于表面外侧)，又由于DXT5nm纹理压缩格式的原因，只存储了X与Y分量舍弃了Z分量(Y分量存储在G通道，X分量存储在A通道，RB通道被舍弃)。通过推导法向量的单位向量可得Z分量： [ N = N ^2 = N_x{^2} + N_y{^2} + N_z{^2} = 1] [Nz = \\sqrt{1 - N_x{^2} - N_y{^2}}] //第一种方法 // Unpack normal as DXT5nm (1, y, 1, x) or BC5 (x, y, 0, 1) //dxt5压缩对应的位置取wy i.normal.xy = tex2D(_NormalMap, i.uv).wy * 2 - 1; i.normal.xy *= _BumpScale;//计算Z之前缩放才有效，平坦凹凸程度 i.normal.z = sqrt(1 - saturate(dot(i.normal.xy, i.normal.xy)));//dot模拟平方计算-((x,y)*(x,y))=-x方-y方 i.normal = i.normal.xzy; i.normal = normalize(i.normal); //第二种方法 //UnityStandardUtils.cginc包含了解码法线函数，替代上面的方法 i.normal = UnpackScaleNormal(tex2D(_NormalMap, i.uv), _BumpScale); i.normal = i.normal.xzy; i.normal = normalize(i.normal); 细节纹理与细节法线-Detail Maps(Second Texture) 与 Detail Normals 第二细节纹理与MainTexture合并，简要代码如下： //顶点uv坐标映射到纹理uv i.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); i.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); //计算第二纹理的影响 float3 albedo = tex2D(_MainTex, i.uv.xy).rgb * _Tint.rgb; albedo *= tex2D(_DetailTex, i.uv.zw) * unity_ColorSpaceDouble;//颜色空间转换 第二细节纹理的法线映射 i.normal = UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale); i.normal = UnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale); i.normal = i.normal.xzy; i.normal = normalize(i.normal); 法线融合-Blending Normals 方式一：(main.normal + details.normal) * 0.5; 简单容易，但结果不是很好。主纹理和细节纹理都变得平坦。理想情况下，当其中一个是平的，期望它不会影响到另一个。 float3 mainNormal = UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale); float3 detailNormal = UnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale); i.normal = (mainNormal + detailNormal) * 0.5; i.normal = i.normal.xzy; i.normal = normalize(i.normal); 方式二：用z分量做缩放因子求偏导函数，然后相加。[Mx, My, Mz]T = [Mx/Mz, My/Mz, 1]T 同理求得detail偏导函数，然后相加：[Mx/Mz + Dx/Dz, My/Mz + Dy/Dz, 1]T .效果很好，但是在合并陡峭时仍将失去细节。 float3 mainNormal = UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale); float3 detailNormal = UnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale); i.normal = float3(mainNormal.xy / mainNormal.z + detailNormal.xy / detailNormal.z, 1); i.normal = i.normal.xzy; i.normal = normalize(i.normal); 方式三：白色调和，对上一步合并法线分别乘以MzDz,然后再去掉x和y的缩放因子夸大缩放，使陡峭更加明显，同时平坦的法线，它不会影响到另一个了。 float3 mainNormal = UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale); float3 detailNormal = UnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale); i.normal = float3(mainNormal.xy + detailNormal.xy, mainNormal.z * detailNormal.z); //UnityStandardUtils包含了混合函数 //i.normal = BlendNormals(mainNormal, detailNormal); i.normal = i.normal.xzy; i.normal = normalize(i.normal);View Code 切线空间-Tangent Space 切线空间的法线纹理：顶点为原点，z轴为法线方向，x轴为切线方向，y轴为垂直于xz的副切线方向。Unity导入模型计算切线默认使用了mikktspace(在顶点着色器计算)，也可以在片元着色器计算cross得到副切线向量。 顶点下计算： struct VertexData { \tfloat4 tangent : TANGENT; }; struct Interpolators { \tfloat4 tangent : TEXCOORD2; }; 使用UnityCG中的UnityObjectToWorldDir在顶点程序中将切线转换为世界空间。 当然，这仅适用于切线的XYZ部分。 它的W分量需要不加修改地传递。 Interpolators MyVertexProgram (VertexData v) { \tInterpolators i; \ti.position = mul(UNITY_MATRIX_MVP, v.position); \ti.worldPos = mul(unity_ObjectToWorld, v.position); \ti.normal = UnityObjectToWorldNormal(v.normal); \ti.tangent = float4(UnityObjectToWorldDir(v.tangent.xyz), v.tangent.w); \ti.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); \ti.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); \tComputeVertexLightColor(i); \treturn i; } 现在我们可以将法线从切线空间转换为世界空间。 float3 binormal = cross(i.normal, i.tangent.xyz) * i.tangent.w; i.normal = normalize( \ttangentSpaceNormal.x * i.tangent + \ttangentSpaceNormal.y * i.normal + \ttangentSpaceNormal.z * binormal ); 去掉显式YZ交换，将其与空间转换结合在一起。 //tangentSpaceNormal = tangentSpaceNormal.xzy;\t float3 binormal = cross(i.normal, i.tangent.xyz) * i.tangent.w; i.normal = normalize( \ttangentSpaceNormal.x * i.tangent + \ttangentSpaceNormal.y * binormal + \ttangentSpaceNormal.z * i.normal ）; 在构造副法线时，还有一个额外的细节。假设一个对象的scale设置为(- 1,1,1)，这意味着它是镜像的。在这种情况下，我们必须翻转副法线，来正确地镜像切线空间。事实上，当奇数维数为负时，我们必须这样做。_UnityShaderVariables_通过定义float4 unity_WorldTransformParams变量来帮助我们完成这个任务。当需要翻转副法线时，它的第四个分量为- 1，否则为1。 float3 binormal = cross(i.normal, i.tangent.xyz) *(i.tangent.w * unity_WorldTransformParams.w); 转换空间 在世界空间下计算 fixed4 MyFrag(v2f v) : SV_TARGET{ \t//... \tfloat3 tangentSpaceNormal= UnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale); \t#if defined(BINORMAL_PER_FRAGMENT) \t float3 binormal = cross(v.normal, v.tangent.xyz) * v.tangent.w; \t#else \t float3 binormal = v.binormal; \t#endif \t//把切线空间转到世界空间 \t//tangentSpaceNormal * [v.tangent,binromal, v.normal]T \tv.normal = normalize( \t tangentSpaceNormal.x * v.tangent + \t tangentSpaceNormal.y * binormal + \t tangentSpaceNormal.z * v.normal \t); \t//... } 在切线空间计算 //计算副切线 float3 binormal = cross(normalize(i.normal), normalize(i.tangent.xyz)) * i.tangent.w; //切线空间矩阵//行优先的填充 float3x3 t_matrix = float3x3(i.tangent.xyz, binormal, i.normal); //把各种信息转到切线空间下参与计算 副切线在哪算合适 在顶点计算不必计算叉乘函数，通过宏定义开启。 struct Interpolators { \tfloat4 position : SV_POSITION; \tfloat4 uv : TEXCOORD0; \tfloat3 normal : TEXCOORD1; \t#if defined(BINORMAL_PER_FRAGMENT) \t\tfloat4 tangent : TEXCOORD2; \t#else \t\tfloat3 tangent : TEXCOORD2; \t\tfloat3 binormal : TEXCOORD3; \t#endif \tfloat3 worldPos : TEXCOORD4; \t#if defined(VERTEXLIGHT_ON) \t\tfloat3 vertexLightColor : TEXCOORD5; \t#endif }; 如果不确定在哪里计算比较好，可以同时支持这两种方法。假设定义了BINORMAL_PER_FRAGMENT，我们逐像素计算每个片段的副法线。否则，逐顶点计算。在前一种情况下，我们保持我们的float4 tangent变量 。在后者中，我们需要两个float3变量。 float3 CreateBinormal (float3 normal, float3 tangent, float binormalSign) { \treturn cross(normal, tangent.xyz) * \t\t(binormalSign * unity_WorldTransformParams.w); } Interpolators MyVertexProgram (VertexData v) { \tInterpolators i; \ti.position = mul(UNITY_MATRIX_MVP, v.position); \ti.worldPos = mul(unity_ObjectToWorld, v.position); \ti.normal = UnityObjectToWorldNormal(v.normal); \t#if defined(BINORMAL_PER_FRAGMENT) \t\ti.tangent = float4(UnityObjectToWorldDir(v.tangent.xyz), v.tangent.w); \t#else \t\ti.tangent = UnityObjectToWorldDir(v.tangent.xyz); \t\ti.binormal = CreateBinormal(i.normal, i.tangent, v.tangent.w); \t#endif \ti.uv.xy = TRANSFORM_TEX(v.uv, _MainTex); \ti.uv.zw = TRANSFORM_TEX(v.uv, _DetailTex); \tComputeVertexLightColor(i); \treturn i; } void InitializeFragmentNormal(inout Interpolators i) { \tfloat3 mainNormal = \t\tUnpackScaleNormal(tex2D(_NormalMap, i.uv.xy), _BumpScale); \tfloat3 detailNormal = \t\tUnpackScaleNormal(tex2D(_DetailNormalMap, i.uv.zw), _DetailBumpScale); \tfloat3 tangentSpaceNormal = BlendNormals(mainNormal, detailNormal); \t#if defined(BINORMAL_PER_FRAGMENT) \t\tfloat3 binormal = CreateBinormal(i.normal, i.tangent.xyz, i.tangent.w); \t#else \t\tfloat3 binormal = i.binormal; \t#endif \ti.normal = normalize( \t\ttangentSpaceNormal.x * i.tangent + \t\ttangentSpaceNormal.y * binormal + \t\ttangentSpaceNormal.z * i.normal \t); } 要对所有Pass块生效，需要使用CGINCLUDE … ENDCG包含" }, { "title": "Unity 基础光照多光源采样(翻译五)", "url": "/posts/Unity_Multi_Light/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-03 09:00:00 +0800", "content": "本篇摘要： 使用多个光源渲染 支持多光源类型 使用光照信息 计算顶点光照 了解球谐函数 Include Files 为了给Shader增加支持多个光源，我们需要增加更多Pass通道。但是这些Pass最终包含了几乎完全相似的代码，为了避免代码的重复性，我们可以通过把着色器代码移动到一个CG文件，然后在Shader代码中引用该文件 在文件目录中手动创建一个MyLighting.cginc文件，再把FirstLighting.shader内从#pragma以下到ENDCG以上区间内代码拷贝进MyLighting.cginc文件。这样 we不直接在shader中写这些重复的代码，而是通过include引用。 注意，.cginc文件也提供了类似的避免重复定义，#define XXX_INCLUDED，再把整个文件内容放置在预处理文件块中。 #if !defined(MY_LIGHTING_INCLUDED) #define MY_LIGHTING_INCLUDED //... #endif 第二光源-Direction 新建两个方向光对象，参数设置如下图： 两个光源参数 现在场景中有两个光，但是每个物体看起来没有什么区别。现在我们一次只激活一个光源，看看有什么变化。 左main光源，右minor光源 增加第二个Pass 当前场景内只能看见一个光源效果，这是由于MyMultiLightShader只有一个Pass且只计算了一个光源。Pass光照标签ForwardBase只计算主光源， 为了渲染额外的光源，需要增加一个Pass且指定光照标签为ForwardAdd方可计算额外的光源。 SubShader { Pass{ Tags { \"LightMode\" = \"ForwardBase\" } CGPROGRAM #pragma target 3.0 #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"MyLighting.cginc\" ENDCG } Pass { Tags { \"LightMode\" = \"ForwardAdd\" } CGPROGRAM #pragma target 3.0 #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"MyLighting.cginc\" ENDCG } } 现在虽然计算了两个光源，但是ForwardAdd计算结果会直接覆盖ForwardBase的结果。我们需要把这两个光照效果结合起来，需要在ForwardAdd Pass内使用混合。 UnityShader的Blend函数：如何通过定义两个因子来合并新旧数据？ 新旧数据分别与Blend函数的因子相乘然后相加，得到最终结果。如果Pass内没有Blend默认不混合，Blend One Zero。每个Pass计算后的数据会写入帧缓冲区中，也就会替换之前任何写入该缓冲区的内 容。为了把新旧数据都能加到帧缓冲区，我们可以需要指示GPU使用Blend one one模式。 Pass { Tags { \"LightMode\" = \"ForwardAdd\" } Blend One One //... } 左无混合， 右one one混合 Z-buffer GPU`s depth buffer： 一个物体第一次被渲染，GPU就会检查该片元是否会渲染在其他已经渲染过的像素的前面，这些距离 信息就存储在该缓冲区中。因此每个像素都有颜色和深度信息**，该深度表示从相机到最近表面的 每个像素的距离。 ForwardBase中，如果要渲染的片元前面没有任何内容(深度值最小)，它就是最靠近摄像机的表面。GPU也会继续运行fragment程序，生成新的颜色和记录新的深度。如果要渲染的片元的深度值最终比已经存在的大，说明它前面有东西，它就不会被渲染也不能看见. 在forward add中重复计算minor光时，要添加到已经存在的灯光，再次运行fragment程序时，因为针对的是同一个对象，最终记录了完全相同的深度值。因此两次写入相同的深度信息是没必要的，用ZWrite off关闭它。 Blend One One ZWrite Off 合批-Draw Call Batches 在Game视图右上角打开Stats窗口，可以更好地了解运行时发生的事情。查看Batches、Saved by batching数据。先只激活main光源。 Batches数据6，总共7 场景内有5个对象，应该是5个Batches。见下图图 实际Batches 通过FrameDebugger分析，实际是5个draw mesh加上3个内置阴影render函数，一共8个Batches。但是由于启用了动态批处理dynamic batching，所以有一个Saved by batching统计。 那现在来消除这3个阴影渲染函数调用，打开Edit/Project Settings/Quality。Shadows选择Disable Shadows. 先无视它这个系统清屏Clear函数。 去掉了阴影渲染函数 激活minor光源，如下图： **10** + 1 = 11 10个Batches？ 因为这5个对象被渲染了两次，最终为10个批次，而不是上面的4个。 动态批处理失效了！Unity规定动态批处理最多只支持一个方向光作用的物体对象。 帧调试-Frame Debugger 通过Window/Frame Debugger打开可以清楚了解屏幕画面是如何被渲染出来的 Frame Debugger调试 通过选择滑动条可单步调试渲染，窗口会自动显示每一步的细节。按照上面的顺序，优先画出了靠近相机的不透明物体，同时开启depth-buffer深度缓冲，这个front-to-back从前到后的渲染顺序是有效的，被遮挡的片元就会被跳过不渲染。如果使用back-to-front从后到前的顺序，同时关闭zwrite，就会覆写远处的像素，发生overdraw。 Unity渲染顺序是front-to-back，同时Unity喜欢把相似的物体分组。例如，sphere和cube分开，可避免在不同mesh网格间切换；或者把使用相同的material分组。 点光源Point Lights 先关闭两个方向光，再创建一个Point Light光。然后打开Frame Debugger调试查看。单步调试发现，第一次渲染的的纯黑色，然后才有怪异的光。 什么奇怪现象？ 即使没有激活方向光第一个base Pass始终都会渲染，因此渲染得到一个黑色轮廓。而第二个Pass会额外渲染一次，这次使用了point light代替了方向光，而代码任然是假设使用了方向光。 光照函数-Light Function 光越来越复杂了，现在把UnityLight的计算单独剥离为一个函数： UnityLight CreateLight(Interpolators i){ UnityLight light; light.color = _LightColor0.rgb; light.dir = _WorldSpaceLightPos0.xyz; light.ndotl = DotClamped(i.normal, lightDir); return light; } 修改后的Fragment代码如下： float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); //float3 lightDir = _WorldSpaceLightPos0.xyz; //float3 lightColor = _LightColor0.rgb; float3 viewDir = normalize(_WorldSpaceCameraPos - i.worldPos); float3 albedo = tex2D(_MainTex, i.uv).rgb * _Tint.rgb; float3 specularTint; float oneMinusReflectivity; albedo = DiffuseAndSpecularFromMetallic( albedo, _Metallic, specularTint, oneMinusReflectivity ); // UnityLight light; // light.color = lightColor; // light.dir = lightDir; // light.ndotl = DotClamped(i.normal, lightDir); UnityLight light = CreateLight(i); UnityIndirect indirectLight; indirectLight.diffuse = 0; indirectLight.specular = 0; return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir, light, indirectLight ); } 光照位置-Light Position _WorldSpaceLightPos0变量包含的是当前光的位置，但是在方向光的情况下，它实际上保存的是光方向的朝向。而我们使用了Point Light，这个变量就只是光的位置了(如其名)。因此必须要我们自己计算光的方向：减去片元的世界位置再归一化得到。 //light.dir = _WorldSpaceLightPos0.xyz; light.dir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos); 光s的衰减-Light Attenuation 在使用方向光的情况下，只需知道光的方向即可，因为它被认为是无限远的。 但是Point Light有明确的位置，这意味它到物体表面的距离也会产生影响，距离物体越远，物体表面越暗。也就是光的衰减。方向光的衰减是被假设为非常缓慢的以至于可以作为常亮，不需担心。那Point Light的衰减是什么样的？ 球形衰减：想象一下，从一个点向四面八方发射一束光子，随着时间推移，这些光子会以相同的移动速度逐渐远离这个点，就像组成了一个球体表面，而这个点就是球体中心。球的半径随着光子移动增长，光子的密度随着移动就会逐渐降低。这就决定了可见光的亮度。 球形衰减 衰减公式：球的表面积计算公式 $ s= 4πr^2 $。我们可以通过除以该公式得到光子的密度。把4π作为影响光的强度因子，先忽略掉这个常数。这就得到了衰减因子为$ 1\\over d^2 $，其中d是光的距离. UnityLight CreateLight(Interpolators i){ UnityLight light; //light.dir = _WorldSpaceLightPos0.xyz; float3 lightVec = _WorldSpaceLightPos0.xyz - i.worldPos; light.dir = normalize(lightVec); float attenuation = 1 / dot(lightVec, lightVec); light.color = _LightColor0.rgb * attenuation; light.ndotl = DotClamped(i.normal, light.dir); return light; } 过曝 靠近光源时非常明亮，这是因为越靠近球体的中心点，距离就越小，直到趋近于0。修改公式$ 1 \\over 1 + d^2 $。 float attenuation = 1 / (1 + dot(lightVec, lightVec)); 正常光强 光源范围-Light Range 现实中，光子持续移动直到击中某个物体停止。光变的非常微弱直到肉眼不可见，这意味着光的范围是可能无限远的。而实际上我们不会浪费时间去渲染不可见光，所以我们必须在某个时候停止渲染。 Point Light 和 Spot Light都有范围，位于范围内的物体将会使用此光源参与绘制，否则不会参与。它们的默认范围都是10，随着范围缩小，调用额外draw Call的物体会更少，这也会提高帧率. 把范围缩小到1，当拖动光源时会清楚看见物体何时进出这个范围，物体会突然变亮或不亮，要修复它需要确保衰减和范围是同步的。为了确保物体移出光源范围不会突然出现光线过渡，这就要求衰减系数在最大范围时为0 Unity把片元从世界空间转换到光源空间来计算点光源的衰减，光源空间是灯光对象本地空间坐标，按比例衰减。在该空间，点光源位于原点，任何超过一个单位的都不在该范围内，所以点到原点的距离的平方定义了衰减系数。Unity更进一步，使用距离的平方采样衰减图. 这样做确保了衰减早一点下降到0。没有这步，移动光源进出范围时我们仍将看见物体突然变亮或不亮环境因素。这个算法函数在AutoLight.cginc文件中。 AutoLight 引用结构 我们可以使用_UNITY_LIGHT_ATTENUATION_指令，注意其中有if预处理块，包含三个参数：第一个参数是attenuation；第二个参数是计算阴影；第三个参数是世界坐标。 //UNITY_LIGHT_ATTENUATION #ifdef POINT uniform sampler2D _LightTexture0; uniform unityShadowCoord4x4 unity_WorldToLight; #define UNITY_LIGHT_ATTENUATION(destName, input, worldPos) unityShadowCoord3 lightCoord = mul(unity_WorldToLight, unityShadowCoord4(worldPos, 1)).xyz; fixed destName = (tex2D(_LightTexture0, dot(lightCoord, lightCoord).rr). UNITY_ATTEN_CHANNEL * SHADOW_ATTENUATION(input)); #endif unityShadowCoord4在其他地方定义的；点击产生一个单精度值，.rr是重复取值组成float2.然后用来采样衰减纹理，而纹理是1维数据，第二个分量也无关紧要； UNITY_ATTEN_CHANNEL可能是r或a,取决于目标平台。 UnityLight CreateLight (Interpolators i) { UnityLight light; light.dir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos); // float3 lightVec = _WorldSpaceLightPos0.xyz - i.worldPos; // float attenuation = 1 / (dot(lightVec, lightVec)); UNITY_LIGHT_ATTENUATION(attenuation, 0, i.worldPos); light.color = _LightColor0.rgb * attenuation; light.ndotl = DotClamped(i.normal, light.dir); return light; } 需要在引用AutoLight文件之间宏定义POINT，才能呈现最终正确的画面. 混合光源-Mixing Light 关闭Point Light再次打开两个Directional light，这里又出现了错误的addition pass计算，把minor 方向光作为点光源计算。为了解决它，我们引入Shader variant 变体. 变体-Shader Variants 选中Shader文件，在Inspector点击Compileed code查看 // Total snippets: 2 // -- // Snippet #0 platforms ffffffff: Just one shader variant. // -- // Snippet #1 platforms ffffffff: Just one shader variant. 打开文件看到2个snippets代码片段，这是shader的passes。分别是base pass 和 additive pass。我们想要在additive pass中创建既支持directional 光又支持point 光的变体，需要使用Unity提供的multi_compile声明关键字，Unity将自动为每个关键字生成独立的shader。变体数量多少会影响编译效率！ Pass { Tags { \"LightMode\" = \"ForwardAdd\" } Blend One One ZWrite Off CGPROGRAM #pragma target 3.0 #pragma multi_compile DIRECTION POINT #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram //#define POINT #include \"MyLighting.cginc\" ENDCG } 编译后能看见2个关键字： // Total snippets: 2 // -- // Snippet #0 platforms ffffffff: Just one shader variant. // -- // Snippet #1 platforms ffffffff: DIRECTION POINT 2 keyword variants used in scene: DIRECTION POINT 使用关键字-KeyWords Unity决定使用那个变体，是基于当前光源类型和shader中定义的变体关键字。当渲染方向光它就使用_DIRECTIONAL_变体，当渲染点光源它就使用_POINT_变体。如果都不匹配，它就选着变体关键字列表中第一个变体。 UnityLight CreateLight(Interpolators i){ UnityLight light; #ifdef POINT float3 lightVec = _WorldSpaceLightPos0.xyz - i.worldPos; #else float3 lightVec = _WorldSpaceLightPos0.xyz; #endif UNITY_LIGHT_ATTENUATION(attenuation, 0, i.worldPos); light.color = _LightColor0.rgb * attenuation; light.dir = normalize(lightVec); light.ndotl = DotClamped(i.normal, light.dir); return light; } 变体-两次渲染 聚光源-Spotlights 上面说了方向光和点光源，Unity还提供了聚光灯。聚光灯与点光源类似，不过它发射的是呈圆锥形光束。同样，为了支持聚光灯，需再加一个变体支持. #pragma multi_compile DIRECTIONAL POINT 查看增加了SPOT光源编译后的变体文件 // -- // Snippet #1 platforms ffffffff: DIRECTION POINT SPOT 3 keyword variants used in scene: DIRECTION POINT SPOT 聚光灯同样有一个（原点）发射点，朝锥形方向发射光子，所以也需要手动计算光的方向 #if defined(POINT) || defined(SPOT) float3 lightVec = _WorldSpaceLightPos0.xyz - i.worldPos; #else float3 lightVec = _WorldSpaceLightPos0.xyz; #endif //... 聚光源衰减 聚光灯衰减方式开始时与点光源相同，转换到光源空间然后计算衰减因子。然后把原点后面所有点强制衰减为0，将光线限制在聚光灯前面的物体上。然后把光空间中X和Y坐标作为UV坐标采样纹理，用于遮罩光线，该纹理是一个边缘模糊的圆，就像圆锥体. 同时变换到光 空间实际上是透视变换并使用了其次坐标。 //UNITY_LIGHT_ATTENUATION #ifdef SPOT sampler2D _LightTexture0; uniform unityShadowCoord4x4 unity_WorldToLight; sampler2D _LightTextureB0; inline fixed UnitySpotCookie(unityShadowCoord4 LightCoord) { return tex2D(_LightTexture0, LightCoord.xy / LightCoord.w + 0.5).w; } inline fixed UnitySpotAttenuate(unityShadowCoord3 LightCoord) { return tex2D(_LightTextureB0, dot(LightCoord, LightCoord).xx).UNITY_ATTEN_CHANNEL; } #defineUNITY_LIGHT_ATTENUATION(destName, input, worldPos) unityShadowCoord4 lightCoord = mul(unity_WorldToLight, unityShadowCoord4(worldPos, 1)); fixed shadow = UNITY_SHADOW_ATTENUATION(input, worldPos); fixed destName = (lightCoord.z &gt; 0) * UnitySpotCookie(lightCoord) * UnitySpotAttenuate(lightCoord.xyz) * shadow; #endif 光斑阴影-Light Cookies Cookies名字来源于剪影[cucoloris]，是指在电影、戏剧、摄影中为光线添加阴影。Unity支持3种light Cookies：DirectionLight、spotLight、pointLight。Cookie需要采样到纹理中. 聚光灯光斑阴影-Spotlight cookie **默认的聚光灯遮罩纹理是一个模糊的圆，但它也可以是任意的正方形纹理且它的边缘alpha降到0即可. ** 使用cookies的alpha通道遮罩光线，其他rgb通道无关紧要。 spot-light cookies设置 方向光斑阴影-Directon light Cookie direction lights的cookie is无限平铺，因此边缘必须无缝衔接，边缘不必过渡到0. direction cookie cookie size大小决定了可视面积，反过来又影响平铺速度。默认为10。 带有cookie的Direction light必须转换到光照空间，它也有自己的_UNITY_LIGHT_ATTENUTION_指令。Unity把它作为不同的方向光对待，放置到addive pass渲染，使用_DIRECTIONAL_COOKIE_启用。 #pragma multi_compile DIRECTIONAL DIRECTIONAL_COOKIE POINT SPOT 电光源光斑阴影-Point Light cookie 点光源的cookie是一个围绕球性的cube map映射纹理，同时必须指定_Mapping_映射模式，使Unity知道如何解释图像，最好的方法是自己提供一张cube map，这里先指定自动映射模式. Mapping模式 必须增加POINT_COOKIE关键字编译，Unity提供了一个简短的的关键字语义。 #pragma multi_compile_fwdadd //#pragma multi_compile DIRECTIONAL DIRECTIONAL_COOKIE POINT SPOT 打开编译后的文件 // Snippet #1 platforms ffffffff: DIRECTIONAL DIRECTIONAL_COOKIE POINT POINT_COOKIE SPOT 5 keyword variants used in scene: POINT DIRECTIONAL SPOT POINT_COOKIE DIRECTIONAL_COOKIE 同时带有cookie的点光源方向也需要自己计算 #if defined(POINT) || defined(POINT_COOKIE) || defined(SPOT) light.dir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos); #else light.dir = _WorldSpaceLightPos0.xyz; #endif Your browser does not support the video tag. Here is a link to the video file instead. 同时渲染三个光 顶点光照计算-Vertex Lights 在Forward前向渲染路径，每个可见的物体都必须在BasePass中渲染一次。这个Pass只关心主方向光，而有cookie的方向光会忽略。在此之上其他多余的光会自动增加additive pass。因此有多少光就会产生多少Draw Call. 举例，场景中增加四个点光源，让所有物体处于光源范围内。1个base加上4个additive pass，一共25个draw call。即使再增加一个方向光，也不会增加draw call. 在Unity/Edit/Quality中可以设置 Pixel Light Count，定义最大逐像素光照数量，这个决定了有多少个光会在片元函数被作为逐像素光照计算。默认为4个。每个物体渲染的灯光是不同的，Unity根据光的强度和距离从高到低排序，贡献最少的光首先被丢弃不参与计算。 由于不同的光影响不同的物体，有可能出现矛盾的光照效果. 当物体移动时可能变得更糟，移动会导致光线突然变化。这个问题很麻烦，因为光完全关闭了，幸运的是我们可以使用逐顶点渲染这一更节省性能的方式。这意味着光照计算都在顶点函数进行，然后得到的插值结果并传递给片元函数。可以使用定义_VERTEXLIGHT_ON_关键字激活计算。 Unity自带顶点光只支持Point Light。 这和逐顶点光照有区别(完全可以在顶点函数计算法线、光线方向、视野方向、反射反向，再用着色模型计算 颜色传递给片元函数，优点性能好，缺点着色粗糙) Your browser does not support the video tag. Here is a link to the video file instead. 物体受光数量差异 Pass { Tags { \"LightMode\" = \"ForwardBase\" } CGPROGRAM #pragma target 3.0 #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #pragma multi_compile _ VERTEXLIGHT_ON #include \"MyLighting.cginc\" ENDCG } 一个顶点光-One Vertex Light 把顶点光传到片元函数，需要在Interpolators结构体使用VERTEXLIGHT_ON关键字。然后定义一个生成顶点颜色的函数以解耦，由于是从Interpolators读写成员变量，需要inout修饰符. struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float3 normal : TEXCOORD1; float3 worldPos : TEXCOORD2; #if defined(VERTEXLIGHT_ON) float3 vertexLightColor : TEXCOORD3; #endif }; 创建一个单独的函数来计算这种颜色，使用了inout修饰符，同时读取和写入插值器。 void ComputeVertexLightColor (inout Interpolators i) { } Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.worldPos = mul(unity_ObjectToWorld, v.position); i.normal = UnityObjectToWorldNormal(v.normal); i.uv = TRANSFORM_TEX(v.uv, _MainTex); ComputeVertexLightColor(i); return i; } UnityShaderVariables定义了一个顶点光颜色数组：unity_LightColor[0].rgb void ComputeVertexLightColor (inout Interpolators i) { #if defined(VERTEXLIGHT_ON) i.vertexLightColor = unity_LightColor[0].rgb; #endif } 然后，在片元函数把顶点光照色增加到所有其他光照色。这可以把顶点光照色作为间接光对待。再把生成间接光的代码剥离解耦，把顶点光照色传递给间接光的漫反射. UnityIndirect CreateIndirectLight (Interpolators i) { UnityIndirect indirectLight; indirectLight.diffuse = 0; indirectLight.specular = 0; #if defined(VERTEXLIGHT_ON) indirectLight.diffuse = i.vertexLightColor; #endif return indirectLight; } float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); float3 viewDir = normalize(_WorldSpaceCameraPos - i.worldPos); float3 albedo = tex2D(_MainTex, i.uv).rgb * _Tint.rgb; float3 specularTint; float oneMinusReflectivity; albedo = DiffuseAndSpecularFromMetallic( albedo, _Metallic, specularTint, oneMinusReflectivity ); // UnityIndirect indirectLight; // indirectLight.diffuse = 0; // indirectLight.specular = 0; return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir, CreateLight(i), CreateIndirectLight(i) ); } 当把 Pixel Light Count 数量调为0时，每个物体被渲染为对应光照色的剪影。 纯色轮廓，0 Pixel Light Count Unity支持多达四个顶点光，这些光的坐标存储在float4变量： unity_4LightPosX0 unity_4LightPosY0 unity_4LightPosZ0 void ComputeVertexLightColor (inout Interpolators i) { #if defined(VERTEXLIGHT_ON) float3 lightPos = float3( unity_4LightPosX0.x, unity_4LightPosY0.x, unity_4LightPosZ0.x ); i.vertexLightColor = unity_LightColor[0].rgb; #endif } 这几个变量定义在 UnityShaderVariables.cginc 文件，这些变量的x y z w表示依次表示每个光的(x,y,z)。 接下来计算光的方向、反射方向、衰减$ 1\\over 1+d^2 $,得到最终的颜色. void ComputeVertexLightColor (inout Interpolators i) { #if defined(VERTEXLIGHT_ON) float3 lightPos = float3( unity_4LightPosX0.x, unity_4LightPosY0.x, unity_4LightPosZ0.x ); float3 lightVec = lightPos - i.worldPos; float3 lightDir = normalize(lightVec); float ndotl = DotClamped(i.normal, lightDir); float attenuation = 1 / (1 + dot(lightVec, lightVec)); i.vertexLightColor = unity_LightColor[0].rgb * ndotl * attenuation; #endif } 用这计算大三角形插值的镜面反射会很糟，所幸Unity提供了衰减因子unity_4LightAtten0，可帮助近似计算像素光的衰减，$ 1\\over 1+d^2a $ 一个像素光呈现的轮廓色 四个顶点光-Four Vertex Light 为了支持4个顶点光，就需要手写四次类似的代码，然后把结果加在一起。Unity提供了Shade4PointLights函数，参数需要：3个光的位置，4个顶点光的颜色，4个顶点光的衰减色，顶点世界坐标，顶点法线。 void CreateVertexLightColor(inout Interpolators i){ #if defined(VERTEXLIGHT_ON) i.vertexLightCoolr = Shade4PointLights( unity_4LightPosX0, unity_4LightPosY0, unity_4LightPosZ0, unity_LightColor[0].rgb, unity_LightColor[1].rgb, unity_LightColor[4].rgb, unity_LightColor[3].rgb, unity_4LightAtten0, i.worldPos, i.normal ); #endif } Shade4PointLights源码。不同的是计算光的方向 和 方向的摸，rsqrt平方根的倒数 // Used in ForwardBase pass: Calculates diffuse lighting from 4 point lights, with data packed in a special way. float3 Shade4PointLights ( float4 lightPosX, float4 lightPosY, float4 lightPosZ, float3 lightColor0, float3 lightColor1, float3 lightColor2, float3 lightColor3, float4 lightAttenSq, float3 pos, float3 normal) { // to light vectors float4 toLightX = lightPosX - pos.x; float4 toLightY = lightPosY - pos.y; float4 toLightZ = lightPosZ - pos.z; // squared lengths float4 lengthSq = 0; lengthSq += toLightX * toLightX; lengthSq += toLightY * toLightY; lengthSq += toLightZ * toLightZ; // don't produce NaNs if some vertex position overlaps with the light lengthSq = max(lengthSq, 0.000001); // NdotL float4 ndotl = 0; ndotl += toLightX * normal.x; ndotl += toLightY * normal.y; ndotl += toLightZ * normal.z; // correct NdotL float4 corr = rsqrt(lengthSq);//平方根倒数 ndotl = max (float4(0,0,0,0), ndotl * corr); // attenuation float4 atten = 1.0 / (1.0 + lengthSq * lightAttenSq); float4 diff = ndotl * atten; // final color float3 col = 0; col += lightColor0 * diff.x; col += lightColor1 * diff.y; col += lightColor2 * diff.z; col += lightColor3 * diff.w; return col; } 4个顶点光 像素光与顶点光：在Light组件RenderMode有两个重要选项，Important将指示该light被渲染为像素光，not Important指示该light被渲染为顶点光。下图是2个顶点和2个像素光对比. 不管物体有没有处于四个顶点光范围内，其计算量不变。 Your browser does not support the video tag. Here is a link to the video file instead. 顶点和像素光对比 球谐函数-Spherical Harmonics 除了用像素光和顶点光以外，还可以用球谐函数计算支持所有光源类型. 球谐函数思想是用一个函数描述入射光在球体表面某一点的情况。通常该函数用球坐标描述，但也可以用3D坐标。这允许使用物体的法向量来采样该函数。要创建该函数，需要采样所有方向上的光照强度，然后想办法转为一个连续函数. 理想情况是在物体表面每个点都采样，但这是不现实的，这需要噪声理论算法近似模拟来完成。 首先，只能从物体本地原点角度定义该函数，这对沿物体表面变化不大的光照条件来说很好。尤其是对于小物体来说，光照要么弱要么距离远。这也意味着，这种计算方式与像素光或顶点光的情况不同. 其次，我们还需要近似模拟函数本身。这就要把任何连续函数拆解为多个不同频率的连续函数，这可能有无限多个频率函数来组合。 从基本正弦函数开始 Sine wave, $ Sin 2πx $ 增大一倍震动频率，降低振幅 双频率半振幅，$ Sin 4πx\\over 2 $ 把以上两种频率振幅函数加在一起 $ Sin 2πx + {Sin 4πx\\over 2} $ 基于上面，我们可以无限加大频率降低振幅 3倍 和 4倍 把各频率加在一起，又可组成新的更复杂的函数 4倍振幅$ \\sum{_{i=1} ^4} {Sin2\\pi ix \\over i^2} $ 本示例使用具有固定模式的规则正弦波函数. 为了用正弦波函数描述任意函数，必须调整每个频段的频率，振幅和偏移，直到获得完美的结果匹配为止。使用的频带越少，近似值的准确性就越低。 该技术用于压缩其他很多东西，例如声音和图像数据. 在我们的案例中，我们将使用它来近似计算3D照明。 该函数的最大特征体现在最低频率处，为此需要丢弃最高频率处，这也意味着会丢失一些光照的细节变化。但是当光线变化不快时问题不大，所以我们需要再次限制漫反射光照. 球谐函数频率-Spherical Harmonics Bands 最简单的方式，假设各个方向的照明都是一样的，照明颜色近似为均匀色。 第一个波段标识为$ Y_0^0 $，它由单个子函数定义，该子函数只是一个常数值。 第二个波段引入线性方向光，所有光线方向一致. 有三个函数分别用$ Y_1^{-1} $、$ Y_1^0 $、$ Y_1^1 $标识。 每个函数都包含一个法线坐标，乘以一个常数。 第三个波段更加复杂. 由五个函数，$ Y_2^{−2} $、$ Y_2^{-1} $、$ Y_2^0 $、$ Y_2^1 $、$ Y_2^2 $。这些函数是二次函数，这意味着它们包含两个法线坐标的乘积。 先Unity只使用了三个波段描述球谐函数，定义在一张表内：   -2 -1 0 1 2 0     1     1   $ -y\\sqrt3 $ $ z\\sqrt3 $ $ −x\\sqrt3 $   2 $ xy\\sqrt{15} $ $ −yz\\sqrt{15} $ $ (3z^2−1){\\sqrt{5}\\over2} $ $ −xz\\sqrt{15} $ $ (x^2−y^2){\\sqrt{15}\\over 2} $ 什么决定了这个函数的形状？ 表的索引用Y表示，$ Y_i^j(i∈(0,2), j∈(-2,2)) $ $ Y_i^j $从何而来? 球面谐波是拉普拉斯方程在球面上的一个解. 数学是相当复杂的。函数的定义是$ Y_m^l=K_l^me^{imφ}P_l^{| m|}cosθ,l∈N, –l ≤ m ≤ l $ 而P_l^m项是勒让德多项式和Klm项是标准化常数。 这是复杂形式的定义，使用复数i和球坐标，φ和θ. 你也可以使用它的一个真实的版本，用三维坐标计 算这就引出了我们使用的函数。 最终的结果是把所有九项计算后加在一起，简化后$ a + by + cz + dx + exy + fyz + gz^2 + hxz + i(x^2−y^2) $，其中a到i是常数因子. float t = i.normal.x; return t &gt; 0 ? t : float4(1, 0, 0, 1) * -t; t=1 t=x,y,z t=xy,yz,zz,xz,xx-yy 实际运用球谐函数-Using Spherical Harmonics Unity提供了现成的27数字可供使用，定义在_UnityShadervariables.cginc_文件中的7个half4变量. // SH lighting environment half4 unity_SHAr; half4 unity_SHAg; half4 unity_SHAb; half4 unity_SHBr; half4 unity_SHBg; half4 unity_SHBb; half4 unity_SHC;View Code 同时_UnityCG.cginc_也提供了ShadeSH9球谐函数。 // ShadeSH9源码实现 // normal should be normalized, w=1.0 half3 SHEvalLinearL0L1 (half4 normal) { half3 x; // Linear (L1) + constant (L0) polynomial terms x.r = dot(unity_SHAr,normal); x.g = dot(unity_SHAg,normal); x.b = dot(unity_SHAb,normal); return x; } // normal should be normalized, w=1.0 half3 SHEvalLinearL2 (half4 normal) { half3 x1, x2; // 4 of the quadratic (L2) polynomials half4 vB = normal.xyzz * normal.yzzx; x1.r = dot(unity_SHBr,vB); x1.g = dot(unity_SHBg,vB); x1.b = dot(unity_SHBb,vB); // Final (5th) quadratic (L2) polynomial half vC = normal.x*normal.x - normal.y*normal.y; x2 = unity_SHC.rgb * vC; return x1 + x2; } // normal should be normalized, w=1.0 // output in active color space half3 ShadeSH9 (half4 normal) { // Linear + constant polynomial terms half3 res = SHEvalLinearL0L1 (normal); // Quadratic polynomials res += SHEvalLinearL2 (normal); #ifdef UNITY_COLORSPACE_GAMMA res = LinearToGammaSpace (res); #endif return res; } 在片元函数直接返回球谐光照，并关闭所有light组件. float3 shColor = ShadeSH9(float4(i.normal, 1)); return float4(shColor, 1); 物体不再是纯黑色了，选取了环境光颜色 关闭环境光的球谐着色 当场景里有大于light pixel count数量的光，多余的光会被计算为球谐光。如果不够，就会只采样环境光着色 多余的光用球谐着色 像计算顶点光一样，把球谐光照数据加到漫反射间接光之上，同时确保不要提供负数值. UnityIndirect CreateIndirectLight (Interpolators i) { UnityIndirect indirectLight; indirectLight.diffuse = 0; indirectLight.specular = 0; #if defined(VERTEXLIGHT_ON) indirectLight.diffuse = i.vertexLightColor; #endif indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); return indirectLight; } float4 MyFragmentProgram (Interpolators i) : SV_TARGET { // float3 shColor = ShadeSH9(float4(i.normal, 1)); // return float4(shColor, 1); return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir, CreateLight(i), CreateIndirectLight(i) ); } 2个important 6个not important 由于球谐光是不重要的光，我们也像计算顶点光一样在base pass通道计算，但是不能跟顶点光使用同一个关键字，需要独立定义FORWARD_BASE_PASS关键字. #if defined(FORWARD_BASE_PASS) indirectLight.diffuse += max(0, ShadeSH9(float4(i.normal, 1))); #endif 三种着色：像素、顶点、球谐 球谐采样Skybox 关闭所有的光，使用默认天空盒. 这时开始渲染天空盒，它是基于主方向光程序化生成的天空盒。由于没有激活light，光就像在地平线附近徘徊，同时物体选取了天空盒颜色着色，有那么点微妙变化。这是球谐函数作用的结果。物体突然变得更亮了!因为环境因素的影响非常大。程序skybox代表的是一个完美的晴天. 在这种情况下，白色的表面会显得非常明亮。这种效果在伽马空间渲染时是最强的。在现实生活中并没有很多完全白色的表面，它们通常要暗得多。 左有球谐函数，右无" }, { "title": "Unity 基础光照(翻译四)", "url": "/posts/Unity_First_light/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-03 08:00:00 +0800", "content": "本篇摘要 将法线从对象空间转换为世界空间。 使用方向光。 计算光的漫反射和镜面反射。 调整光的能耗强度。 应用金属工作流程。 学习使用Unity的PBS算法。 法线 Normals 可见光是电磁波谱中人眼可以感知的部分，可见光谱没有精确的范围。我们可以看到电磁波谱的一部分，也就是我们所知的可见光，因为人眼睛可以检测到电磁辐射，而光谱的其余部分对我们来说是不可见的。光的单个光量子被称为光子 整个电磁频谱是多少？ 光谱被分成光谱带。从低频到高频，这些被称为无线电波、微波、红外线、可见光、紫外线、X 射线和伽马射线。 光源发出一束光，一些光会照射到物体上，一些光会从物体反射回来。如果那道反射到光最终照射到我们的眼睛或相机镜头上，那么我们就会看到这个物体。 为了能看见游戏里的物体，我们已知道模型表面各个顶点和顶点坐标，但是不知道顶点方向。为此，我们需要知道顶点法线以计算其方向。 计算网格法线 创建一个材质球和shader，创建一些cube、sphere物体并使用创建的材质。 Shader \"Custom/My First Lighting Shader\" { } 未着色物体 Unity内置的cube、sphere网格包含了顶点法线，可直接获取它们并传递给片元函数。 struct VertexData { float4 position : POSITION; float3 normal : NORMAL; float2 uv : TEXCOORD0; }; struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float3 normal : TEXCOORD1; }; Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.uv = TRANSFORM_TEX(v.uv, _MainTex); i.position = mul(UNITY_MATRIX_MVP, v.position); i.normal = v.normal; return i; } 现在能看见法线向量作为颜色输出的显示 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return float4(i.normal = 0.5 + 0.5, 1); } 法线向量作为颜色渲染 这些是直接来自网格的原始法线。立方体的面看起来是平的，因为每个面都只有四个顶点组成了两个三角面，这些顶点的法线都指向同一个方向。相比之下，球体的顶点法线都指向了不同的方向，从而产生了平滑之颜色过渡。 动态批处理 立方体法线发生了一些奇怪显示。我们希望每个立方体显示相同的颜色，不同视角下的立方体却改变了颜色。 Your browser does not support the video tag. Here is a link to the video file instead. 变色立方体 这是由动态批处理引起的。Unity规定将一定数量的网格顶点动态合并在一起，以减少绘制调用。而球体的网格数量顶点太多了导致不能合批，所以它们不会受到影响。 要合并网格，必须将它们从本地空间转换为世界空间。对象是否被批处理以及如何被合批有一方面取决于它们如何排序以进行渲染。由于这种转换也会影响法线，这就是我们看到颜色变化的原因。 这里关闭动态合批处理，先专注法线知识。 关闭动态合批 除了动态批处理，Unity还支持进行静态批处理。这对于静态几何体的工作方式不同，但也涉及到世界空间的转换。静态批处理需要预先构建，这里先跳过。 没有批处理影响的法线颜色 转换法线到世界空间 除了动态批处理之外的物体, 其他所有物体法线都是在对象空间, 但是又需要知道它们在世界空间下的方向. 所以必须将法线从物体空间转换到世界空间, 为此需要进行矩阵计算. Unity将gamebject的多次的矩阵转换结构合并成一个变换矩阵,类似这样:$O = T_1\\cdot T_2\\cdot T_3\\cdot …$， 其中T是单独变换矩阵, O是所有T的组合. Unity内置了float4 unity_objectToWolrd矩阵变量, 该矩阵定义在UnityShaderVariables.cginc文件, 将该矩阵与顶点函数内的法线相乘, 可以将其转换到世界空间. 虽然是不同空间转换但是也要保持空间内的方向一致, 所以其次坐标第四个分量设为0. Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.normal = mul(unity_ObjectToWorld, float4(v.normal, 0)); i.uv = TRANSFORM_TEX(v.uv, _MainTex); return i; } 或者，也可以仅与矩阵的3x3部分相乘, 编译后的代码最终是一样的，因为编译器将消除所有乘以常数零的东西。 i.normal = mul((float3x3)unity_ObjectToWorld, v.normal); 对象到世界空间 法线现在位于世界空间中，但有些法线颜色看起来比其他法线更亮。那是因为它们当中有被缩放过。所以我们必须在转换后对它们进行归一化。 i.normal = mul(unity_ObjectToWorld, float4(v.normal, 0)); i.normal = normalize(i.normal); 归一化法线 虽然对向量进行了归一化，但对于没有统一比例的法线，颜色看起来也有些不一致。这是因为当一个表面在某一维上被拉伸时，它的法线不会以同样的方式拉伸。 缩放x轴，顶点和法线变为$1 \\over 2$ 当缩放比例不均匀时，应将其反转为法线。法线将在归一标准化后与变形表面的形状相匹配。它对统一的比例没有影响。 缩放x轴，顶点变为$1 \\over 2$法线变为2 所以我们必须反转比例，但旋转应该保持不变。那如何去翻转缩放？ 根据\\(O = [T_1]\\cdot[T_2]\\cdot[T_3]\\cdot...\\)矩阵描述了S缩放、R旋转、P位移，而每个T也可以拆解为SRP.即\\(O=[S1R1P1]\\cdot[S_2R_2P_2]\\cdot[S_3R_3P_3]\\cdot[...]\\)成立，但是法线不需要改变位移，去掉T3；同时每个T也不需要位移，去掉P。最后简化为：\\(O=[S_1R_1]\\cdot[S_2R_2]\\) 目标是翻转S，所以object-to-world矩阵: \\(N = S^{-1}_1R_1 \\cdot S^{-1}_2R_2\\) 而Unity提供了world-to-object矩阵，它是object-to-world的逆矩阵: \\(N^{-1} = O^{-1} =S_2^{-1}R_2^{-1}\\cdot S_1^{-1}R_1^{-1}\\) 这个\\(O^{-1}\\)同时把旋转和变换顺序也翻转了，需要对\\(O^{-1}\\)进行转置消除对旋转的影响: \\((O^{-1})^T =(S_2^{-1}R_2^{-1}\\cdot S_1^{-1}R_1^{-1})^T = N\\) 推导：$R^T = R^{-1}$。 $sin(-z) = -sin z$, $cos(-z) = cos z$ [\\begin{bmatrix} Cosz &amp; -Sinz &amp; 0 Sinz &amp; Cosz &amp; 0 0 &amp; 0 &amp; 0 \\end{bmatrix}^T = \\begin{bmatrix} Cosz &amp; SinZ &amp; 0 -Sinz &amp; Cosz &amp; 0 0 &amp; 0 &amp; 0 \\end{bmatrix} = \\begin{bmatrix} Cosz &amp; -Sinz &amp; 0 Sinz &amp; Cosz &amp; 0 0 &amp; 0 &amp; 0 \\end{bmatrix}^{-1}] $O^{-1}=R_2^{-1}S_2^{-1}\\cdot R_1^{-1}S_1^{-1}=R_2^TS_2^{-1}\\cdot R_1^TS_1^{-1}.$ [(O^{−1})^T=(S_1^{−1})^T(R_1^T)^T\\cdot (S_2^1)^T(R_2^T)^T=(S_1^{−1})^TR_1\\cdot (S_2^{−1})^TR_2.] 缩放矩阵具有单位矩阵特性\\((S^T=S)\\): [(O^{−1})^T=S_1^{−1}R_1\\cdot S_2^{−1}R_2=N] 因此，再对unity_WorldToObject矩阵转置后并与顶点法线相乘。 i.normal = mul( transpose((float3x3)unity_WorldToObject), v.normal ); i.normal = normalize(i.normal); 正确的世界空间法线 i.normal = mul(transpose((float3x3)unity_WorldToObject, v.normal);//这个写法行的通，只是学习一下上面的知识.汇编也很难看 //汇编-- 9: dp3 r0.x, cb1[4].xyzx, v1.xyzx 10: dp3 r0.y, cb1[5].xyzx, v1.xyzx 11: dp3 r0.z, cb1[6].xyzx, v1.xyzx 12: dp3 r0.w, r0.xyzx, r0.xyzx 13: rsq r0.w, r0.w 14: mul o2.xyz, r0.wwww, r0.xyzx //汇编-- i.normal = mul(unity_ObjectToWorld, v.normal);//简化写法 Unity也提供了更好的函数：unityObjectToWorldNormal Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.normal = UnityObjectToWorldNormal(v.normal); i.uv = TRANSFORM_TEX(v.uv, _MainTex); return i; } UnityObjectToWorldNormal内部实现： // Transforms normal from object to world space inline float3 UnityObjectToWorldNormal( in float3 norm ) { // Multiply by transposed inverse matrix, // actually using transpose() generates badly optimized code return normalize( unity_WorldToObject[0].xyz * norm.x + unity_WorldToObject[1].xyz * norm.y + unity_WorldToObject[2].xyz * norm.z ); } 再次归一化法线 ReNormalsed 在顶点函数中计算出正确的法线后，传递给片元函数。然而，在不同的单位长度向量之间进行值传递，并不能确保法线的归一化。 所以我们必须在片元函数中再次对法线进行归一化。 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); return float4(i.normal = 0.5 + 0.5, 1); } 再次归一化 虽然再次归一化会产生更好的结果，但是值的误差其实通常非常小。如果在移动设备上，这一步可以省略。 夸张的错误 漫反射 Diffuse Shading 我们看到的物体本身因为它们反射了光。反射有不同方式:漫反射、镜面反射、基于物理着色，先讨论下漫反射。 漫反射发生的原因是光线不会直接从物体表面反弹，而是一部分光到达粗糙的表面后弥射开，另一部光穿透物体，在物体内部游走然后再次离开表面。这里我们先不用完全遵循现实世界的物理细节。 从表面漫反射多少光取决于光线照射它的角度。大多数光在正面撞击表面时以0°角反射。随着这个角度的增加，反射的光量随之会减少。在90°时，不再有光线照射到表面，因此是黑暗的。漫射光的量与光的方向和表面法线之间的夹角的余弦成正比。这被称为兰伯特余弦定律。 $Diffuse = Albedo \\cdot lightColor \\cdot DotClamped(lightDir, normal)$ 漫反射 我们可以通过计算表面法线和光方向的点积来确定这个朗伯反射系数。我们已经知道法线，但还不知道光的方向。在代码内给定一个直接从物体头顶上方照射的方向。 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); return dot(float3(0, 1, 0), i.normal); } 从上方照亮，左伽马 vs. 右线性。 什么是点积？ 1 在几何上定义为 $A \\cdot B = |A| \\cdot |B| \\cdot \\cos \\theta$ 点积表示两向量间角度的余弦值乘以两向量的长度，所以在两个单位位向量的情况下 $A \\cdot B = \\cos \\theta = A \\cdot B = |A| \\cdot |B| \\cdot \\cos \\theta$ 2 在代数上定义为 $A \\cdot B = \\sum_{i=1}^n A_i B_i = A_1 B_1 + A_2 B_2 + … + A_n B_n$ 这意味着可以通过将所有组件相乘后并将它们相加来计算它。 float dotProduct = v1.x * v2.x + v1.y * v2.y + v1.z * v2.z; 从视觉上看，这个操作将一个向量直接投射到另一个向量。仿佛在上面投下了阴影, 最终得到一个直角三角 形，其底边的长度是点积的结果。如果两个向量都是单位长度，那就是它们角度的余弦值。 点积 约束负数光照 Clamped Lighting 当表面朝向光时计算点积有效，但当它远离光时则无效。这样做是防止物体表面被从后面来的光源照亮。当表面朝向光计算点积才是有意义的；当物体表面处于自己的阴影面是不需要接受光照的；当光的方向与法线的方向大于90°时点积的结果是负数。使用max函数 return max(0, dot(float3(0, 1, 0), i.normal)); Unity着色器更推荐使用saturate, 这个标准函数限制值在0-1之间。 return saturate(dot(float3(0, 1, 0), i.normal)); 但是UnityStandardBRDF.cginc文件为程序定义了更方便方便的DotClamped函数。此函数执行点积结果并确保它永远不会是负数。它还包含许多其他照明函数，也包含其他有用的文件！ #include \"UnityCG.cginc\" #include \"UnityStandardBRDF.cginc\" float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); return DotClamped(float3(0, 1, 0), i.normal); } DotClamped 内部实现 unity决定saturate在针对低能力着色器硬件和针对S3时使用它会更好 inline half DotClamped (half3 a, half3 b) { #if (SHADER_TARGET &lt; 30 || defined(SHADER_API_PS3)) return saturate(dot(a, b)); #else return max(0.0h, dot(a, b)); #endif } 因为UnityStandardBRDF已经包括UnityCG和其他一些文件，我们不必再明确地包含UnityCG //#include \"UnityCG.cginc\" #include \"UnityStandardBRDF.cginc\" UnityStandardBRDF引用结构 光源 Light Source 现在在场景中新建一个的方向光，而不是硬编码的光方向。默认情况下，每个Unity场景都需要有一个方向光。方向光被认为是无限远的，它的所有光线都来自完全相同的方向。当然，这在现实生活中是不正确的，但是太阳离地球太远了，光的方向近似平行的。 方向光 UnityShaderVariables定义float4 _WorldSpaceLightPos0，其中包含当前灯光的位置和光线来自的方向，在定向光的情况下，它有四个分量。因为它们是齐次坐标，所以我们的定向光的第四个分量是不需要的。 float3 lightDir = _WorldSpaceLightPos0.xyz; return DotClamped(lightDir, i.normal); 灯光模式 Light Mode 在产生正确的结果之前，我们必须告诉Unity我们想要使用哪些光照数据。我们通过添加一个LightMode变量标记到我们的着色器通道。 需要哪种光照模式取决于我们如何渲染场景。我们可以使用正向或延迟渲染路径，还有两种较旧的渲染模式。可以通过渲染设置选择渲染路径。它位于色彩空间选择的正上方。我们正在使用前向渲染，这是默认设置。 渲染通道 Pass { Tags { \"LightMode\" = \"ForwardBase\" } CGPROGRAM ENDCG } Your browser does not support the video tag. Here is a link to the video file instead. 漫反射光 光照颜色 Light Color 光并不总是白色的。每个光源都有自己的颜色，我们可以通过fixed4 _LightColor0获取, 它定义在UnityLightingCommon.cginc文件 是什么fixed4？ 这些是低精度数字关键字，在移动设备上以精度换取速度。在台式机上，fixed只是float. 精度优化是以后的主题。 _LightColor0变量表示光的颜色乘以光的强度：首先它是有rgba颜色值，同时光组件有一个Intensity强度属性，会改变颜色值的大小。 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); float3 lightDir = _WorldSpaceLightPos0.xyz; float3 lightColor = _LightColor0.rgb; float3 diffuse = lightColor = DotClamped(lightDir, i.normal); return float4(diffuse, 1); } 光的颜色 反照率 Albedo 大多数物体材料会吸收一部分光，这成了它们的颜色。物体的漫反射率的颜色被称为反照率，它描述了多少红色、绿色、蓝色通道被漫反射，其余的颜色被吸收不反射。我们使用材料的纹理和色调定义它。而Albedo带有白化whiteness的含义，它作为因子控制物体由暗到亮 。 无法逃脱的光会发生什么？ 光的能量被储存在物体中，通常以热量的形式存在。这就是为什么黑色的东西往往比白色的东西更温暖。 材料漫反射率的颜色称为其反照率。Albedo在拉丁语中是白色的意思。所以它描述了有多少红色、绿色和蓝色通道被漫反射。其余的被吸收。我们可以使用材质的纹理和色调来定义它。 Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Albedo\", 2D) = \"white\" {} } float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); float3 lightDir = _WorldSpaceLightPos0.xyz; float3 lightColor = _LightColor0.rgb; float3 albedo = tex2D(_MainTex, i.uv).rgb = _Tint.rgb; float3 diffuse = albedo = lightColor = DotClamped(lightDir, i.normal); return float4(diffuse, 1); } albedo-inspector.png 在伽马和线性空间中使用反照率进行漫反射着色 高光(镜面)反射 Specular Shading 除了漫反射，还有镜面反射。当光线在撞击表面后没有扩散而是直接反射，光线从表面反射回来的角度等于它撞击表面的角度。这就是导致您在镜子中看到的反射的原因。表面需要极其光滑。 观察者的位置对镜面反射很重要：仅当最终反射出的光朝向观察者是可见的，其他都看不见。所以，我们要知道从表面一点到观察者的方向，这就要求表面点和摄像机的世界位置。 struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float3 normal : TEXCOORD1; float3 worldPos : TEXCOORD2;//add } Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.worldPos = mul(unity_ObjectToWorld, v.position);//add i.normal = UnityObjectToWorldNormal(v.normal); i.uv = TRANSFORM_TEX(v.uv, _MainTex); return i; } 可以通过访问摄像机的位置float3 _WorldSpaceCameraPos定义在UnityShaderVariables.cginc文件，我们发现视图方向从中减去表面位置并进行归一化。 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); float3 lightDir = _WorldSpaceLightPos0.xyz; float3 viewDir = normalize(_WorldSpaceCameraPos - i.worldPos);//add float3 lightColor = _LightColor0.rgb; float3 albedo = tex2D(_MainTex, i.uv).rgb = _Tint.rgb; float3 diffuse = albedo = lightColor = DotClamped(lightDir, i.normal); return float4(diffuse, 1); } 反射光照颜色 Reflecting Light 要知道反射光的去向，我们可以使用标准reflect函数。它接受入射光线的方向和基于表面法线反射光线，我们要反向调整光的方向。 float3 reflectionDir = reflect(-lightDir, i.normal);//add return float4(reflectionDir = 0.5 + 0.5, 1); 反射方向 reflect内部实现 $D\\cdot N\\cdot D - 2\\cdot N\\cdot (N\\cdot D)$ 假设物体表面极其光滑，我们将只能在表面角度合适的地方看见反射光，在其他地方反射光对观察者不可见并呈现黑色。但实际上物体表面是不平整的，有太多细微的凹凸，这也意味着表面法线差别很大。 尽管我们的观察方向不完全匹配反射方向，我们仍能看见一些反射光。当我们偏离反射方向越多，我们能看见的反射光就越少，所以我们继续约束dot点积值 return DotClamped(viewDir, reflectionDir); 镜面反射 光滑度 Smoothness 这种效果产生的高光大小取决于材料的粗糙度。光滑的材料更好地聚焦光线，因此它们的高光较小。我们可以通过将其设为材质属性来控制这种平滑度。它通常定义为 0 到 1 之间的值，所以让我们将其设为滑块。 Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {} _Smoothness (\"Smoothness\", Range(0, 1)) = 0.5//add } float _Smoothness;//add 我们使用smoothness作为因子，通过提高点积的幂次来缩小光点。但是必须要比1大得多才具有更好的效果。再给因子提高100倍。 return pow(DotClamped(viewDir, reflectionDir), _Smoothness * 100); 看着像玉 Blinn-Phong光照公式 上面使用的是Blinn reflection计算公式，但业界更常用Blinn-Phong reflection公式。通过使用一个光照方向和视野方向的半角方向，然后再取法向量和半角向量的点积结果来决定镜面反射。 float3 reflectionDir = reflect(-lightDir, i.normal); float3 halfVector = normalize(lightDir + viewDir); return pow( DotClamped(halfVector, i.normal), _Smoothness = 100 ); Blinn-Phong反射 这种方法会产生更大范围的高光，但这可以通过使用更高的平滑度值来抵消。尽管这两种方法仍然是近似值。一个很大的限制是它可以为从后面照亮的对象产生无效的高光。 specular-error.png 当使用低平滑度值时，这些伪影会变得很明显。它们可以通过使用阴影来隐藏，或者通过基于光照角度淡出高光来隐藏。Unity 的传统着色器也有这个问题，所以我们也不用担心。无论如何，我们很快就会转向另一种照明方法。 高光颜色 Specular Color 镜面反射的颜色与光源的颜色相匹配,所以增加颜色渲染 float3 halfVector = normalize(lightDir + viewDir); float3 specular = lightColor = pow(//add DotClamped(halfVector, i.normal), _Smoothness = 100 ); return float4(specular, 1);//add 反射的颜色也取决于材料，这与反照率不同。金属的反照率往往很小，同时具有很强的且通常是彩色的镜面反射率。相比之下，非金属往往具有明显的反照率，而它们的镜面反射率较弱且未着色。 我们可以添加纹理和色调来定义镜面反射颜色，我们不要打扰另一种纹理，只需使用色​​调即可。 Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Albedo\", 2D) = \"white\" {} _SpecularTint (\"Specular\", Color) = (0.5, 0.5, 0.5) _Smoothness (\"Smoothness\", Range(0, 1)) = 0.1 } float4 _SpecularTint; float _Smoothness; float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float3 halfVector = normalize(lightDir + viewDir); float3 specular = _SpecularTint.rgb = lightColor = pow( DotClamped(halfVector, i.normal), _Smoothness = 100 ); return float4(specular, 1); } 我们可以使用颜色属性来控制镜面反射的着色和强度 有色镜面反射 漫反射和镜面反射 Diffuse and Specular 漫反射和镜面反射是照明中的两个难点部分。我们将它们加在一起，使我们的画面更加完整。 return float4(diffuse + specular, 1); 漫反射加镜面反射 gama vs. linear 能量守恒 Energy Conservation 将漫反射和镜面反射加在一起是有问题的，结果可能比光源更亮。当使用低平滑度的全白高光时，这一点非常明显。 很亮的镜面，0.1平滑度 当光照射到表面时，它的一部分光会作为镜面光反射回来，其余部分要么会穿透表面，要么会作为漫射光返回，要么被吸收。但我们目前没有考虑到这一点。相反，我们的光既反射又漫射。 我们必须确保材质的漫反射和镜面反射部分的总和不能超过1。这保证了我们不会凭空创造光。如果总数小于1就可以了。那意味着部分光被吸收了。 由于我们使用的是恒定的镜面反射色调，我们可以通过将其乘以1减去镜面反射来简单地调整反照率色调。但是手动执行此操作很不方便，特别是如果我们想使用特定的反照率色调。所以让我们在着色器中执行此操作。 float3 albedo = tex2D(_MainTex, i.uv).rgb * _Tint.rgb; albedo == 1 - _SpecularTint.rgb;//add 不再那么亮 漫反射和镜面反射的贡献现在是相关联的。高光越强，漫反射部分越暗。黑色镜面色调产生零反射，只会看到全强度的反照率。白色镜面色调会形成完美的镜子，因此完全消除了反照率。 Your browser does not support the video tag. Here is a link to the video file instead. 能量守恒 非金属单色 Monochrome 在这种色调模型下，当Specular色调是灰度图时这中方法工作良好。其他颜色就会有奇怪的结果，例如红色Specular色调只会减少漫反射的红色部分(其他颜色被吸收了)。 红色镜面反射，青色反照率 为了防止这种着色，我们可以使用单色能量守恒。这只是意味着我们使用镜面反射颜色的最强分量来降低反照率。 To prevent this coloration, we can use monochrome energy conservation. This just means that we use the strongest component of the specular color to reduce the albedo. albedo *= (1 - max(_SpecularTint.r, max(_SpecularTint.g, _SpecularTint.b))); 单色能量守恒 辅助函数 Unity有一个实用函数来处理能量守恒EnergyConservationBetweenDiffuseAndSpecular，定义在UnityStandardUtils.cginc #include \"UnityStandardBRDF.cginc\" #include \"UnityStandardUtils.cginc\" UnityStandardBRDF引用结构 此函数将反照率和镜面反射颜色作为输入，并输出调整后的反照率和输出 $1 - \\text{reflectivity}$。第三个输出参数是是一个减去镜面反射强度，我们拿来乘以反照率的因子。 float3 albedo = tex2D(_MainTex, i.uv).rgb = _Tint.rgb; albedo *= (1 - max(_SpecularTint.r, max(_SpecularTint.g, _SpecularTint.b))) float oneMinusReflectivity; albedo = EnergyConservationBetweenDiffuseAndSpecular( albedo, _SpecularTint.rgb, oneMinusReflectivity ); EnergyConservationBetweenDiffuseAndSpecular内部实现 它具有三种模式：无保护、单色或彩色。这些由#define语句控制。默认为单色。 half SpecularStrength(half3 specular) { #if (SHADER_TARGET &lt; 30) // SM2.0: instruction count limitation // SM2.0: simplified SpecularStrength // Red channel - because most metals are either monochrome // or with redish/yellowish tint return specular.r; #else return max(max(specular.r, specular.g), specular.b); #endif } // Diffuse/Spec Energy conservation inline half3 EnergyConservationBetweenDiffuseAndSpecular ( half3 albedo, half3 specColor, out half oneMinusReflectivity ) { oneMinusReflectivity = 1 - SpecularStrength(specColor); #if !UNITY_CONSERVE_ENERGY return albedo; #elif UNITY_CONSERVE_ENERGY_MONOCHROME return albedo * oneMinusReflectivity; #else return albedo * (half3(1, 1, 1) - specColor); #endif } 金属工作流程 Metallic Workflow 基本上有两种我们关心的材料。金属和非金属，后者也称为介电材料。目前，我们可以通过使用强烈的镜面反射色调来创建金属。我们可以通过使用弱单色镜面反射来创建电介质。这是镜面反射工作流程。 如果我们可以在金属和非金属之间切换会简单得多。由于金属没有反照率，我们可以使用该颜色数据来代替它们的镜面色调。而且非金属无论如何都没有彩色镜面反射，所以我们根本不需要单独的镜面反射色调。这被称为金属工作流程。 我们可以使用另一个滑块属性作为金属切换，以替换镜面反射色调。通常把它设置为0或1，因为某物要么是金属，要么不是金属。介于两者之间的值表示混合了金属和非金属成分的材料。 Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Albedo\", 2D) = \"white\" {} //_SpecularTint (\"Specular\", Color) = (0.5, 0.5, 0.5) _Metallic (\"Metallic\", Range(0, 1)) = 0 _Smoothness (\"Smoothness\", Range(0, 1)) = 0.1 } //float4 _SpecularTint; float _Metallic; float _Smoothness; 金属滑块 现在我们可以从反照率和金属属性中推导出镜面色调。然后可以简单地将反照率乘以 $1 - \\text{metallic}$。 float3 specularTint = albedo * _Metallic; float oneMinusReflectivity = 1 - _Metallic; // albedo = EnergyConservationBetweenDiffuseAndSpecular( // albedo, _SpecularTint.rgb, oneMinusReflectivity~ // ); albedo *= oneMinusReflectivity; float3 diffuse = albedo * lightColor * DotClamped(lightDir, i.normal); float3 halfVector = normalize(lightDir + viewDir); float3 specular = specularTint * lightColor * pow( DotClamped(halfVector, i.normal), _Smoothness = 100 ); 即使是纯电介质也有一些镜面反射,因此镜面反射强度和反射值与金属滑块的值不完全匹配,这也受到色彩空间的影响。幸运的是，UnityStandardUtils.cginc定义了DiffuseAndSpecularFromMetallic功能函数。 float3 specularTint; // = albedo = _Metallic; float oneMinusReflectivity; // = 1 - _Metallic; //albedo *= oneMinusReflectivity; albedo = DiffuseAndSpecularFromMetallic( albedo, _Metallic, specularTint, oneMinusReflectivity ); Your browser does not support the video tag. Here is a link to the video file instead. 金属工作流 DiffuseAndSpecularFromMetallic内部实现 它使用了half4 unity_ColorSpaceDielectricSpecUnity根据颜色空间设置的变量。 inline half OneMinusReflectivityFromMetallic(half metallic) { // We'll need oneMinusReflectivity, so // 1-reflectivity = 1-lerp(dielectricSpec, 1, metallic) // = lerp(1-dielectricSpec, 0, metallic) // store (1-dielectricSpec) in unity_ColorSpaceDielectricSpec.a, then // 1-reflectivity = lerp(alpha, 0, metallic) // = alpha + metallic=(0 - alpha) // = alpha - metallic = alpha half oneMinusDielectricSpec = unity_ColorSpaceDielectricSpec.a; return oneMinusDielectricSpec - metallic = oneMinusDielectricSpec; } inline half3 DiffuseAndSpecularFromMetallic ( half3 albedo, half metallic, out half3 specColor, out half oneMinusReflectivity ) { specColor = lerp(unity_ColorSpaceDielectricSpec.rgb, albedo, metallic); oneMinusReflectivity = OneMinusReflectivityFromMetallic(metallic); return albedo = oneMinusReflectivity; } 有一个细节是金属滑块本身应该在伽马空间中，但是在线性空间中渲染时，Unity不会自动校正这个值。我们可以使用该Gamma属性告诉Unity它还应该对我们的金属滑块应用伽马校正。 [Gamma]_Metallic (\"Metallic\", Range(0, 1)) = 0 经过一番调整之后，镜面反射对于非金属来说变得相当模糊，效果不好。为了改善这一点，我们需要一种更好的方法来计算光照。 基于物理着色 Physically-Based Shading Blinn-Phong方案长期以来一直是游戏行业的主流方案，但现在基于物理的着色（称为 PBS）风靡一时，因为它更加真实和可预测。 Unity 的标准着色器也使用 PBS 方法。Unity 实际上有多种实现。它根据目标平台、硬件和 API 级别决定使用哪个。该算法可通过UNITY_BRDF_PBS宏访问，该宏定义在UnityPBSLighting.cginc,BRDF代表双向反射率分布函数。 #include \"UnityStandardBRDF.cginc\" #include \"UnityStandardUtils.cginc\" #include \"UnityPBSLighting.cginc\" 引用层次结构 UNITY_BRDF_PBS内部实现 UNITY_PBS_USE_BRDF1默认情况下由 Unity 设置，作为平台定义。这将选择最佳着色器，除非着色器目标低于 3.0。 // Default BRDF to use: #if !defined (UNITY_BRDF_PBS) // allow to explicitly override BRDF in custom shader // still add safe net for low shader models, // otherwise we might end up with shaders failing to compile #if SHADER_TARGET &lt; 30 #define UNITY_BRDF_PBS BRDF3_Unity_PBS #elif UNITY_PBS_USE_BRDF3 #define UNITY_BRDF_PBS BRDF3_Unity_PBS #elif UNITY_PBS_USE_BRDF2 #define UNITY_BRDF_PBS BRDF2_Unity_PBS #elif UNITY_PBS_USE_BRDF1 #define UNITY_BRDF_PBS BRDF1_Unity_PBS #elif defined(SHADER_TARGET_SURFACE_ANALYSIS) // we do preprocess pass during shader analysis and we dont // actually care about brdf as we need only inputs/outputs #define UNITY_BRDF_PBS BRDF1_Unity_PBS #else #error something broke in auto-choosing BRDF #endif #endif UnityPBSLighting的引用结构 这里跳过pbs详细算饭介绍，使用就好了。PBS仍然计算漫反射和镜面反射，与Blinn-Phong不同点在于PBS又一个菲涅耳反射分量计算。为了在以掠射角查看对象时获得反射，就要先确保能获取环境反射。 Unity的BRDF函数返回一个RGBA颜色，alpha分量总是设置为1，所以我们可以直接让我们的片段程序返回它的结果。 // float3 diffuse = albedo * lightColor * DotClamped(lightDir, i.normal); // float3 halfVector = normalize(lightDir + viewDir); // float3 specular = specularTint * lightColor * pow( // DotClamped(halfVector, i.normal), // _Smoothness * 100 // ); return UNITY_BRDF_PBS(); UNITY_BRDF_PBS有八个参数，前两个是漫反射和镜面反射 return UNITY_BRDF_PBS( albedo, specularTint ); 第三第四个参数必须是反射率和粗糙度。这些参数必须是减一的形式，这是一种优化。我们已经使用oneMinusReflectivity计算出来DiffuseAndSpecularFromMetallic。而平滑度是粗糙度的反义词，所以我们可以直接使用它。 return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness ); 第五第六个参数是表面法线和观察方向 return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir ); 最后两个参数是直接光和间接光 光源结构 Light Structures UnityLightingCommon定义了一个简单的UnityLight结构，Unity着色器使用它来传递光照数据。它包含光的颜色、方向和一个ndotl值：漫反射。 得到这些光源信息把它放到光源结构中，并将它作为第七个参数传递。 UnityLight light; light.color = lightColor; light.dir = lightDir; light.ndotl = DotClamped(i.normal, lightDir); return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir, light ); 最后一个参数是间接光。我们必须为此使用UnityIndirect结构，它也定义在UnityLightingCommon. 它包含两种颜色：漫反射和镜面反射。漫反射颜色代表环境光，而镜面反射颜色代表环境反射。 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { i.normal = normalize(i.normal); float3 lightDir = _WorldSpaceLightPos0.xyz; float3 viewDir = normalize(_WorldSpaceCameraPos - i.worldPos); float3 lightColor = _LightColor0.rgb; float3 albedo = tex2D(_MainTex, i.uv).rgb = _Tint.rgb; float3 specularTint; float oneMinusReflectivity; albedo = DiffuseAndSpecularFromMetallic( albedo, _Metallic, specularTint, oneMinusReflectivity ); UnityLight light; light.color = lightColor; light.dir = lightDir; light.ndotl = DotClamped(i.normal, lightDir); UnityIndirect indirectLight; indirectLight.diffuse = 0; indirectLight.specular = 0; return UNITY_BRDF_PBS( albedo, specularTint, oneMinusReflectivity, _Smoothness, i.normal, viewDir, light, indirectLight ); } nonmetal-gamma.png,nonmetal-linear.png 非金属和金属，gama vs. linear" }, { "title": "Unity 多纹理融合(翻译三)", "url": "/posts/Unity_Combine_Texture/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-02 20:00:00 +0800", "content": "本篇摘要： 采样多个纹理 应用细节纹理 处理线性空间中的颜色 使用 splat 地图 纹理合并 贴图在游戏应用广泛，但它们有局限性。无论以何种尺寸显示，它们都有固定数量的像素。如果需要被渲染到很小网格，可以使用mipmap来保持它们的部分细节。但是当渲染到很大的网格上，会变得模糊。我们也不能无中生有地渲染更多额外的细节。本文讨论了一些解决办法。 细节纹理 通常可以使用更大的纹理，意味着更多的像素和更多的细节。但是纹理的大小也是有限制的，取决于游戏包体大小和目标平台的内存，以及gpu采样能力。 另一种增加像素密度的方法是平铺纹理。出一张尽可能小的贴图，设置为重复模式。近距离观察下重复感可能不会很明显。毕竟当你站着用鼻子接触墙壁时，你只会看到整面墙壁的一小部分。 因此，我们能够通过拉伸与平铺纹理相结合的方式来尽可能地添加细节。为了尝试这一点，我们使用一张棱角明显的纹理。这是一个方格图，放入的工程内使用默认导入设置。 略微扭曲的网格纹理 新建一个纹理融合shader Shader \"Custom/Textured With Detail\" { Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {} } SubShader { } } 使用此着色器创建一个新材质，然后为其指定该shader和网格纹理。 网格纹理 将材质分配给quad并查看它。从远处看效果还行。但是靠得太近看会变得模糊不清。缺失一些细节，同时纹理压缩造成的伪影也会变得很明显。 网格特写，显示低纹素密度和 DXT1 伪影。 多个纹理样本 带有低像素密度和DXT1伪影 多张纹理贴图采样 现在我们只采样了单个纹理样本并将其用作片段着色器的结果，将采样的颜色存储在一个临时变量中。 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float4 color = tex2D(_MainTex, i.uv) * _Tint; return color; } 先假设可以通过引入平铺纹理的方式来增加像素密度。执行一次纹理采样函数，给它一个十倍采样面积，用这个结果替换临时存储的颜色原来的颜色输出到屏幕。 float4 color = tex2D(_MainTex, i.uv) * _Tint; color = tex2D(_MainTex, i.uv * 10); return color; 屏幕上会产生很多小网格。靠的很近再观察，结果不那么糟糕了。因为采样纹理用了平铺10次，所以很明显是一个重复的图案。 硬编码平铺。 平铺纹理 请注意，此时我们正在执行两个纹理采样，但最终却只使用其中一个。这似乎很浪费。是吗？看看编译的顶点程序。 uniform sampler2D _MainTex; in vec2 vs_TEXCOORD0; layout(location = 0) out vec4 SV_TARGET0; vec2 t0; void main() { t0.xy = vs_TEXCOORD0.xy * vec2(10.0, 10.0); SV_TARGET0 = texture(_MainTex, t0.xy); return; } SetTexture 0 [_MainTex] 2D 0 ps_4_0 dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw dcl_temps 1 0: mul r0.xy, v0.xyxx, l(10.000000, 10.000000, 0.000000, 0.000000) 1: sample o0.xyzw, r0.xyxx, t0.xyzw, s0 2: ret 是否注意到编译后的代码中只有一个纹理采样？没错，编译器为我们去掉了不必要的代码！编译器基本上会丢弃任何最终未使用的内容。 我们不想丢弃原始采样到颜色，就要合并两次采样结果。让我们通过将它们相乘来做到这一点。再添加一个_Tint属性，叠加一层自定义颜色。 float4 color = tex2D(_MainTex, i.uv) * _Tint; color *= tex2D(_MainTex, i.uv); return color; 着色编译器会生成什么样的代码呢，对此有何影响？ uniform sampler2D _MainTex; in vec2 vs_TEXCOORD0; layout(location = 0) out vec4 SV_TARGET0; mediump vec4 t16_0; lowp vec4 t10_0; void main() { t10_0 = texture(_MainTex, vs_TEXCOORD0.xy); t16_0 = t10_0 * t10_0; SV_TARGET0 = t16_0 * _Tint; return; } SetTexture 0 [_MainTex] 2D 0 ConstBuffer \"Globals\" 144 Vector 96 [_Tint] BindCB \"Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw dcl_temps 1 0: sample r0.xyzw, v0.xyxx, t0.xyzw, s0 1: mul r0.xyzw, r0.xyzw, r0.xyzw 2: mul o0.xyzw, r0.xyzw, cb0[6].xyzw 3: ret 这次的纹理采样，编译器检测到重复对_MainTex采样代码。对其进行优化后纹理只采样一次，结果存储在寄存器中并重复使用。即使使用_Tint中间变量等，编译器也足够聪明，可以检测到此类代码重复。最终将所有结果汇总后输出。 现在再对UV坐标平铺×10次，最终看到大网格和小网格的融合在一起 color *= tex2D(_MainTex, i.uv * 10); 平铺纹理 由于纹理采样时参数不再相同，编译器也必须保留两次采样。 uniform sampler2D _MainTex; in vec2 vs_TEXCOORD0; layout(location = 0) out vec4 SV_TARGET0; vec4 t0; lowp vec4 t10_0; vec2 t1; lowp vec4 t10_1; void main() { t10_0 = texture(_MainTex, vs_TEXCOORD0.xy); t0 = t10_0 * _Tint; t1.xy = vs_TEXCOORD0.xy * vec2(10.0, 10.0); t10_1 = texture(_MainTex, t1.xy); SV_TARGET0 = t0 * t10_1; return; } SetTexture 0 [_MainTex] 2D 0 ConstBuffer \"Globals\" 144 Vector 96 [_Tint] BindCB \"Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw dcl_temps 2 0: sample r0.xyzw, v0.xyxx, t0.xyzw, s0 1: mul r0.xyzw, r0.xyzw, cb0[6].xyzw 2: mul r1.xy, v0.xyxx, l(10.000000, 10.000000, 0.000000, 0.000000) 3: sample r1.xyzw, r1.xyxx, t0.xyzw, s0 4: mul o0.xyzw, r0.xyzw, r1.xyzw 5: ret 单独的细节纹理 将两个纹理相乘时，结果会更暗。除非至少其中一种纹理是白色的。这是因为像素的每个颜色通道都有一个介于 0 和 1 之间的值。当向纹理添加细节时，可以通过该通道值来实现变暗或变亮。 要使原始纹理变亮，给原始颜色乘2，使得每个颜色值都增大。 color *= tex2D(_MainTex, i.uv * 10) * 2; 增亮颜色 这种直接扩大倍数的做法很粗暴。我们知道任何数乘以1不变，但是对细节纹理色加倍时，但对于1/2这个分界值就有用了。颜色区间是0-1，低于1/2的值将是结果变暗，高于1/2的值将变亮。这里引入一张特殊的灰度细节纹理来处理。 细节灰度图 灰度细节纹理？ 一般都是用灰度细节纹理来增白或加深原始颜色做二次细节调整，不是灰度图跳出 的颜色不是那么直观的结果。 要使用这个单独的细节纹理，我们必须在着色器中添加第二个纹理属性。使用灰色作为默认值，因为这不会改变主纹理的外观。 Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {} _DetailTex (\"Detail Texture\", 2D) = \"gray\" {} } 将细节纹理分配给我们的材质并将其平铺设置为10。 两种纹理 我们必须添加变量来访问细节纹理及其平铺和偏移数据 sampler2D _MainTex, _DetailTex; float4 _MainTex_ST, _DetailTex_ST; 使用两个UV 我们应该使用细节纹理的平铺和偏移数据，而不是使用硬编码乘10。 struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float2 uvDetail : TEXCOORD1; } 通过使用主纹理uv对细节纹理进行采样，得到一个新的细节纹理uv。 Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = TRANSFORM_TEX(v.uv, _MainTex); i.uvDetail = TRANSFORM_TEX(v.uv, _DetailTex); return i; } 再一次看看汇编代码 uniform vec4 _Tint; uniform vec4 _MainTex_ST; uniform vec4 _DetailTex_ST; in vec4 in_POSITION0; in vec2 in_TEXCOORD0; out vec2 vs_TEXCOORD0; out vec2 vs_TEXCOORD1; vec4 t0; void main() { t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1]; t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0; t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0; gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0; vs_TEXCOORD0.xy = in_TEXCOORD0.xy * _MainTex_ST.xy + _MainTex_ST.zw; vs_TEXCOORD1.xy = in_TEXCOORD0.xy * _DetailTex_ST.xy + _DetailTex_ST.zw; return; } Vector 112 [_MainTex_ST] Vector 128 [_DetailTex_ST] ConstBuffer \"UnityPerDraw\" 352 Matrix 0 [glstate_matrix_mvp] BindCB \"Globals\" 0 BindCB \"UnityPerDraw\" 1 vs_4_0 dcl_constantbuffer cb0[9], immediateIndexed dcl_constantbuffer cb1[4], immediateIndexed dcl_input v0.xyzw dcl_input v1.xy dcl_output_siv o0.xyzw, position dcl_output o1.xy dcl_output o1.zw dcl_temps 1 0: mul r0.xyzw, v0.yyyy, cb1[1].xyzw 1: mad r0.xyzw, cb1[0].xyzw, v0.xxxx, r0.xyzw 2: mad r0.xyzw, cb1[2].xyzw, v0.zzzz, r0.xyzw 3: mad o0.xyzw, cb1[3].xyzw, v0.wwww, r0.xyzw 4: mad o1.xy, v1.xyxx, cb0[7].xyxx, cb0[7].zwzz 5: mad o1.zw, v1.xxxy, cb0[8].xxxy, cb0[8].zzzw 6: ret 注意两个 UV 输出是如何在两个编译器顶点程序中定义的。OpenGLCore使用vs_TEXCOORD0和vs_TEXCOORD1输出，相反Direct3D11只使用一个输出o1. // Output signature: // // Name Index Mask Register SysValue Format Used // -- -- -- -- - // SV_POSITION 0 xyzw 0 POS float xyzw // TEXCOORD 0 xy 1 NONE float xy // TEXCOORD 1 zw 1 NONE float zw 上面代码意味着两个 UV 对都被打包到一个输出寄存器中。第一个在 X 和 Y 通道，第二个在 Z 和 W 通道。因为寄存器总是由四个数字组成的组。Direct3D 11 编译器利用了这一点。 试着手动打包输出? 手动打包输出的常见原因是只有少数几个插值器可用。Shader Model 2硬件支持8个通用插补 器，而Shader Model 3硬件支持10个。复杂着色器可能会遇到这个限制。 现在我们可以在片段程序中使用额外的UV对。 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float4 color = tex2D(_MainTex, i.uv) * _Tint; color *= tex2D(_DetailTex, i.uvDetail) * 2; return color; } uniform vec4 _Tint; uniform vec4 _MainTex_ST; uniform vec4 _DetailTex_ST; uniform sampler2D _MainTex; uniform sampler2D _DetailTex; in vec2 vs_TEXCOORD0; in vec2 vs_TEXCOORD1; layout(location = 0) out vec4 SV_TARGET0; vec4 t0; lowp vec4 t10_0; lowp vec4 t10_1; void main() { t10_0 = texture(_MainTex, vs_TEXCOORD0.xy); t0 = t10_0 * _Tint; t10_1 = texture(_DetailTex, vs_TEXCOORD1.xy); t0 = t0 * t10_1; SV_TARGET0 = t0 + t0; return; } SetTexture 0 [_MainTex] 2D 0 SetTexture 1 [_DetailTex] 2D 1 ConstBuffer \"Globals\" 144 Vector 96 [_Tint] BindCB \"Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_sampler s0, mode_default dcl_sampler s1, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_resource_texture2d (float,float,float,float) t1 dcl_input_ps linear v0.xy dcl_input_ps linear v0.zw dcl_output o0.xyzw dcl_temps 2 0: sample r0.xyzw, v0.xyxx, t0.xyzw, s0 1: mul r0.xyzw, r0.xyzw, cb0[6].xyzw 2: sample r1.xyzw, v0.zwzz, t1.xyzw, s1 3: mul r0.xyzw, r0.xyzw, r1.xyzw 4: add o0.xyzw, r0.xyzw, r0.xyzw 5: ret 基于细节纹理，主纹理变得更亮和更暗。 明暗两张纹理 细节纹理渐变融合 添加细节的想法是它们可以在近距离或放大时改善材质的外观。它们不应该在远处可见或缩小，因为这会使平铺变得明显。所以我们需要一种方法来随着纹理显示尺寸的减小而淡化细节。我们可以通过将细节纹理淡化为灰色来做到这一点，因为这不会导致颜色变化。 需要做的就是在细节纹理的导入设置中启用Fadeout Mip Maps属性。这也会自动将过滤器模式切换为三线性，以便渐变为灰色是渐进的。 纹理过渡 网格从详细到不详细的过渡非常明显，但通常不会注意到它。例如，这里是大理石材质的主纹理和细节纹理。 大理石纹理 一旦我们的材质使用了这些纹理，细节纹理的过渡痕迹就不再明显了。 大理石材质 然而，由于细节纹理的过渡加持，大理石材质在近距离看起来要好得多。 没有细节和有细节特写 线性色彩空间 当我们在 gamma 颜色空间中渲染场景时，我们的着色器工作正常，但如果我们切换到线性颜色空间，它就会出错。色彩空间在项目中设置。它配置在Other Settings播放器设置面板，可以通过Edit / Project Settings / Player. 选择颜色空间 什么是 Gamma 和 Linear 色彩空间？ 在计算机图形学中，Gamma 和 Linear 是两种处理光照和颜色的不同方式，理解它们的区别对于渲染真实感画面至关重要。 1. 为什么会有 Gamma 校正？ 这源于早期的 CRT（阴极射线管）显示器。CRT 显示器的物理特性决定了它的输入电压和输出亮度不是线性的。如果你给它 50% 的电压，它并不会产生 50% 的亮度，而是大约 21.8% 的亮度（$0.5^{2.2}$）。这种关系呈现出一条向下弯曲的曲线，称为 Gamma 2.2 曲线。 为了让显示器输出正确的亮度，我们需要在把图像存入文件时，预先进行一次反向的增亮处理（$0.5^{1/2.2} \\approx 0.73$），这个过程叫 Gamma Encoding。这样，当显示器再次以 2.2 的幂次压暗图像时，原本的增亮和显示器的压暗相互抵消，最终人眼就能看到正确的、线性的光照结果。 此外，人眼对暗部细节比对亮部细节更敏感。sRGB 标准利用了这一点，通过 Gamma Encoding 这种非线性存储方式，在有限的 8 位（0-255）数据中，把更多的数据位分配给了暗部。这是一种极其高效的数据压缩方式。 gamma $1\\over 2.2$ encoding vs. $2.2$ deconding 2. Gamma Space vs Linear Space Gamma Space（伽马空间）： 工作流：这是旧时代的流程。贴图是 sRGB 的（经过 Gamma 编码），渲染计算（光照、混合）直接使用这些 sRGB 数值，最后直接输出到显示器。 问题：数学上是错误的。光照计算（如 Light * Albedo）假设数值是线性的（1+1=2），但在 Gamma 空间中，0.5 并不代表一半的物理亮度。这会导致光照衰减不自然、混合颜色发黑、高光过曝等问题。 Linear Space（线性空间）： 工作流：这是现代标准流程。 输入：读取 sRGB 贴图时，GPU 自动去除 Gamma 编码，将其还原为线性数值（Linear）。 计算：Shader 中的所有光照、混合计算都在线性空间进行，符合物理规律。 输出：最终输出到屏幕前，再次进行 Gamma 校正（Encoding），以适应显示器的特性。 优势：光照计算符合物理规律，光影过渡自然，是 PBR（基于物理的渲染）的基础。 3. 游戏开发中的选择与统一 选择哪种空间？ Linear Space 是目前游戏开发的绝对主流。PC、主机和中高端移动设备都首选线性空间，因为它能提供更真实的光照效果。 Gamma Space 仅在针对极低端老旧设备或某些特定的非写实 2D 游戏中保留。 如何保持统一性？ 在 Unity 中，通过 Project Settings -&gt; Player -&gt; Other Settings -&gt; Color Space 选择 Linear。 颜色贴图（Albedo/Diffuse）：Unity 默认会将纹理标记为 sRGB。在 Linear 模式下，Unity 会在采样时自动移除 Gamma，将其转为线性值供 Shader 使用。 数值贴图（Normal/Metallic/Roughness）：这些纹理存储的是数学数据而非颜色，不需要 Gamma 校正。必须在纹理导入设置中取消勾选 “sRGB (Color Texture)”，确保 Shader 读取到的是未经修改的原始数值。 回到我们的例子，当切换到 Linear 空间后，细节纹理变暗了。这是因为我们原本的算法（* 2）是基于 Gamma 空间的经验公式。在线性空间中，原本的中性灰（Gamma 0.5）被还原为线性值（约为 0.217），乘以 2 后仅为 0.434，远小于 1，所以变暗了。 为了修复这个问题，我们需要在线性空间中使用正确的数学转换，或者使用 Unity 提供的 unity_ColorSpaceDouble 变量，它会自动根据当前色彩空间提供正确的倍增系数（Linear 空间下约为 4.59，Gamma 空间下为 2），从而保证渲染结果的一致性。 color *= tex2D(_DetailTex, i.uvDetail) * unity_ColorSpaceDouble; 通过这种更改，无论我们在哪个颜色空间中渲染，我们的细节材质看起来都一样。 纹理splat过渡遮罩 细节纹理的一个限制是对整个表面使用相同的细节。 这适用于均匀的表面，如大理石板。但是，如果材质没有统一的外观，不希望在任何地方都使用相同的细节。 考虑一个大地形。它可以有草、沙、岩石、雪等。希望这些地形类型有一定的细节。但是覆盖整个地形的纹理永远不会有足够的细节。可以通过为每种表面类型使用单独的纹理并平铺这些纹理来解决该问题。但是怎么知道在哪里使用哪种纹理呢？ 假设我们有一个具有两种不同表面类型的地形，什么时候决定使用哪种表面纹理呢。不是一就是二。我们可以用一个布尔值来表示这个逻辑。如果设置为true，我们使用第一个纹理，否则使用第二个。我们可以使用灰度纹理来存储这个选择。值1表示第一个纹理，而值0表示第二个纹理。事实上，我们可以使用这些值在两个纹理之间进行线性插值。然后介于 0 和 1 之间的值表示两种纹理之间的混合，这使得平滑过渡成为可能。 这样的纹理被称为splat贴图。就像将多个地形特征喷溅到画布上一样。由于插值，这张地图甚至不需要高分辨率。 splat遮罩贴图 将其添加到项目后，将其导入类型切换为高级。启用Bypass sRGB Sampling并指示其mipmap应在Linear Space中生成。因为该纹理不需要sRGB颜色。所以在线性空间中渲染时不应该进行转换。另外，将其 Wrap Mode 设置为clamp，因为我们不会平铺这张地图。 splat导入设置 创建一个新的 Texture Splatting 着色器。 Shader \"Custom/Texture Splatting\" { Properties { MainTex (\"Splat Map\", 2D) = \"white\" {} } SubShader { Pass { CGPROGRAM #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"UnityCG.cginc\" sampler2D _MainTex; float4 _MainTex_ST; struct VertexData { float4 position : POSITION; float2 uv : TEXCOORD0; } struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; } Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = TRANSFORM_TEX(v.uv, _MainTex); return i; } float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return tex2D(_MainTex, i.uv); } ENDCG } } } 创建一个新材质并引用该shader，并将splat贴图指定为其主要纹理。 splat图渲染 增加融合纹理 为了能够在两个纹理之间进行选择，作为属性添加命名为Texture1和Texture2到我们的着色器中。 Properties { _MainTex (\"Splat Map\", 2D) = \"white\" {} _Texture1 (\"Texture 1\", 2D) = \"white\" {} _Texture2 (\"Texture 2\", 2D) = \"white\" {} } 可以为他们使用任何你想要的纹理,这里使用网格纹理和大理石纹理。 增加的额外纹理 为添加到着色器的每个纹理修改平铺和偏移控制值。这需要我们将更多数据从顶点传递到片段着色器，或者在片元着色器中计算UV调整。 这很好，但通常地形的所有纹理都平铺相同。并且 splat 地图根本没有平铺。所以我们只需要一个平铺和偏移控件的实例。 可以将属性控制添加到着色器属性之前，就像在C#代码中一样。NoScaleOffset将禁用纹理平铺和偏移。 Properties { _MainTex (\"Splat Map\", 2D) = \"white\" {} [NoScaleOffset] _Texture1 (\"Texture 1\", 2D) = \"white\" {} [NoScaleOffset] _Texture2 (\"Texture 2\", 2D) = \"white\" {} } 同时修改splat贴图tiling为4。 不需要额外的贴图纹理 将采样器变量添加到我们的着色器代码中，但是不必添加它们对应的 _ST 变量。 sampler2D _MainTex; float4 _MainTex_ST; sampler2D _Texture1, _Texture2; 对两张纹理采样后叠加，颜色会得到加深，然后输出 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return tex2D(_Texture1, i.uv) + tex2D(_Texture2, i.uv); } 纹理叠加 使用splat贴图 采样splat纹理需要顶点程序提供的UV坐标 struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; float2 uvSplat : TEXCOORD1; }; Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = TRANSFORM_TEX(v.uv, _MainTex); i.uvSplat = v.uv; return i; } 然后，以在对其他纹理进行采样之前对splat贴图进行采样。 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { float4 splat = tex2D(_MainTex, i.uvSplat); return tex2D(_Texture1, i.uv) + tex2D(_Texture2, i.uv); } 因为splat本身是单通道，可以任选一个RGB通道来来存储值，这里先决定使用第一个纹理与splat贴图R通道相乘。 return tex2D(_Texture1, i.uv) * splat.r + tex2D(_Texture2, i.uv); 调制第一个纹理 第一个纹理现在由splat贴图R通道调制。为了完成插值，我们必须将另一个纹理1-R相乘。 return tex2D(_Texture1, i.uv) * splat.r + tex2D(_Texture2, i.uv) * (1 - splat.r); 调制两个纹理 RGB Splat贴图 我们有一个功能性的splat材质，但它只支持两种纹理。我们可以支持更多吗？我们现在只使用了R通道，那么我们添加G和B通道怎么样？那么(1,0,0)代表第一个纹理，(0,1,0)代表第二个纹理，(0,0,1)代表第三个纹理。 为了在这三个之间获得正确的插值，我们只需要确保RGB通道的总和为1即可。 但是等等，当我们只使用一个通道时，我们可以支持两个纹理。这是因为第二个纹理的权重是通过1-R得出的。同样的技巧适用于任意数量的通道。因此可以通过1-R-G-B支持另一种纹理。 这导致了一个具有三种颜色和黑色的splat贴图。只要三个通道加起来不超过1，它就是一个有效的贴图。这里给出一张这样的贴图，导入Unity。 RGB splat 贴图 当 R + G + B 超过1时会发生什么？ 那么前三个纹理的组合会太强。 同时，第四个纹理将被减去而不是被添加。 如果错误很小，那么不会注 意到并且结果足够好。 示例 RGB 映射实际上并不完美，但不会注意到。 纹理压缩引入了更多错误，但 同样难以察觉。 可以使用alpha通道吗？ 确实可以！ 这意味着单个 RGBA splat 贴图最多可以支持五种不同的地形类型。 但是对于本教程，四个 就足够了。 如果要使用超过五个纹理，则必须使用多个splat贴图。虽然这是可能的，但最终会得到很多纹理采样 此时可以使用更好的技术，例如纹理数组。 为了支持 RGB splat贴图，我们必须在着色器中添加两个额外的纹理。为它们分配了大理石细节和测试纹理。 Properties { _MainTex (\"Splat Map\", 2D) = \"white\" {} [NoScaleOffset] _Texture1 (\"Texture 1\", 2D) = \"white\" {} [NoScaleOffset] _Texture2 (\"Texture 2\", 2D) = \"white\" {} [NoScaleOffset] _Texture3 (\"Texture 3\", 2D) = \"white\" {} [NoScaleOffset] _Texture4 (\"Texture 4\", 2D) = \"white\" {} } Four textures 将所需的变量添加到着色器。 再一次，没有额外的 _ST需要的变量。 sampler2D _Texture1, _Texture2, _Texture3, _Texture4; 在片段程序中，添加额外的纹理样本。第二个样本现在使用G通道，第三个使用B通道。最终样本用 (1 - R - G - B) 调制。 return tex2D(_Texture1, i.uv) * splat.r + tex2D(_Texture2, i.uv) * splat.g + tex2D(_Texture3, i.uv) * splat.b + tex2D(_Texture4, i.uv) * (1 - splat.r - splat.g - splat.b); 四个纹理飞溅 为什么混合区域在线性色彩空间中看起来不同？ 我们的 splat 贴图绕过了 sRGB 采样，所以混合不应该取决于我们使用的颜色空间，对吧？ splat 地图 确实不受影响。 但是发生混合的色彩空间确实发生了变化。 在伽马空间渲染的情况下，样本在伽马空间中混合，仅此而已。 但是在线性空间中渲染时，它们首先转换为 线性空间，然后混合，然后再转换回伽马空间。 结果略有不同。 在线性空间中，混合也是线性的。 但在伽 马空间中，混合偏向较深的颜色。 现在知道如何应用细节纹理以及如何将多个纹理与splat贴图混合。也可以组合这些方法。 可以将四个细节纹理添加到splat着色器并使用贴图在它们之间进行混合。当然，这需要四个额外的纹理采样，性能有限。 还可以使用贴图来控制应用细节纹理的位置以及省略的位置。在这种情况下，需要一张单色贴图，它可以用作遮罩。当单个纹理同时包含表示多个不同材质的区域但没有地形那么大的面积时，这很有用。例如，如果我们的大理石纹理还包含金属片，则不希望在此处应用大理石细节。" }, { "title": "Unity Shader 基本语法(翻译二)", "url": "/posts/Unity_Shader_Fundamentals/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-02 16:00:00 +0800", "content": "本篇摘要信息 顶点变换 Color pixels shader 属性 从顶点传数据至片元函数 查看编译后的shader代码 场景初始化 新建一个默认场景，新建一个圆球。这个默认场景本身进行了大量复杂的渲染，为了更容易的掌握Unity的渲染过程，我们先做一些简化设置，把默认的某些花里胡哨的东西先剥离掉。 剥离天空盒 打开Window-Lighting，查看光照设置选项。弹出带有3个选项卡的面板，我们先关注Scene选项卡. 默认光照 第一选项卡Environment是跟环境光照相关，在这里可以设置天空盒。这个Default-Skybox当前正被用于场景的背景光、环境光、和反射光。设置为none就能关闭这些光。顺便把下面的Realtime Ligting和Mixed Lighting也关掉，现在还用不上，后面会陆续介绍。 关闭了天空盒，环境颜色自动切换为了纯色，这个颜色默认是带着一丝蓝的黑灰色(说好的纯呢，外表很黑内心很蓝？)。而反射光会变成纯黑色。如下所示，设置后球体变暗了，背景变成了纯色。而这个背景深蓝色从哪里来的呢？ 简单光照 这个深蓝色被定义在摄像机，它默认使用天空盒渲染，当天空盒失效后场景会默认退回到使用相机纯色模式。 默认的摄像机设置 为了进一步简化渲染，再隐藏或删除方向光对象。这将消除场景中的直接光照，以及所有它投射的阴影。剩下纯色背景和球体的轮廓。 未着色球体 图像渲染 分两步绘制上面的场景，一是使用相机的背景色填充图像，然后再在上面画出球体的轮廓。 Unity如何知道该画这个球体呢?我们有一个球体对象并且绑定了 MeshRenderer 组件，如果这个球体位于摄像机的视野内，那么它就会被渲染出来。Unity通过检测球体的边界盒是否与摄像机的视锥体相机来验证这一点。包围盒在Unity中定义为 Bounds结构体 Collider.bounds, Mesh.bounds, Renderer.bounds. 球体默认自带组件 Transform组件用于更改坐标、方向，以及网格和包围盒的尺寸。这里有对Transform层次结构的清晰描述。如果一个物体最终处于摄像机视野内，它就会被安排渲染。 最后，GPU负责渲染物体的mesh。这些具体的渲染指令在物体的material定义好的，这个material引用了一个shader-GPU程序。 2u分工 当前这个球体使用了Unity的默认材质，自带了一个标准shader。我们现在把它去掉替换成自己的shader，从头开始写。 创建一个Shader 通过点击 Assets / Create / Shader / Unlit Shader 创建并命名自己的shader，双击shader文件打开，并删除里面的内容从头写. 第一个shader Shader是通过shader关键字定义，关键字是一个字符串，在下拉界面中选择时显示的也是该关键字。它不必与文件名相同。 Shader \"Unlit/MyShader\" { //... } 保存文件，回到编辑器会收到警告提示none of subshaders/fallbacks are suitable，因为它是空的，没有sub-shader或回调shader。尽管这个shader没有内容也有警告，我们仍能指定给material。点击 Assets / Create / Material 创建，然后通过下拉菜单指定。 给材质指定Shader 给球体指定上我们新建的Material，替换掉默认的。这时的球体会立即变成紫红色。发生这个的原因是Unity切换到了错误的shader，它故意使用这个颜色来提醒开发者这是一个错误。 指定MyMaterial shader warning中提到了没有sub-shader. 我们可以使用sub-shader操作shader变量进行分组, 这允许程序员为不同的编译平台提供不同的sub-shader.例如我们可以用一个sub-shader既支持pc又支持手机平台.定义一个SubShader块 Shader \"Unlit/MyShader\" { SubShader { //... } } sub-shader至少包含一个以上的pass块, pass代码块是物体实际被渲染的地方，我们先写一个pass，然后在写多个pass。为了呈现多种效果，pass数量可能会超过一个以上，而则代表着物体要被渲染多次。 Shader \"Unlit/MyShader\" { SubShader { Pass { //... } } } 我们的球体现在应该变成了白色，因为我们使用了一个空pass渲染，这也意味着我们的Shader没有出现任何错误了。 空shader效果 Shader程序 现在我们要开始编写shader代码了，我们用的Unity着色器语言是HLSL和CG着色器语言的变体。所以必须指示 CGPROGRAM 关键字为代码的开始，同时要用 ENDCG 关键字做为结束。 Pass{ CGPROGRAM ENDCG } 再次打开编辑器编译后有一个警告 Both vertex and fragment programs must be present,表示没有顶点和片元程序。shader由这两个程序组成，vertex顶点程序负责处理网格的顶点数据，这包含了从对象空间到显示空间的转换；而fragment片元程序负责为网格的三角形内的单个像素进行着色。 顶点片元函数 同时，我们必须通过pragma指令告诉编译器使用哪些程序 CGPROGRAM #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram ENDCG 编译器再次发出来错误提示，这次是因为它不能找到我们指定的程序片段，因为我们光声明没实现。首先vertex和fragment被写成方法，类似C#函数。先简单地创建两个同名的void方法。 CGPROGRAM #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram void MyVertexProgram() { } void MyFragmentProgram() { } ENDCG 这次编译后没有报错，但是球体从屏幕上消失了。 Shader汇编 Unity的shader编译器把我们的代码根据不同target-compile成了不同程序。不同的平台需要不同的解决方案，例如Direct3D是服务于Windows平台，OpenGL针对MacOs，OpenGL ES针对手机平台。这里我们不是在处理单个编译器，而是多个编译器。 最终使用哪种编译器取决于目标平台，这些编译器也是不完全相同的，每个平台可能得到不同的结果。在这个例子中，我们的空程序在OpenGL和Direct3D 11下能很好的工作，但在Direct3D 9就会报错。 在编辑器下点选 MyFirstShader,在inspector面板可以查看该shader的一些信息，以及编译错误。这也有一个 Compiled code and show 按钮和下拉菜单。 shader检视面板信息 如果你点击该按钮，Unity将会编译该shader并打开它，接着就可以查看生成的代码。我们试着先选择OpenGL Core，然后再选择D3D11，看看底层代码是怎么回事的。 Shader \"Unlit/MyShader\" { SubShader { Pass { No keywords set in this variant. -- Vertex shader for \"glcore\": Shader Disassembly: #ifdef VERTEX #version 150 #extension GL_ARB_explicit_attrib_location : require #extension GL_ARB_shader_bit_encoding : enable void main() { return; } #endif #ifdef FRAGMENT #version 150 #extension GL_ARB_explicit_attrib_location : require #extension GL_ARB_shader_bit_encoding : enable void main() { return; } #endif -- Fragment shader for \"glcore\": Shader Disassembly: // All GLSL source is contained within the vertex program } } } 提炼出两个main函数，有vertex和fragment程序 #ifdef VERTEX void main() { return; } #endif #ifdef FRAGMENT void main() { return; } #endif D3D11自行查看，因为编译后的代码实在是太长了，不方便贴上来。只选取了一个片段： Pass { No keywords set in this variant. -- Vertex shader for \"d3d11\": Shader Disassembly: vs_4_0 0: ret -- Fragment shader for \"d3d11\": Shader Disassembly: ps_4_0 0: ret } 引入其他文件 编写shader代码很费劲，有时需要重复写类似的函数，为了简化书写，这里有一个类似C#程序的功能，引用其他类中的通用变量、函数等。使用 #include 指令就能加载一个文件。先试着加载 UnityCG.cginc CGPROGRAM #pragma vertex MyVertexProgram #pragma fragment MyFragmentProgram #include \"UnityCG.cginc\" void MyVertexProgram() { } void MyFragmentProgram() { } ENDCG 下面是 UnityCg.cginc 的引用层次结构 UnityCG.cginc结构 UnityShaderVariables.cginc 定义了一大堆渲染所需的着色器变量，比如 矩阵变换、相机和光照数据等等。 UnityInstancing.cginc 内置在引擎安装包内，这是一种 减少绘制调用的特定呈现技术。虽然它不直接包含文件，但它依赖于UnityShaderVariables。 HLSLSupport.cginc 设置了一些无论您的目标是哪个平台都可以使用相同的代码的功能。 请注意，这些文件的内容将被复制到文件中，取代include指令。这发生在预处理步骤中，该步骤执行所有预处理指令。比如 #include 和 #pragma。 产生输出(输出语义) 为了渲染物体，shader必须要产生结果。 Vertex顶点函数必须要返回每个顶点的最终坐标：SV_POSITION。一个顶点有几个坐标分量？4个，因为我们使用了4x4变换矩阵。现在把函数类型从void改为float4,一个float4类型是一个由4个float类型简单组成。 float4 MyVertexProgram() : SV_POSITION { return 0; } Fragment片元函数返回像素的最终颜色：SV_TARGET。也是float4。 float4 MyFragmentProgram() : SV_TARGET { return 0; } Vertex顶点函数的输出作为Fragment片元函数的输入。输入的参数需要匹配！ float4 MyFragmentProgram( float4 position : SV_POSITION ) : SV_TARGET { return 0; } 然后看看Unity的shader汇编 //--D3D11-- -- Vertex shader for \"d3d11\": Shader Disassembly: vs_4_0 //顶点着色器版本 dcl_output_siv o0.xyzw, position //声明o0作为输出值，带有系统值 0: mov o0.xyzw, l(0,0,0,0) //把(0,0,0,0)移动到o0中 1: ret //返回 -- Fragment shader for \"d3d11\": Shader Disassembly: ps_4_0 dcl_output o0.xyzw 0: mov o0.xyzw, l(0,0,0,0) 1: ret //GL CORE-- #ifdef VERTEX void main() { gl_Position = vec4(0.0, 0.0, 0.0, 0.0); return; } #endif #ifdef FRAGMENT layout(location = 0) out vec4 SV_TARGET0; void main() { SV_TARGET0 = vec4(0.0, 0.0, 0.0, 0.0); return; } #endif 顶点变换 把球给我画出来！ 为了得到模型空间的顶点坐标，给vertex顶点函数增加一条语义POSITON。而模型空间的顶点坐标是其次坐标。先直接返回这个顶点坐标，贴汇编： //D3d11- vs_4_0 //版本 dcl_input v0.xyzw //申明v0 输入系统值 dcl_output_siv o0.xyzw, position //申明o0 输出系统值 0: mov o0.xyzw, v0.xyzw //把v0值 移动到 o0 1: ret //--GL CORE #ifdef VERTEX in vec4 in_POSITION0; void main() { gl_Position = in_POSITION0; return; } #endifView Code 扭曲的球 使用MVP：model_view_projection矩阵变换顶点坐标，该值定义在 UnityShaderVariables 文件，变量名是 UNITY_MATRIX_MVP。改为： mul 函数定义 return mul(UNITY_MATRIX_MVP, position); 贴汇编看看 -- Vertex shader for \"d3d11\": // Stats: 8 math Uses vertex data channel \"Vertex\" //cbuffers常量数据 Constant Buffer \"UnityPerDraw\" (160 bytes) on slot 0 { Matrix4x4 unity_ObjectToWorld at 0 } Constant Buffer \"UnityPerFrame\" (384 bytes) on slot 1 { Matrix4x4 unity_MatrixVP at 272 } Shader Disassembly: //版本 vs_4_0 //声明常量缓冲区cbuffers，逐字索引 dcl_constantbuffer CB0[4], immediateIndexed //cbuffers dcl_constantbuffer CB1[21], immediateIndexed //声明输入v0 dcl_input v0.xyz //声明输入o0 dcl_output_siv o0.xyzw, position //声明临时寄存器2个(r0-r1) dcl_temps 2 //将v0与cb0[1]相乘传递给r0 0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw /*第0行的计算步骤 dest.x = cb0[0].x * v0.x + r0.x; dest.y = cb0[0].y * v0.x + r0.y; dest.z = cb0[0].z * v0.x + r0.z; dest.w = cb0[0].w * v0.x + r0.w;*/ //r0 = cb0 * v0 + r0 1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw //同理1： 2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw //r0 = r0 + cb0 3: add r0.xyzw, r0.xyzw, cb0[3].xyzw //r1 = r0 * cb1 4: mul r1.xyzw, r0.yyyy, cb1[18].xyzw 5: mad r1.xyzw, cb1[17].xyzw, r0.xxxx, r1.xyzw //同理1： 6: mad r1.xyzw, cb1[19].xyzw, r0.zzzz, r1.xyzw //同理1： 7: mad o0.xyzw, cb1[20].xyzw, r0.wwww, r1.xyzw //同理1： 8: retView Code 正确的球 像素颜色 先给Fragment函数返回点东西， float4 MyFragmentProgram(float4 position : SV_POSITION) : SV_TARGET { return float4(1, 1, 0, 1); } 黄色球 使用Shader属性Properties 需要在pass块内声明一个同类型的同命名变量float4 _Tint; float4 MyFragmentProgram(float4 position : SV_POSITION) : SV_TARGET{ return _Tint; } 看看片元函数的汇编 -- Fragment shader for \"d3d11\": Constant Buffer \"Globals\" (48 bytes) on slot 0 { Vector4 _Tint at 32 } Shader Disassembly: ps_4_0 dcl_constantbuffer CB0[3], immediateIndexed dcl_output o0.xyzw 0: mov o0.xyzw, cb0[2].xyzw 1: retView Code 纯色球 从顶点到片元 上图纯色球，每个像素都是同一个颜色，但是美术给的效果图是五彩斑斓的， 就需要GPU光栅化三角形，取三个处理过的顶点进行插值，找到三角形内所有像素并着色。 shader程序执行流程 处理过的顶点数据不直接传递给Fragment片元函数，而在片元函数中访问插值本地数据，需要增加一个参数，并指定语义 TEXCOORD0,它表示贴图的UV坐标。 float4 MyVertexProgram ( float4 position: POSITION, out float3 localPosition : TEXCOORD0 ) : SV_POSITION{ localPosition = position.xyz; return UnityObjectToClipPos(position); } float4 MyFragmentProgram ( float4 position : SV_POSITION, float3 localPosition : TEXCOORD0 ) : SV_TARGET { return float4(localPosition, 1); } 插值本地数据作为颜色 结构体 简化传递Fragment函数的参数，新建一个结构体 struct Interpolators{ float4 position : SV_POSITION; float3 localPosition : TEXCOORD0; }; Interpolators MyVertexProgram (float4 position: POSITION ){ Interpolators i; i.localPosition = position.xyz; i.position = UnityObjectToClipPos(position); return i; } float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return float4(i.localPosition, 1); } UnityObjectToClipPos 是 Unity5.6 之后的优化： 它对应 mul(UNITY_MATRIX_MVP, v.vertex)，但是该函数使用了常数1作为第四个坐标而不是 依赖网格数据，源码: inline float4 UnityObjectToClipPosInstanced(in float3 pos) { float4 w = mul(unity_ObjectToWorldArray[unity_InstanceID], float4(pos, 1.0) return mul(UNITY_MATRIX_VP, w)); } 因为通过网格提供的数始终为1，但是编译器不能知晓。所幸干脆就直接写死为1.0，优化掉运行 时再去计算第四个数到底是多少这一步。 调整颜色 因为负颜色被约束限制为零，我们的球体最终变得相当暗。 球体的自身半径为 $1\\over 2$,因此颜色通道最终介于 $-{1\\over 2}$ 和 $1\\over 2$ 之间。我们希望将它们移动到 0-1 范围内，我们可以通过向所有通道添加 $1\\over 2$ 来实现。 return float4(i.localPosition + 0.5, 1); 再看看汇编代码 uniform vec4 _Tint; in vec3 vs_TEXCOORD0; layout(location = 0) out vec4 SV_TARGET0; vec4 t0; void main() { t0.xyz = vs_TEXCOORD0.xyz + vec3(0.5, 0.5, 0.5); t0.w = 1.0; SV_TARGET0 = t0 * _Tint; return; } ConstBuffer \"$$Globals\" 128 Vector 96 [_Tint] BindCB \"$$Globals\" 0 ps_4_0 dcl_constantbuffer cb0[7], immediateIndexed dcl_input_ps linear v0.xyz dcl_output o0.xyzw dcl_temps 1 0: add r0.xyz, v0.xyzx, l(0.500000, 0.500000, 0.500000, 0.000000) 1: mov r0.w, l(1.000000) 2: mul o0.xyzw, r0.xyzw, cb0[6].xyzw 3: ret 调整之后的颜色 纹理 如果想在模型网格上不添加更多三角形面数的情况下为网格添加更多更明显的细节和多样性呈现，可以使用纹理投影到网格的三角形上。 纹理坐标用于控制投影，下图是2D坐标。不管纹理的实际纵横比如何，其水平坐标称为U，垂直坐标称为V，它们通常称为UV坐标。 uv坐标图 U坐标从左到右递增，起始点为0终点为1。 V坐标从下到上增加，除了Direct3D它从上到下。 使用UV坐标 Unity的默认网格具有适合纹理映射的UV坐标。顶点程序可以通过参数访问它们 TEXCOORD0 语义。然后传递给片元函数使用。 struct Interpolators { float4 position : SV_POSITION; float2 uv : TEXCOORD0; }; Interpolators MyVertexProgram (VertedData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = v.uv; return i; } float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return float4(i.uv, 1, 1); } 再看看汇编 in vec4 in_POSITION0; in vec2 in_TEXCOORD0; out vec2 vs_TEXCOORD0; vec4 t0; void main() { t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1]; t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0; t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0; gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0; //将UV坐标从顶点数据复制到Interpolators输出 vs_TEXCOORD0.xy = in_TEXCOORD0.xy; return; } Bind \"vertex\" Vertex Bind \"texcoord\" TexCoord0 ConstBuffer \"UnityPerDraw\" 352 Matrix 0 [glstate_matrix_mvp] BindCB \"UnityPerDraw\" 0 vs_4_0 dcl_constantbuffer cb0[4], immediateIndexed dcl_input v0.xyzw dcl_input v1.xy dcl_output_siv o0.xyzw, position dcl_output o1.xy dcl_temps 1 0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw 1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw 2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw 3: mad o0.xyzw, cb0[3].xyzw, v0.wwww, r0.xyzw 4: mov o1.xy, v1.xyxx//将UV坐标从v1.xy传递到o1.xy 5: ret Unity将UV坐标包裹在其球体周围，在球体两极处图像的顶部和底部。图像的左侧和右侧接缝连接在一起。 沿着该接缝，UV坐标值从0到1。 球体缝隙 添加纹理 要使用纹理，必须添加另一个着色器属性。 常规纹理属性的类型是2D ，因为还有其他类型的纹理。 默认值是引用 Unity 的默认纹理之一的字符串，可以是white 、 black 或 gray 。 主纹理命名约定是 _MainTex。 这也使您可以使用方便的 Material.mainTexture属性以通过脚本访问它。 Properties { _Tint (\"Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Texture\", 2D) = \"white\" {} } “white” {} 这个花括号有什么用？ 上古时代开发的固定功能着色器曾经需要纹理设置，这些设置被放在这些括号内，但现在它们不再使用 了。即使它们现在已经无用，着色编译器仍然需要它们，如果忽略它们会产生错误。 选中材质，查看inspector信息 纹理_white显示 通过使用 sampler2D 类型为变量来访问着色器中的纹理。 float4 _Tint; sampler2D _MainTex; 通过在片段程序中使用tex2D函数，完成对纹理UV坐标进行采样。 float4 MyFragmentProgram (Interpolators i) : SV_TARGET { return tex2D(_MainTex, i.uv); } 再查看汇编后的shader代码 uniform sampler2D _MainTex; in vec2 vs_TEXCOORD0; layout(location = 0) out vec4 SV_TARGET0; void main() { SV_TARGET0 = texture(_MainTex, vs_TEXCOORD0.xy); return; } SetTexture 0 [_MainTex] 2D 0 ps_4_0 dcl_sampler s0, mode_default dcl_resource_texture2d (float,float,float,float) t0 dcl_input_ps linear v0.xy dcl_output o0.xyzw 0: sample o0.xyzw, v0.xyxx, t0.xyzw, s0 1: ret 带纹理球体 球体两极附近会显得非常杂乱。 为什么会这样？ 发生纹理失真是因为插值在三角形中是线性的。 Unity 的球体在两极附近只有几个三角形，其中UV坐标失真最大。所以UV坐标在三角形与三角形的顶点之间是非线性变化，但在三角形内部的顶点之间的变化是线性的。所以 球体两极纹理中的直线在三角形边界处突然改变方向。 跨三角形插值 不同的网格具有不同的 UV 坐标，从而产生不同的映射。 Unity 的默认球体使用经纬度纹理映射，而网格是低分辨率立方体球体。 这足以进行测试，但您最好使用自定义球体网格以获得更好的结果。 平铺和偏移 为着色器添加纹理属性后，材质检查器不仅显示了纹理字段。它还显示了平铺和偏移控制。但是，更改这些 2D 向量目前没有任何效果。 这些额外的纹理数据存储在材质中，也可以由着色器访问。 可以通过与关联材料同名的变量加上 _ST 后缀来执行此操作。这个变量的类型必须是float4. sampler2D _MainTex; float4 _MainTex_ST; 平铺向量用于缩放纹理，因此默认为 $(1, 1)$。 它存储在变量的XY部分。 要使用它只需将它与UV 坐标相乘。 这可以在顶点着色器或片段着色器中完成。 在顶点着色器中这样做是有意义的，只为每个顶点而不是每个像素执行乘法。 Interpolators MyVertexProgram (VertexData v) { Interpolators i; i.position = mul(UNITY_MATRIX_MVP, v.position); i.uv = v.uv * _MainTex_ST.xy; return i; } 偏移向量用于移动纹理，并存储在变量的ZW部分中。 i.uv = v.uv * _MainTex_ST.xy + _MainTex_ST.zw; UnityCG.cginc包含一个方便的宏:TRANSFORM_TEX i.uv = TRANSFORM_TEX(v.uv, _MainTex); 纹理设置 默认的纹理设置 Wrap Mode决定了在使用0-1范围之外的UV坐标进行采样时的输出。 设置为clamped时，UV被限制在 0–1 范围内。 这意味着超出边缘的像素与位于边缘的像素相同。 设置为repeat时，UV从0-1循环。这意味着超出边缘的像素与纹理另一侧的像素相同。 Wrap Mode默认模式是重复纹理，这会导致它平铺。 在(2, 2)开始平铺 Mipmap和Filter 当纹理的像素与它们投影到网格的像素不完全匹配时会发生什么？ 存在不匹配，必须以某种方式解决。这是由Filter Mode完成如何控制。 Point (no filter) 。这意味着当在某些 UV 坐标处对纹理进行采样时，将使用最近的像素。 这将使纹理呈现块状外观，除非像素精确映射到显示像素。 因此，它通常用于像素完美的渲染，或者需要块状样式时。 bilinear filtering双线性过滤。 当纹理在两个像素之间的某处被采样时，这两个像素被插值。 由于纹理是 2D 的，这发生在 U 轴和 V 轴上。 因此是双线性过滤。 双线性过滤方法在像素密度小于显示像素密度时有效，因此当放大纹理时。结果会看起来很模糊。 当缩小纹理时，几乎不起作用。 相邻的显示像素最终会得到相距超过一个像素的样本。 这意味着将跳过部分纹理，这将导致粗糙的过渡，就像图像被锐化一样。 双线性过滤问题的解决方案是在像素密度变得太高时使用较小的纹理。 显示屏上出现的纹理越小，应使用的版本越小。 这些较小的版本称为 mipmap，unity会自动为您生成。 每个连续的 mipmap 的宽度和高度都是上一层的一半。 所以当原始纹理大小为 512x512 时，mip 贴图为 256x256、128x128、64x64、32x32、16x16、8x8、4x4 和 2x2。 mipmap 是什么？ mipmap这个词是 MIP map 的缩写。 字母 MIP 代表拉丁短语 multum in parvo ，意思 是狭小空间中的众多 。 它是由 Lance Williams 在首次描述 mipmap 技术时创造的。 mipmap上有下无 mipmap上有下无 那么在何时使用哪个mipmap级别，它们看起来有什么不同呢？ 先通过在高级纹理设置中启用Fadeout Mip Maps。启用一个淡入淡出范围后，inspector将显示滑块。它定义了一个mipmap范围，在该范围内 mipmap 将转换为纯灰色。 越向右滑动过渡级别越小。 mipmap过渡级别 mipmap过渡之间呈现出模糊到锐利的快速，过渡不自然。这可以通过将过滤器模式切换为Trilinear三线性。 这与双线性过滤的工作方式相同，但它是 在相邻的mipmap级别之间进行插值，这使得采样成本更高 ,它平滑了 mipmap 级别之间的转换。 另一种有用的技术是各向异性过滤。 当把Aniso Level设置为 0 时，纹理变得更加模糊。这与mipmap 级别的选择有关。 各向异性是什么意思？ 粗略地说，当某物在不同方向上看起来相似时，它就是各向同性的。 例如，一个无特征的立方体。 如果不是这种情况，则它是各向异性的。 例如，一个三角体，因为它的纹理朝着一个方向而不是另一个 方向。 当纹理以某个角度投影时，由于透视关系，通常最终会发现其中一个维度比另一个维度扭曲得更多。一个很好的例子是带纹理的地面。在远处，纹理的前后维度会显得比左右维度小得多。 Aniso Level 选择哪个 mipmap 级别基于最差维度。 如果差异很大，那么会得到一个在一维上非常模糊的结果。各向异性过滤通过解耦维度来缓解这种情况。除了统一缩小纹理外，它还提供在任一维度上缩放不同数量的版本。 因此，您不仅有 256x256 的 mipmap，而且还有 256x128、256x64 等的 mipmap。 没有和有Aniso Level 请注意，这些额外的 mipmap 不像常规 mipmap 那样预先生成。 相反，它们是通过执行额外的纹理样本来模拟的。 因此它们不需要更多空间，但采样成本更高。 各向异性双线性过滤，过渡到灰色 各向异性过滤的深度由 Aniso Level 控制。 为 0 时，它被禁用。 为 1 时，它变为启用并提供最小的效果。 16达到最大值。 但是，这些设置会受到项目质量设置的影响。 可以通过 Edit / Project Settings / Quality 访问质量设置。 找到 各向异性纹理 项目设置 渲染质量设置 项目设置禁用各向异性纹理时，无论纹理设置如何，都不会发生各向异性过滤。当它设置为 Per Texture时，它​​完全由纹理自身设置控制。也可以设置为 Forced On ，强制把每个纹理都开启Ansio Level，但是若纹理设置Aniso Level为0，仍然不会使用各向异性过滤。" }, { "title": "Unity 基本矩阵(翻译一)", "url": "/posts/Unity-Matrix-Transform/", "categories": "Unity3D, Shader", "tags": "Shader", "date": "2018-01-01 09:00:00 +0800", "content": "本篇摘要信息 matrix介绍 matrix推导 模拟transform缩放 旋转 位移功能 可视空间 Unity Shader是怎么知道一个像素该画在哪个位置？下面是先展示一组Cube，一步步分析下去 cube数组 操控一组3维坐标 创建一组\\(10*10*10\\)的3维Cube数组，并作为UnityMatrices对象的成员变量，接下来显示这些Cube在空间中的位置 void InitCubeArray() { for (int i =0 , z = 0; z &lt; generalCount; z++) { for (int y = 0; y &lt; generalCount; y++) { for (int x = 0; x &lt; generalCount; x++) { cubes[i++] = CreateCubesPoint(x, y, z); } } } } Transform CreateCubesPoint(int x, int y, int z) { GameObject cube = GameObject.CreatePrimitive(PrimitiveType.Cube); cube.transform.localScale = new Vector3(0.5f, 0.5f, 0.5f); cube.transform.localPosition = CreateCoordinate(x, y, z); cube.GetComponent&lt;MeshRenderer&gt;().material.color = CreateColor(x, y, z); return cube.transform; } 设置每个Cube的位置，都以(0,0,0)为原点，(10-1)*0.5为Center左右两边对称 Vector3 CreateCoordinate(int x, int y, int z) { return new Vector3( x - center, y - center, z - center ); } 然后再用自身坐标xyz分量与center的比率初始化颜色rgb。效果如上图 Color CreateColor(int x, int y, int z) { return new Color( (float)x / generalCount, (float)y / generalCount, (float)z / generalCount ); } 空间变换 positionning, rotating, and scaling Cube数组中每个元素在空间中的变换有可能会有差异，虽然每个Cube变换的细节不同，但它们都需要经过某个方法来变换到空间中的某个坐标点。为此我们可以为所有变换创建一个abstract 基类，包含一个抽象的_Applay()_成员方法，由具体的变换组件去实现这个方法。 public abstract class Transformation : MonoBehaviour { public abstract Vector3 Apply(Vector3 point); } 我们给这个UnityMatrices对象添加Transformation组件，同时检索Cube数组每个对象，将其坐标传入这个组件的_Apply()_方法进行计算得到新坐标并应用，这里始终以(0，0，0)作为每个Cube对象的原点坐标，而不能依赖其实际坐标，因为会每帧实时计算并改变。 最后我们用泛型列表存储这种一系列变换组件方便统一计算。 private void Update() { GetComponents&lt;Transformation&gt;(transformations); // for (int i = 0; i &lt; cubes.Length; i++) // { // cubes[i].localPosition = TransformPoint(cubes[i].localPosition); // } for (int i =0 , z = 0; z &lt; generalCount; z++) { for (int y = 0; y &lt; generalCount; y++) { for (int x = 0; x &lt; generalCount; x++) { cubes[i++].localPosition = TransformPoint(x, y, z); } } } } Vector3 TransformPoint(int x, int y, int z) { Vector3 coordinates = CreateCoordinate(x, y, z); for (int i = 0; i &lt; transformations.Count; i++) { coordinates = transformations[i].Apply(coordinates); } return coordinates; } 位移 现在来做第一种变换：translation位移，这很简单。首先创建一个继承自Transformation组件子类，并定义一个表示自身位置属性的变量，并实现基类的抽象方法。然后添加给Cube数组对象 public class PositionTransformation : Transformation { public Vector3 position; public override Vector3 Apply(Vector3 point) { return point + position; } } 现在可以向UnityMatrices对象添加PositionTransformation组件。这允许我们在不移动UnityMatrices对象的情况下移动数组中每个对象的坐标，所有的变换都发生在cube的局部空间。 位移 缩放 接下来做第二种变换：Scaling缩放。 public class ScaleTransformation : Transformation { public Vector3 scale = new Vector3(1, 1, 1); public override Vector3 Apply(Vector3 point) { point.x *= scale.x; point.y *= scale.y; point.z *= scale.z; return point; } } 缩放 这里有一个问题：当进行缩放时，缩放会改变每个Cube对象的position。这是因为我们先计算了空间坐标，然后才缩放的它。而Unity中Transform组件是先缩放后位移。所以正确的计算顺序是：先缩放后位移。 旋转(二维) 第三种变换：Rotation旋转。 public class RotationTransform : Transformation { public Vector3 rotation; public override Vector3 Apply(Vector3 point) { return point;//先占位 } } 旋转该如何工作呢？现在先假定在2维空间下一点P，绕Z轴旋转。Unity使用了左手坐标系，正向旋转是逆时针方向，如下图： 2维空间下绕Z轴旋转 旋转一个点坐标后会发什么吗？先简单的考虑一个以原点为中心的单位圆上的一点P，设p初始位置为(1,0)，然后再以每90°增量进行一次旋转，如下图： 0°旋转到90°和180°变化 由上图可知，点p(1,0)旋转一次(90°)变为了(0,1)，再旋转一次(180°)变为了(-1,0)，再往下旋转会变为(0,-1)，最后回到原位置(1,0). 那如果用点(0,1)作为初始位置，其变换顺序(0,1)$\\rightarrow$(-1,0)$\\rightarrow$(0,-1)$\\rightarrow$(1,0)$\\rightarrow$(0,1). 因此这个点坐标始终围绕0，1，0，-1进行循环，唯一得区别是起始点位置不同。 那如果以45°增量进行旋转呢?它会在XY平面对角线上产生一点，其坐标为($\\pm \\sqrt{1 \\over 2},\\pm \\sqrt{1 \\over 2}$)，这些点到原点的距离始终是一致的。而这个循环顺序也类似上面，是$0, \\sqrt{1 \\over 2}, 1, \\sqrt{1 \\over 2}, 0, −\\sqrt{1 \\over 2}, −1, −\\sqrt{1 \\over 2}$。如果继续减小增量值，我们就可以得到一个Sine曲线。 Sine 和 Cosine曲线 结合上面两张图，Sine曲线代表了Y分量，Cosine曲线代表了X分量，坐标用曲线表示就是$(\\cos z, \\sin z)$，若起始点为(1,0)则结果为$(\\cos z,\\sin z)$，逆时针旋转90°后(根据$\\sin (-z) = –\\sin z, \\cos (-z) = \\cos z$的对称性质)则结果为$(−\\sin z,\\cos z)$。因此我们可以用绕Z轴计算sine和cosine曲线，由于提供的是角度，但实际上sin及cos只能作用于弧度，所以我们需要转化它: public override Vector3 Apply(Vector3 point) { float radz = rotation.z * Mathf.Deg2Rad; float \\sin z = Mathf.Sin(radz); float \\cos z = Mathf.Cos(radz); return point; } 什么是弧度? 像度数一样，可以用作旋转的度量。使用单位圆时，弧度与圆周行进的距离相等。由于圆周的长度等于2π乘以圆的半径，因此1度等于2π/360 = π/180弧度。π 是圆的周长与其直径之间的比率。 上述方法对于旋转(1,0)或(0,1)或许很好，那有米有旋转任意点的方式呢？ 这些点都是由X和Y定义的，我们可以把2维点(x,y)拆分为一个公式$xX+yY$，那么$x(1,0)+y(0,1)=(x, y)$是成立的。当旋转之后，可以用$x(\\cos z, \\sin z)+y(-\\sin z, \\cos z)$来得到经过正确旋转后的点。组合为坐标就变成了$(x\\cos Z−y\\sin Z,x\\sin Z+y\\cos Z)$. public override Vector3 Apply(Vector3 point) { float radz = rotation.z * Mathf.Deg2Rad; float \\sin z = Mathf.Sin(radz); float \\cos z = Mathf.Cos(radz); //return point; return new Vector3( point.x * \\cos z - point.y * \\sin z, point.x * \\sin z + point.y * \\cos z, point.z ); } 按照上述分析的缩放、旋转、位移的先后计算顺序，再在Unity内对比Transform的拖动旋转缩放的显示，二者的效果是一致的。 void Start () { //... gameObject.AddComponent&lt;ScaleTransformation&gt;(); gameObject.AddComponent&lt;RotationTransform&gt;(); gameObject.AddComponent&lt;PositionTransformation&gt;(); } 最终变换效果 旋转完全体 - 现在我们只能绕Z轴旋转，但是为了能够复刻Unity的Transform组件那样的旋转，现在就得要支持绕X轴和绕Y轴旋转。虽然分别绕这些轴旋转与绕Z轴旋转的方法相似，但是当一次同时绕多个轴旋转时这就很复杂了。目标：一次同时绕多个轴旋转，迎难而上。 2D矩阵 现在开始，我们要把坐标书写格式由水平式替代为垂直式。把(x,y)被改写为 \\(\\begin{bmatrix} x\\\\ y\\\\ \\end{bmatrix}\\) 把$(x\\cos Z−y\\sin Z,x\\sin Z+y\\cos Z)$也同样被拆分为 \\(\\begin{bmatrix} x\\cos Z-y\\sin Z\\\\ x\\sin Z+y\\cos Z\\\\ \\end{bmatrix}\\) ，再把这个表达式进一步拆分： \\(\\begin{bmatrix} \\cos Z&amp;-\\sin Z\\\\ \\sin Z&amp;\\cos Z\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} x\\\\ y\\\\ \\end{bmatrix}\\) 这就是矩阵乘法， 2x2矩阵的第一列值代表X轴，第二列值代表了Y轴，计算公式如下： \\(\\begin{bmatrix} Xx&amp;Yx\\\\ Xy&amp;Yy\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} a\\\\ b\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} aXx + bYx\\\\ aXy + bYy\\\\ \\end{bmatrix}\\) 由于Unity是采用左手法则，在上文中单位圆上一点绕Z轴旋转的增量度不同，cos代表X轴，sin代表Y轴，在结合本文的矩阵可得 \\(\\begin{bmatrix} \\cos \\theta&amp;-\\sin \\theta\\\\ \\sin \\theta&amp;\\cos \\theta\\\\ \\end{bmatrix}\\) \\(=\\cos \\theta\\) \\(\\begin{bmatrix} 1&amp;0\\\\ 0&amp;1\\\\ \\end{bmatrix}\\) \\(+\\sin \\theta\\) \\(\\begin{bmatrix} 0&amp;-1\\\\ 1&amp;0\\\\ \\end{bmatrix}\\) 数学上定义，当两个矩阵相乘时，只有在第一个矩阵的列数（column）和第二个矩阵的行数（row）相同时才有意义。结果矩阵的每项元素等于第一个矩阵行元素与第二个矩阵列元素的乘积之和 \\(\\begin{bmatrix} 1&amp;2\\\\ 3&amp;4\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} a&amp;c\\\\ b&amp;d\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} 1a+2b&amp;1c+2d\\\\ 3a+4b&amp;3c+4d\\\\ \\end{bmatrix}\\) A矩阵 * B矩阵 = A矩阵的行 * B矩阵的列；只有当A矩阵列数 = B矩阵行数时，矩阵相乘才有效。因此结果矩阵的行数等于第一个矩阵的行，列数等于第二个矩阵的列相同。 3D矩阵 到目前为止，我们有了一个2x2阶矩阵，可以用这个矩阵来绕Z轴旋转一个2D点。但我们实际上使用的是3D坐标。若试图用这个矩阵乘法 \\(\\begin{bmatrix} \\cos Z&amp;-\\sin Z\\\\ \\sin Z&amp;\\cos Z\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ \\end{bmatrix}\\) 就是错误的，因为这两个矩阵的行与列的个数不匹配。为确保满足矩阵相乘，我们就需要填充这个第三维Z轴，先用0填充第一个矩阵第三行： \\(\\begin{bmatrix} \\cos Z&amp;-\\sin Z&amp;0\\\\ \\sin Z&amp;\\cos Z&amp;0\\\\ 0&amp;0&amp;0\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} x\\cos Z-y\\sin Z+0z\\\\ x\\sin Z+y\\cos Z+0z\\\\ 0x+0y+0z\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} x\\cos Z-y\\sin Z\\\\ x\\sin Z+y\\cos Z\\\\ 0\\\\ \\end{bmatrix}\\) 得到的结果中X轴和Y轴是正确的，但是Z轴结果总是为0。为了确保绕Z旋转而不改变Z轴的值，我们先插入一个数字1在旋转矩阵的右下角位置。简化理解，这个第三列值就是代表了Z轴： \\(\\begin{bmatrix} \\cos Z&amp;-\\sin Z&amp;0\\\\ \\sin Z&amp;\\cos Z&amp;0\\\\ 0&amp;0&amp;1\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} x\\cos Z-y\\sin Z\\\\ x\\sin Z+y\\cos Z\\\\ z\\\\ \\end{bmatrix}\\) 由数学上定义，任何矩阵与单位矩阵相乘都等于本身，单位矩阵如同乘法中的1 \\(\\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;1&amp;0\\\\ 0&amp;0&amp;1\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ \\end{bmatrix}\\) 绕X轴和Y轴的旋转矩阵推导 根据绕Z轴旋转的方式推理可以得出绕X轴和Y轴的旋转矩阵。 以绕Y轴为例，首先，X轴是以 \\(\\begin{bmatrix} 1\\\\ 0\\\\ 0\\\\ \\end{bmatrix}\\) 开始，经过逆时针旋转90°后以 \\(\\begin{bmatrix} 0\\\\ 0\\\\ -1\\\\ \\end{bmatrix}\\) 结束。那么经过旋转后的X轴可以表示为 \\(\\begin{bmatrix} \\cos Y\\\\ 0\\\\ -\\sin Y\\\\ \\end{bmatrix}\\) 而Z轴与X轴垂直，所以Z轴就是 \\(\\begin{bmatrix} \\sin Y\\\\ 0\\\\ \\cos Y\\\\ \\end{bmatrix}\\) 而Y轴始终保持不变，最后绕Y轴的旋转矩阵： \\(\\begin{bmatrix} \\cos Y&amp;0&amp;\\sin Y\\\\ 0&amp;1&amp;0\\\\ -\\sin Y&amp;0&amp;\\cos Y\\\\ \\end{bmatrix}\\) 同理绕X轴的旋转矩阵， X轴不变： \\(\\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;\\cos X&amp;-\\sin X\\\\ 0&amp;\\sin X&amp;\\cos X\\\\ \\end{bmatrix}\\) 那么就此可以得出三个矩阵： 绕X轴旋转矩阵 \\(\\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;\\cos \\alpha&amp;\\sin \\alpha\\\\ 0&amp;-\\sin \\alpha&amp;\\cos \\alpha\\\\ \\end{bmatrix}\\) 绕Y轴旋转矩阵 \\(\\begin{bmatrix} \\cos \\theta&amp;0&amp;-\\sin \\theta\\\\ 0&amp;1&amp;0\\\\ \\sin \\theta&amp;0&amp;\\cos \\theta\\\\ \\end{bmatrix}\\) 绕Z轴旋转矩阵 \\(\\begin{bmatrix} \\cos \\theta&amp;\\sin \\theta&amp;0\\\\ -\\sin \\theta&amp;\\cos \\theta&amp;0\\\\ 0&amp;0&amp;1\\\\ \\end{bmatrix}\\) 统一的旋转矩阵 通过上文我们分别得到了单独绕某个轴的旋转矩阵，现在开始我们要组合起来使用。这里的同时旋转本质上也是分步进行的，先绕Z轴旋转，然后绕Y轴，最后是绕X轴。 这里有两种算法： 第一种：先计算坐标点绕Z旋转，得出的结果坐标再计算绕Y轴旋转，再得出的结果坐标计算绕X轴旋转，最后得到最终的旋转坐标。 第二种：把每个旋转矩阵相乘得到一个最终的新的旋转矩阵，这将同时作用与三个轴旋转。首先计算Y乘Z，这个结果矩阵的第一项的值是$\\cos Y\\cos Z−0\\sin Z−0\\sin Y=\\cos Y\\cos Z$，最终矩阵 [\\begin{bmatrix} \\cos Y\\cos Z&amp;-\\cos Y\\sin Z&amp;\\sin Y \\sin Z&amp;\\cos Z&amp;0 -\\sin Y\\cos Z&amp;\\sin Y\\sin Z&amp;\\cos Y \\end{bmatrix}] 最后计算X × (Y × Z)得出最终矩阵： \\(\\begin{bmatrix} \\cos Y\\cos Z&amp;-\\cos Y\\sin Z&amp;\\sin Y\\\\ \\cos X\\sin Z+\\sin X\\sin Y\\cos Z&amp;\\cos X\\cos Z-\\sin X\\sin Y\\sin Z&amp;-\\sin X\\cos Y\\\\ \\sin X\\sin Z-\\cos X\\sin Y\\cos Z&amp;\\sin X\\cos Z+\\cos X\\sin Y\\sin Z&amp;\\cos X\\cos Y\\\\ \\end{bmatrix}\\) public Vector3 rotation;//每个分量表示角度 public int rotDelta; private void Update() { rotation = new Vector3(rotDelta, rotDelta, rotDelta); } public override Vector3 Apply(Vector3 point) { float radx = rotation.x * Mathf.Deg2Rad; float rady = rotation.y * Mathf.Deg2Rad; float radz = rotation.z * Mathf.Deg2Rad; float \\sin x = Mathf.Sin(radx); float \\cos x = Mathf.Cos(radx); float \\sin y = Mathf.Sin(rady); float \\cos y = Mathf.Cos(rady); float \\sin z = Mathf.Sin(radz); float \\cos z = Mathf.Cos(radz); Vector3 xRot = new Vector3( \\cos y * \\cos z, \\cos x * \\sin z + \\sin x * \\sin y * \\cos z, \\sin x * \\sin z - \\cos x * \\sin y * \\cos z ); Vector3 yRot = new Vector3( -\\cos y * \\sin z, \\cos x * \\cos z - \\sin x * \\sin y * \\sin z, \\sin x * \\cos z + \\cos x * \\sin y * \\sin z ); Vector3 zRot = new Vector3( \\sin y, -\\sin x * \\cos y, \\cos x * \\cos y ); return xRot * point.x + yRot * point.y + zRot * point.z; } 矩阵变换 实现一个矩阵完成缩放、旋转、位移的计算 为了实现这个目标，所以借鉴3.4旋转矩阵组合的方式，先对缩放和位移组合，即位移 x 缩放。 缩放，根据单位矩阵的性质，任何矩阵与单位矩阵相乘的结果都是本身。那么对单位矩阵进行缩放即可： \\(\\begin{bmatrix} 2&amp;0&amp;0\\\\ 0&amp;3&amp;0\\\\ 0&amp;0&amp;4\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} 2x\\\\ 3y\\\\ 4z\\\\ \\end{bmatrix}\\) 位移，不是对三个分量完全重新计算，而是在现有的坐标之上进行偏移。因此现在不能简单的重新表示为3x3阶矩阵，而是需要额外增加一列表示偏移。 \\(\\begin{bmatrix} 1&amp;0&amp;0&amp;2\\\\ 0&amp;1&amp;0&amp;3\\\\ 0&amp;0&amp;1&amp;4\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} x+2\\\\ y+3\\\\ z+4\\\\ \\end{bmatrix}\\) 但是，又由于矩阵乘法规定，第一个矩阵的列数等于第二个矩阵的行数才有意义。上图就是错误的。所以我们需要给坐标矩阵增加第四个元素，偏移矩阵增加一行。当它们增加的这个分量进行矩阵相乘时，其结果为1(我们先保留下这个数字1，以备后续使用).那就变成了4x4阶矩阵样式和一个4D点。 \\(\\begin{bmatrix} 1&amp;0&amp;0&amp;2\\\\ 0&amp;1&amp;0&amp;3\\\\ 0&amp;0&amp;1&amp;4\\\\ 0&amp;0&amp;0&amp;0\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} x+2\\\\ y+3\\\\ z+4\\\\ 1\\\\ \\end{bmatrix}\\) 根据位移矩阵，所以我们要统一用4×4的变换矩阵。缩放和旋转矩阵会额外增加一行一列，其右下角是1。所有的点都带有一个第四维坐标分量，它总是1——其次坐标。 其次坐标(Homogeneous Coordinates) 这个坐标的第四分量坐标是个啥？ 它有啥用啊？ 我们只知道上文提到位移时有用，那么缩放、旋转有用吗？ 当它的值为0，1，-1时会发生什么呢？ 有这样一个东西不叫坐标而叫向量，它可以被缩放和旋转，但不能被移动。向量描述了相对于某个点的偏移，具有方向和长度属性，没有位置属性。 它 \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\ \\end{bmatrix}\\) 表示为一个点，而它 \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 0\\\\ \\end{bmatrix}\\) 表示为一个向量。这样区分非常有用，因为我们可以使用相同的矩阵来变换一个点的位置、法线和切线。 当第四个坐标值是0或1或其他数值时会发生什么？答案是什么也不会，准确的说是没有差异。这个坐标的术语叫做其次坐标，它的意思是空间中每个点都可以用一个无穷数量坐标集和来表示。而现在普遍做法的形式是使用1作为第四个坐标值，所有其他的数字都能通过 使用整个集合乘以任意数来找到 \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} 2x\\\\ 2y\\\\ 2z\\\\ 2\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} 3x\\\\ 3y\\\\ 3z\\\\ 3\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} wx\\\\ wy\\\\ wz\\\\ w\\\\ \\end{bmatrix}\\) = \\(w\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\ \\end{bmatrix}\\) 当我们知道了一个其次坐标时，需要转为3D坐标，只需要把第四个坐标化为1，怎么做呢？没错，就是把每个坐标除以第四个坐标，然后再舍弃第四个坐标 \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ w\\\\ \\end{bmatrix}\\) =\\(1 \\over w\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ w\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} x \\over w\\\\ y \\over w\\\\ z \\over w\\\\ 1\\\\ \\end{bmatrix}\\) \\(\\rightarrow\\) \\(\\begin{bmatrix} x \\over w\\\\ y \\over w\\\\ z \\over w\\\\ \\end{bmatrix}\\) 所以当第四个坐标为0时是不能做上面的除法的，因此当第四个坐标值为0时，表示为向量，这就是为什么它们像方向一样。 使用矩阵 我们能用Unity的Matrix4x4结构体来完成矩阵乘法。从现在开始，我们将用它来代替上面的3D旋转方法。 在Transformation增加一个抽象只读属性以检索变换矩阵。 public abstract Matrix4x4 Matrix { get; } Transformation组件的Apply方法不再需要设为抽象，它将获取到矩阵并执行乘法运算。 public Vector3 Apply (Vector3 point) { return Matrix.MultiplyPoint(point); } 注意这个Matrix4x4.MultiplyPoint需要一个3D坐标参数，坐标参数假定了第四个值为1.该方法会负责把得到的其次坐标转为3D坐标，若只想计算方向向量可以使用Matrix4x4.MultiplyVector.该方法会忽略第四个坐标。 public Vector3 MultiplyPoint(Vector3 v) { Vector3 vector; vector.x = (((this.m00 * v.x) + (this.m01 * v.y)) + (this.m02 * v.z)) + this.m03; vector.y = (((this.m10 * v.x) + (this.m11 * v.y)) + (this.m12 * v.z)) + this.m13; vector.z = (((this.m20 * v.x) + (this.m21 * v.y)) + (this.m22 * v.z)) + this.m23; float num = (((this.m30 * v.x) + (this.m31 * v.y)) + (this.m32 * v.z)) + this.m33;//其次坐标 num = 1f / num; vector.x *= num;//转换计算 vector.y *= num;//转换计算 vector.z *= num;//转换计算 return vector; } public Vector3 MultiplyVector(Vector3 v) { Vector3 vector; vector.x = ((this.m00 * v.x) + (this.m01 * v.y)) + (this.m02 * v.z); vector.y = ((this.m10 * v.x) + (this.m11 * v.y)) + (this.m12 * v.z); vector.z = ((this.m20 * v.x) + (this.m21 * v.y)) + (this.m22 * v.z); return vector; } 具体的Transformation类现在必须将其Apply()方法更改为Matrix属性。 首先是PositionTransformation组件，Matrix4x4.SetRow接口能很简易地填充这个矩阵。 public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, position.x)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, position.y)); matrix.SetRow(2, new Vector4(0f, 0f, 1f, position.z)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; } } 其次是ScaleTransformation. public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(scale.x, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, scale.y, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, scale.z, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; } } 最后是RotationTransformation, 它设置行与列就更简单了，把之前的方法改改就能用。 public override Matrix4x4 Matrix { get { float radx = rotation.x * Mathf.Deg2Rad; float rady = rotation.y * Mathf.Deg2Rad; float radz = rotation.z * Mathf.Deg2Rad; float \\sin x = Mathf.Sin(radx); float \\cos x = Mathf.Cos(radx); float \\sin y = Mathf.Sin(rady); float \\cos y = Mathf.Cos(rady); float \\sin z = Mathf.Sin(radz); float \\cos z = Mathf.Cos(radz); Matrix4x4 matrix = new Matrix4x4(); matrix.SetColumn(0, new Vector4( \\cos y * \\cos z, \\cos x * \\sin z + \\sin x * \\sin y * \\cos z, \\sin x * \\sin z - \\cos x * \\sin y * \\cos z, 0f )); matrix.SetColumn(1, new Vector4( -\\cos y * \\sin z, \\cos x * \\cos z - \\sin x * \\sin y * \\sin z, \\sin x * \\cos z + \\cos x * \\sin y * \\sin z, 0f )); matrix.SetColumn(2, new Vector4( \\sin y, -\\sin x * \\cos y, \\cos x * \\cos y, 0f )); matrix.SetColumn(3, new Vector4(0f,0f,0f,1f)); return matrix; } } 合并矩阵 现在把上述所有变换矩阵合并为一个矩阵。 为此先在UnityMatrices类增加一个矩阵类型字段transformation。我们将在Update函数每帧更新该变量值，该步骤为先获取到第一个Transformation组件的矩阵，并依次与其他矩阵相乘，需要确保这块正确的相乘顺序。 private void Update() { UpdateTransformation(); for (int i =0 , z = 0; z &lt; generalCount; z++) { //... } } void UpdateTransformation() { GetComponents&lt;Transformation&gt;(transformations); if(transformations.Count &gt; 0) { transformation = transformations[0].Matrix; for (int i = 1; i &lt; transformations.Count; i++) { transformation = transformations[i].Matrix * transformation; } } } 最后不再执行Apply方法，而改用矩阵乘法代替： Vector3 TransformPoint(int x, int y, int z) { Vector3 coordinates = CreateCoordinate(x, y, z); // for (int i = 0; i &lt; transformations.Count; i++) // { // coordinates = transformations[i].Apply(coordinates); // } return transformation.MultiplyPoint(coordinates);; } 这个新方法是非常有效的，因为我们之前使用的方法是分别给每个点乘一个变换矩阵。而现在我们只需要一次创建一个统一的变换矩阵作用与所有点。Unity使用类似的方案将每个对象的变换层次结构简化为单个变换矩阵。 在这个例子中，我们可以使它更有效。所有的变换矩阵都有一个相同的行——[0 0 0 1]。知道了这一点，我们可以忽略这一行，跳过所有0的计算和最后的除法转换。Matrix4x4.MultiplyPoint3x4方法就是这样做的。 public Vector3 MultiplyPoint3x4(Vector3 v) { Vector3 vector; vector.x = (((this.m00 * v.x) + (this.m01 * v.y)) + (this.m02 * v.z)) + this.m03; vector.y = (((this.m10 * v.x) + (this.m11 * v.y)) + (this.m12 * v.z)) + this.m13; vector.z = (((this.m20 * v.x) + (this.m21 * v.y)) + (this.m22 * v.z)) + this.m23; return vector; } 这个方法有时候有用，有时候不能用。因为有时我们需要的一些变换矩阵会改变这最后一行。 到目前为止只有位移变换需要第四行。所以缩放、旋转使用Matrix4x4.MultiplyPoint3x4计算速度会更快。现在就把Apply()方法改为虚方法，再由旋转、缩放组件重写，代码就不贴了。 3D到2D投影矩阵 到目前为止，我们能够把一个点的坐标从一个3D空间变换到另一个3D空间。但是这些点又如何展示到2D空间呢？这肯定需要一个从3D到2D的变换矩阵。那么我们开始寻找这个矩阵吧！ 先构造一个新的继承自Transformation的实体变换组件作用于摄像机的投影，默认值为单位矩阵。 public class CameraTransformation : Transformation { public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 1f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; } } } 正交相机Orthographic Camera 从3D变换到2D空间最直接粗暴的方式是丢弃一个维度数据。就好像把3维空间压缩到2维平面，这个平面就像一个画布，用来渲染屏幕。现在我们把Z轴丢弃，试试看会发生什么 [\\begin{bmatrix} 1&amp;0&amp;0&amp;0 0&amp;1&amp;0&amp;0 0&amp;0&amp;0&amp;0 0&amp;0&amp;0&amp;1 \\end{bmatrix}] 把代码修改为 public class CameraTransformation : Transformation { public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 0f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 0f, 1f)); return matrix; } } } 3d转换到2d 实际上，这种粗暴的方法还蛮像那么回事，确实变成了2D了。其他的X、Y轴同理，就不演示了。这就是正交投影。不管相机如何缩放、旋转、位移，始终呈现的2D效果。移动相机的视觉效果和移动世界的相反方向是一致的，也就是3个变换组件的变量与摄像机的缩放、旋转 、位移变量是互为正负关系。 透视相机Perspective Camera 正交相机不能模拟3D世界就很尴尬。所以我们需要一个透视相机，由于视角的原因，呈现一个原小近大的视觉。那么基于此，我们可以根据点到摄像机的距离重建这个视觉效果。 以Z轴为例，把单位矩阵代表Z轴的列元素全部置0，再把单位矩阵最后一行改为[0,0,1,0]，这步改变将确保结果坐标的第四个值等于Z坐标，最后所有坐标都除以Z \\(\\begin{bmatrix} 1&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;1&amp;0\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} x\\\\ y\\\\ 0\\\\ z\\\\ \\end{bmatrix}\\) \\(\\rightarrow\\) \\(\\begin{bmatrix} x \\over z\\\\ y \\over z\\\\ 0\\\\ \\end{bmatrix}\\) public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(1f, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, 1f, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 0f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 1f, 0f)); return matrix; } } 与正交投影最大的不同是这些点不会直接移向到平面，而是他们会移向摄像机的位置，当然这只对位于摄像机前面的点有效，而在摄像机后面的点就不会正确的投影。先确保所有点都能位于摄像机的前方，把摄像机的Unity.Transform组件Position.Z值调好，保证所有点都 先可见。 透视投影 设置一个点到平面的投影距离，它也会影响投影视觉效果。它就像相机的焦距，值越大视野就越小。现在我们先定义一个变量focalLength值默认为1，这能产生90°的视野。 public float focalLength = 1f; 当这个focalLength值越大就像相机在进行聚焦，这有效地增加了所有点的比例(想象一下相机变焦)。当我们压缩Z轴时，是不必进行缩放的 \\(\\begin{bmatrix} fl&amp;0&amp;0&amp;0\\\\ 0&amp;fl&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;1&amp;0\\\\ \\end{bmatrix}\\) \\(\\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\ \\end{bmatrix}\\) = \\(\\begin{bmatrix} xfl\\\\ yfl\\\\ 0\\\\ z\\\\ \\end{bmatrix}\\) \\(\\rightarrow\\) \\(\\begin{bmatrix} xfl \\over z\\\\ yfl \\over z\\\\ 0\\\\ \\end{bmatrix}\\) public override Matrix4x4 Matrix { get { Matrix4x4 matrix = new Matrix4x4(); matrix.SetRow(0, new Vector4(focalLength, 0f, 0f, 0f)); matrix.SetRow(1, new Vector4(0f, focalLength, 0f, 0f)); matrix.SetRow(2, new Vector4(0f, 0f, 0f, 0f)); matrix.SetRow(3, new Vector4(0f, 0f, 1f, 0f)); return matrix; } } 现在有了一个简单的透视相机，如果要完全模拟Unity的透视相机，我们还必须处理近平面 and 远平面。这将需要处理投影到一个立方体而不是一个平面，因此需要保留深度信息。然后还有视野裁切方面的问题。此外，Unity的摄像头是在负Z方向上拍摄的，这需要对一些数字 进行求负。 矩阵不可怕。" } ]
